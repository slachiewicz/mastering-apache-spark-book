== [[CheckAnalysis]] CheckAnalysis -- Analysis Validation

`CheckAnalysis` defines <<checkAnalysis, checkAnalysis>> method that link:spark-sql-Analyzer.adoc[Analyzer] uses to check if a link:spark-sql-LogicalPlan.adoc[logical plan] is correct (after all the transformations) by applying <<checkAnalysis-validations, validation rules>> and in the end marking it as analyzed.

NOTE: An analyzed logical plan is correct and ready for execution.

`CheckAnalysis` defines <<extendedCheckRules, extendedCheckRules extension point>> that allows for extra analysis check rules.

=== [[checkAnalysis]] Checking Results of Analysis of Logical Plan and Marking Plan As Analyzed -- `checkAnalysis` Method

[source, scala]
----
checkAnalysis(plan: LogicalPlan): Unit
----

`checkAnalysis` recursively checks the correctness of the analysis of the input link:spark-sql-LogicalPlan.adoc[LogicalPlan] and link:spark-sql-LogicalPlan.adoc#setAnalyzed[marks it as analyzed].

NOTE: `checkAnalysis` fails analysis when finds link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation] in the input `LogicalPlan`...FIXME What else?

Internally, `checkAnalysis` processes nodes in the input `plan` (starting from the leafs, i.e. nodes down the operator tree).

`checkAnalysis` skips link:spark-sql-LogicalPlan.adoc#analyzed[logical plans that have already undergo analysis].

[[checkAnalysis-validations]]
.checkAnalysis's Validation Rules (in the order of execution)
[width="100%",cols="1,2",options="header"]
|===
| LogicalPlan/Operator
| Behaviour

| link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation]
a| Fails analysis with the error message:

```
Table or view not found: [tableIdentifier]
```

| Unresolved link:spark-sql-Expression-Attribute.adoc[Attribute]
a| Fails analysis with the error message:

```
cannot resolve '[expr]' given input columns: [from]
```

| link:spark-sql-Expression.adoc[Expression] with link:spark-sql-Expression.adoc#checkInputDataTypes[incorrect input data types]
a| Fails analysis with the error message:

```
cannot resolve '[expr]' due to data type mismatch: [message]
```

| Unresolved `Cast`
a| Fails analysis with the error message:

```
invalid cast from [dataType] to [dataType]
```

| [[Grouping]] `Grouping`
a| Fails analysis with the error message:

```
grouping() can only be used with GroupingSets/Cube/Rollup
```

| [[GroupingID]] `GroupingID`
a| Fails analysis with the error message:

```
grouping_id() can only be used with GroupingSets/Cube/Rollup
```

| link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] with link:spark-sql-Expression-AggregateExpression.adoc[AggregateExpression] with `isDistinct` flag enabled
a| Fails analysis with the error message:

```
Distinct window functions are not supported: [w]
```

Example:

[options="wrap"]
----
val windowedDistinctCountExpr = "COUNT(DISTINCT 1) OVER (PARTITION BY value)"
scala> spark.emptyDataset[Int].selectExpr(windowedDistinctCountExpr)
org.apache.spark.sql.AnalysisException: Distinct window functions are not supported: count(distinct 1) windowspecdefinition(value#95, ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING);;
Project [COUNT(1) OVER (PARTITION BY value UnspecifiedFrame)#97L]
+- Project [value#95, COUNT(1) OVER (PARTITION BY value UnspecifiedFrame)#97L, COUNT(1) OVER (PARTITION BY value UnspecifiedFrame)#97L]
   +- Window [count(distinct 1) windowspecdefinition(value#95, ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS COUNT(1) OVER (PARTITION BY value UnspecifiedFrame)#97L], [value#95]
      +- Project [value#95]
         +- LocalRelation <empty>, [value#95]

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:90)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:108)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:86)
----

| [[deterministic]] link:spark-sql-Expression.adoc#deterministic[Nondeterministic expressions]
| FIXME

| [[UnresolvedHint]] `UnresolvedHint`
| FIXME

| FIXME
| FIXME
|===

After <<checkAnalysis-validations, the validations>>, `checkAnalysis` executes <<extendedCheckRules, additional check rules for correct analysis>>.

`checkAnalysis` then checks if `plan` is analyzed correctly (i.e. no logical plans are left unresolved). If there is one, `checkAnalysis` fails the analysis with `AnalysisException` and the following error message:

```
unresolved operator [o.simpleString]
```

In the end, `checkAnalysis` link:spark-sql-LogicalPlan.adoc#setAnalyzed[marks the entire logical plan as analyzed].

[NOTE]
====
`checkAnalysis` is used when:

* `QueryExecution` link:spark-sql-QueryExecution.adoc#assertAnalyzed[creates analyzed logical plan and checks its correctness] (which happens mostly when a `Dataset` link:spark-sql-Dataset.adoc#creating-instance[is created])

* `ExpressionEncoder` does link:spark-sql-ExpressionEncoder.adoc#resolveAndBind[resolveAndBind]

* `ResolveAggregateFunctions` is executed (for `Sort` logical plan)
====

=== [[extendedCheckRules]] Extra Analysis Check Rules -- `extendedCheckRules` Extension Point

[source, scala]
----
extendedCheckRules: Seq[LogicalPlan => Unit]
----

`extendedCheckRules` is a collection of rules (functions) that <<checkAnalysis, checkAnalysis>> uses for custom analysis checks (after the <<checkAnalysis-validations, main validations>> have been executed).

NOTE: When a condition of a rule does not hold the function throws an `AnalysisException` directly or using `failAnalysis` method.
