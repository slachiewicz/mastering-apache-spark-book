== Settings

The following list are the settings used to configure Spark SQL applications.

You can set them in a link:spark-sql-SparkSession.adoc[SparkSession] upon instantiation using link:spark-sql-sparksession-builder.adoc#config[config] method.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .master("local[*]")
  .appName("My Spark Application")
  .config("spark.sql.warehouse.dir", "c:/Temp") // <1>
  .getOrCreate
----
<1> Sets <<spark_sql_warehouse_dir, spark.sql.warehouse.dir>> for the Spark SQL session

.Spark SQL Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Default
| Description

| [[spark.sql.catalogImplementation]] `spark.sql.catalogImplementation`
| `in-memory`
a| (internal) Selects the active catalog implementation from:

* `in-memory`
* `hive`

TIP: Read link:spark-sql-ExternalCatalog.adoc[ExternalCatalog -- System Catalog of Permanent Entities].

TIP: You can enable Hive support in a `SparkSession` using link:spark-sql-SparkSession.adoc#enableHiveSupport[`enableHiveSupport` builder method].

| [[spark.sql.sources.default]] `spark.sql.sources.default`
| `parquet`
a| Defines the default data source to use for link:spark-sql-DataFrameReader.adoc[DataFrameReader].

Used when:

* Reading (link:spark-sql-DataFrameWriter.adoc[DataFrameWriter]) or writing (link:spark-sql-DataFrameReader.adoc[DataFrameReader]) datasets
* link:spark-sql-Catalog.adoc#createExternalTable[Creating external table from a path] (in `Catalog.createExternalTable`)

* Reading (`DataStreamReader`) or writing (`DataStreamWriter`) in Structured Streaming

| [[spark.sql.TungstenAggregate.testFallbackStartsAt]] `spark.sql.TungstenAggregate.testFallbackStartsAt`
| (empty)
| A comma-separated pair of numbers, e.g. `5,10`, that `HashAggregateExec` link:spark-sql-SparkPlan-HashAggregateExec.adoc#testFallbackStartsAt[uses] to inform `TungstenAggregationIterator` to switch to a sort-based aggregation when the hash-based approach is unable to acquire enough memory.

| [[spark.sql.ui.retainedExecutions]] `spark.sql.ui.retainedExecutions`
| `1000`
| The number of `SQLExecutionUIData` entries to keep in `failedExecutions` and `completedExecutions` internal registries.

When a query execution finishes, the execution is removed from the internal `activeExecutions` registry and stored in `failedExecutions` or `completedExecutions` given the end execution status. It is when `SQLListener` makes sure that the number of `SQLExecutionUIData` entires does not exceed `spark.sql.ui.retainedExecutions` Spark property and removes the excess of entries.
|===

=== [[spark_sql_warehouse_dir]] spark.sql.warehouse.dir

`spark.sql.warehouse.dir` (default: `${system:user.dir}/spark-warehouse`) is the default location of Hive warehouse directory (using Derby) with managed databases and tables.

See also the official https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive Metastore Administration] document.

=== [[spark.sql.parquet.filterPushdown]] spark.sql.parquet.filterPushdown

`spark.sql.parquet.filterPushdown` (default: `true`) is a flag to control the link:spark-sql-Optimizer-PushDownPredicate.adoc[filter predicate push-down optimization] for data sources using parquet file format.

=== [[spark.sql.allowMultipleContexts]] spark.sql.allowMultipleContexts

`spark.sql.allowMultipleContexts` (default: `true`) controls whether creating multiple SQLContexts/HiveContexts is allowed.

=== [[spark.sql.columnNameOfCorruptRecord]] spark.sql.columnNameOfCorruptRecord

`spark.sql.columnNameOfCorruptRecord`...FIXME

=== [[spark.sql.dialect]] spark.sql.dialect

`spark.sql.dialect` - FIXME

=== [[spark.sql.streaming.checkpointLocation]] spark.sql.streaming.checkpointLocation

`spark.sql.streaming.checkpointLocation` is the default location for storing checkpoint data for link:spark-sql-streaming-StreamingQuery.adoc[continuously executing queries].
