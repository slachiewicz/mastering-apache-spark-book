== [[UnifiedMemoryManager]] UnifiedMemoryManager

`UnifiedMemoryManager` is the default link:spark-MemoryManager.adoc[MemoryManager] with `onHeapStorageMemory` being ??? and `onHeapExecutionMemory` being ???

=== [[getMaxMemory]] Calculate Maximum Memory to Use -- `getMaxMemory` Method

[source, scala]
----
getMaxMemory(conf: SparkConf): Long
----

`getMaxMemory` calculates the maximum memory to use for execution and storage.

[source, scala]
----
// local mode with --conf spark.driver.memory=2g
scala> sc.getConf.getSizeAsBytes("spark.driver.memory")
res0: Long = 2147483648

scala> val systemMemory = Runtime.getRuntime.maxMemory

// fixed amount of memory for non-storage, non-execution purposes
val reservedMemory = 300 * 1024 * 1024

// minimum system memory required
val minSystemMemory = (reservedMemory * 1.5).ceil.toLong

val usableMemory = systemMemory - reservedMemory

val memoryFraction = sc.getConf.getDouble("spark.memory.fraction", 0.6)
scala> val maxMemory = (usableMemory * memoryFraction).toLong
maxMemory: Long = 956615884

import org.apache.spark.network.util.JavaUtils
scala> JavaUtils.byteStringAsMb(maxMemory + "b")
res1: Long = 912
----

`getMaxMemory` reads <<spark_testing_memory, the maximum amount of memory that the Java virtual machine will attempt to use>> and decrements it by <<spark_testing_reservedMemory, reserved system memory>> (for non-storage and non-execution purposes).

`getMaxMemory` makes sure that the following requirements are met:

1. System memory is not smaller than about 1,5 of the reserved system memory.
2. link:spark-Executor.adoc#spark.executor.memory[spark.executor.memory] is not smaller than about 1,5 of the reserved system memory.

Ultimately, `getMaxMemory` returns <<spark_memory_fraction, spark.memory.fraction>> of the maximum amount of memory for the JVM (minus the reserved system memory).

CAUTION: FIXME omnigraffle it.

=== [[creating-instance]] Creating `UnifiedMemoryManager` Instance

[source, scala]
----
class UnifiedMemoryManager(
  conf: SparkConf,
  val maxHeapMemory: Long,
  onHeapStorageRegionSize: Long,
  numCores: Int)
----

`UnifiedMemoryManager` requires a link:spark-SparkConf.adoc[SparkConf] and the following values:

* `maxHeapMemory` -- the maximum on-heap memory to manage. It is assumed that `onHeapExecutionMemoryPool` with `onHeapStorageMemoryPool` is exactly `maxHeapMemory`.
* `onHeapStorageRegionSize`
* `numCores`

`UnifiedMemoryManager` makes sure that the sum of `offHeapExecutionMemoryPool` and `offHeapStorageMemoryPool` pool sizes is exactly `maxOffHeapMemory`.

CAUTION: FIXME Describe the pools

=== [[apply]] `apply` Factory Method

[source, scala]
----
apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager
----

`apply` factory method <<creating-instance, creates an instance of `UnifiedMemoryManager`>>.

Internally, `apply` <<getMaxMemory, calculates the maximum memory to use>> (given `conf`). It then creates a `UnifiedMemoryManager` with the following values:

1. `maxHeapMemory` being the maximum memory just calculated.
2. `onHeapStorageRegionSize` being <<spark_memory_storageFraction, spark.memory.storageFraction>> of maximum memory.
3. `numCores` as configured.

NOTE: `apply` is used when link:spark-sparkenv.adoc#create[`SparkEnv` is created].

=== [[acquireStorageMemory]] `acquireStorageMemory` Method

[source, scala]
----
acquireStorageMemory(
  blockId: BlockId,
  numBytes: Long,
  memoryMode: MemoryMode): Boolean
----

`acquireStorageMemory` has two modes of operation per `memoryMode`, i.e. `MemoryMode.ON_HEAP` or `MemoryMode.OFF_HEAP`, for execution and storage pools, and the maximum amount of memory to use.

CAUTION: FIXME Where are they used?

NOTE: `acquireStorageMemory` is part of the link:spark-MemoryManager.adoc#acquireStorageMemory[`MemoryManager` Contract].

In `MemoryMode.ON_HEAP`, `onHeapExecutionMemoryPool`, `onHeapStorageMemoryPool`, and <<maxOnHeapStorageMemory, maxOnHeapStorageMemory>> are used.

In `MemoryMode.OFF_HEAP`, `offHeapExecutionMemoryPool`, `offHeapStorageMemoryPool`, and `maxOffHeapMemory` are used.

CAUTION: FIXME What is the difference between them?

It makes sure that the requested number of bytes `numBytes` (for a block to store) fits the available memory. If it is not the case, you should see the following INFO message in the logs and the method returns `false`.

```
INFO Will not store [blockId] as the required space ([numBytes] bytes) exceeds our memory limit ([maxMemory] bytes)
```

If the requested number of bytes `numBytes` is greater than `memoryFree` in the storage pool, `acquireStorageMemory` will attempt to use the free memory from the execution pool.

NOTE: The storage pool can use the free memory from the execution pool.

It will take as much memory as required to fit `numBytes` from `memoryFree` in the execution pool (up to the whole free memory in the pool).

Ultimately, `acquireStorageMemory` requests the storage pool for `numBytes` for `blockId`.

[NOTE]
====
`acquireStorageMemory` is used when `MemoryStore` link:spark-MemoryStore.adoc#putBytes[acquires storage memory to putBytes] or link:spark-MemoryStore.adoc#putIteratorAsValues[putIteratorAsValues] and link:spark-MemoryStore.adoc#putIteratorAsBytes[putIteratorAsBytes].

It is also used internally when `UnifiedMemoryManager` <<acquireUnrollMemory, acquires unroll memory>>.
====

=== [[acquireUnrollMemory]] `acquireUnrollMemory` Method

NOTE: `acquireUnrollMemory` is part of the link:spark-MemoryManager.adoc#contract[`MemoryManager` Contract].

`acquireUnrollMemory` simply forwards all the calls to <<acquireStorageMemory, acquireStorageMemory>>.

=== [[acquireExecutionMemory]] `acquireExecutionMemory` Method

[source, scala]
----
acquireExecutionMemory(
  numBytes: Long,
  taskAttemptId: Long,
  memoryMode: MemoryMode): Long
----

`acquireExecutionMemory` does...FIXME

Internally, `acquireExecutionMemory` varies per `MemoryMode`, i.e. `ON_HEAP` and `OFF_HEAP`.

.`acquireExecutionMemory` and `MemoryMode`
[options="header",width="100%"]
|===
|                     | ON_HEAP                     | OFF_HEAP
| `executionPool`     | `onHeapExecutionMemoryPool` | `offHeapExecutionMemoryPool`
| `storagePool`       | `onHeapStorageMemoryPool`   | `offHeapStorageMemoryPool`
| `storageRegionSize` | `onHeapStorageRegionSize` <1>   | `offHeapStorageMemory`
| `maxMemory`         | `maxHeapMemory` <2>             | `maxOffHeapMemory`
|===
<1> Defined when <<creating-instance, `UnifiedMemoryManager` is created>>.
<2> Defined when <<creating-instance, `UnifiedMemoryManager` is created>>.

NOTE: `acquireExecutionMemory` is part of the link:spark-MemoryManager.adoc#contract[`MemoryManager` Contract].

CAUTION: FIXME

=== [[maxOnHeapStorageMemory]] `maxOnHeapStorageMemory` Method

[source, scala]
----
maxOnHeapStorageMemory: Long
----

`maxOnHeapStorageMemory` is the difference between `maxHeapMemory` of the `UnifiedMemoryManager` and the memory currently in use in `onHeapExecutionMemoryPool` execution memory pool.

NOTE: `maxOnHeapStorageMemory` is part of the link:spark-MemoryManager.adoc#contract[`MemoryManager` Contract].

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_memory_fraction]] `spark.memory.fraction`
| `0.6`
| Fraction of JVM heap space used for execution and storage.

| [[spark_memory_storageFraction]] `spark.memory.storageFraction`
| `0.5`
|

| [[spark_testing_memory]] `spark.testing.memory`
| Java's link:++https://docs.oracle.com/javase/8/docs/api/java/lang/Runtime.html#maxMemory--++[Runtime.getRuntime.maxMemory]
| System memory

| [[spark_testing_reservedMemory]] `spark.testing.reservedMemory`
| `300M` or `0` (with `spark.testing` enabled)
|
|===
