== [[DataSourceScanExec]] DataSourceScanExec -- Contract for Leaf Physical Operators with Java Code Generation

`DataSourceScanExec` is a <<contract, contract>> for link:spark-sql-SparkPlan.adoc#LeafExecNode[leaf physical operators] that...FIXME

`DataSourceScanExec` supports link:spark-sql-CodegenSupport.adoc[Java code generation] (aka _codegen_).

NOTE: The prefix for variable names for `DataSourceScanExec` operators in a link:spark-sql-CodegenSupport.adoc#variablePrefix[generated Java source code] is *scan*.

[[nodeNamePrefix]]
The default *node name prefix* is an empty string (that is used at the very beginning of <<simpleString, simple node description>>).

[[implementations]]
.DataSourceScanExecs
[width="100%",cols="1,2",options="header"]
|===
| DataSourceScanExec
| Description

| link:spark-sql-SparkPlan-FileSourceScanExec.adoc[FileSourceScanExec]
|

| link:spark-sql-SparkPlan-RowDataSourceScanExec.adoc[RowDataSourceScanExec]
|
|===

=== [[contract]] DataSourceScanExec Contract

[source, scala]
----
package org.apache.spark.sql.execution

trait DataSourceScanExec extends LeafExecNode with CodegenSupport {
  // only required vals and methods that have no implementation
  val metastoreTableIdentifier: Option[TableIdentifier]
  val relation: BaseRelation
  def metadata: Map[String, String]
}
----

.(Subset of) DataSourceScanExec Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[metadata]] `metadata`
| Metadata (as a collection of key-value pairs) that describes this scan.

Used for <<simpleString, simpleString>>

| [[metastoreTableIdentifier]] `metastoreTableIdentifier`
| `TableIdentifier` that...FIXME

| [[relation]] `relation`
| link:spark-sql-BaseRelation.adoc[BaseRelation] that...FIXME
|===

=== [[simpleString]] Simple Text Node Description -- `simpleString` Method

[source, scala]
----
simpleString: String
----

NOTE: `simpleString` is a part of link:spark-sql-catalyst-QueryPlan.adoc#simpleString[QueryPlan Contract] to give the simple text description of a `TreeNode` in a query plan tree.

Internally, `simpleString` redacts values in <<metadata, metadata>> entries and builds the text description of the metadata (with keys and their values separated using `:`).

`simpleString` concatenates <<nodeNamePrefix, nodeNamePrefix>> with <<nodeName, nodeName>>, link:spark-sql-catalyst-QueryPlan.adoc#output[output schema] and the metadata description.

[source, scala]
----
val scanExec = basicDataSourceScanExec
scala> println(scanExec.simpleString)
Scan $line143.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anon$1@57d94b26 [] PushedFilters: [], ReadSchema: struct<>

def basicDataSourceScanExec = {
  import org.apache.spark.sql.catalyst.expressions.AttributeReference
  val output = Seq.empty[AttributeReference]
  val requiredColumnsIndex = output.indices
  import org.apache.spark.sql.sources.Filter
  val filters, handledFilters = Set.empty[Filter]
  import org.apache.spark.sql.catalyst.InternalRow
  import org.apache.spark.sql.catalyst.expressions.UnsafeRow
  val row: InternalRow = new UnsafeRow(0)
  val rdd: RDD[InternalRow] = sc.parallelize(row :: Nil)

  import org.apache.spark.sql.sources.{BaseRelation, TableScan}
  val baseRelation: BaseRelation = new BaseRelation with TableScan {
    import org.apache.spark.sql.SQLContext
    val sqlContext: SQLContext = spark.sqlContext

    import org.apache.spark.sql.types.StructType
    val schema: StructType = new StructType()

    import org.apache.spark.rdd.RDD
    import org.apache.spark.sql.Row
    def buildScan(): RDD[Row] = ???
  }

  val tableIdentifier = None
  import org.apache.spark.sql.execution.RowDataSourceScanExec
  RowDataSourceScanExec(
    output, requiredColumnsIndex, filters, handledFilters, rdd, baseRelation, tableIdentifier)
}
----
