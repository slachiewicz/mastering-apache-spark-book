== [[ExternalSorter]] ExternalSorter

`ExternalSorter` is a `Spillable` of `WritablePartitionedPairCollection` of ``K``-key / ``C``-value pairs.

When <<creating-instance, created>> `ExternalSorter` expects three different types of data defined, i.e. `K`, `V`, `C`, for keys, values, and combiner (partial) values, respectively.

NOTE: `ExternalSorter` is exclusively used when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-value pairs] (for reduce task when link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`ShuffleDependency` has key ordering defined (to sort output)].

[TIP]
====
Enable `INFO` or `WARN` logging levels for `org.apache.spark.util.collection.ExternalSorter` logger to see what happens in `ExternalSorter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.collection.ExternalSorter=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[writePartitionedFile]] `writePartitionedFile` Method

CAUTION: FIXME

=== [[creating-instance]] Creating ExternalSorter Instance

`ExternalSorter` takes the following:

1. TaskContext
2. Optional link:spark-Aggregator.adoc[Aggregator]
3. Optional link:spark-rdd-Partitioner.adoc[Partitioner]
4. Optional Scala's http://www.scala-lang.org/api/current/scala/math/Ordering.html[Ordering]
5. Optional link:spark-Serializer.adoc[Serializer]

NOTE: `ExternalSorter` uses link:spark-sparkenv.adoc#serializer[`SparkEnv` to access the default `Serializer`].

NOTE: `ExternalSorter` is created when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-value pairs] (for reduce task when link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`ShuffleDependency` has key ordering defined (to sort output)].

=== [[spillMemoryIteratorToDisk]] `spillMemoryIteratorToDisk` Internal Method

[source, scala]
----
spillMemoryIteratorToDisk(inMemoryIterator: WritablePartitionedIterator): SpilledFile
----

CAUTION: FIXME

=== [[spill]] `spill` Method

[source, scala]
----
spill(collection: WritablePartitionedPairCollection[K, C]): Unit
----

NOTE: `spill` is part of Spillable contract.

CAUTION: FIXME

=== [[maybeSpillCollection]] `maybeSpillCollection` Internal Method

[source, scala]
----
maybeSpillCollection(usingMap: Boolean): Unit
----

CAUTION: FIXME

=== [[insertAll]] `insertAll` Method

[source, scala]
----
insertAll(records: Iterator[Product2[K, V]]): Unit
----

CAUTION: FIXME

NOTE: `insertAll` is used when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-value pairs] (for reduce task when link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`ShuffleDependency` has key ordering defined (to sort output)].

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_shuffle_file_buffer]] `spark.shuffle.file.buffer`
| `32k`
| Size of the in-memory buffer for each shuffle file output stream. In bytes unless the unit is specified.

These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files.

Used in `ExternalSorter`, link:spark-BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and `ExternalAppendOnlyMap` (for `fileBufferSize`) and in link:spark-ShuffleExternalSorter.adoc[ShuffleExternalSorter] (for link:spark-ShuffleExternalSorter.adoc#fileBufferSizeBytes[fileBufferSizeBytes]).

NOTE: `spark.shuffle.file.buffer` was previously known as `spark.shuffle.file.buffer.kb`.

| [[spark_shuffle_spill_batchSize]] `spark.shuffle.spill.batchSize`
| `10000`
| Size of object batches when reading/writing from serializers.

|===
