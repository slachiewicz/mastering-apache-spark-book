== [[LocalSchedulerBackend]] LocalSchedulerBackend

`LocalSchedulerBackend` is a link:spark-SchedulerBackend.adoc[scheduler backend] and a link:spark-ExecutorBackend.adoc[ExecutorBackend] for link:spark-local.adoc[Spark local run mode].

`LocalSchedulerBackend` acts as a "cluster manager" for local mode to offer resources on the single link:spark-workers.adoc[worker] it manages, i.e. it calls `TaskSchedulerImpl.resourceOffers(offers)` with `offers` being a single-element collection with link:spark-TaskSchedulerImpl.adoc#WorkerOffer[WorkerOffer].

NOTE: link:spark-TaskSchedulerImpl.adoc#WorkerOffer[WorkerOffer] represents a resource offer with CPU cores available on an executor.

When an executor sends task status updates (using `ExecutorBackend.statusUpdate`), they are passed along as <<messages, StatusUpdate>> to link:spark-LocalEndpoint.adoc[LocalEndpoint].

.Task status updates flow in local mode
image::images/LocalSchedulerBackend-LocalEndpoint-Executor-task-status-updates.png[align="center"]

When `LocalSchedulerBackend` starts up, it registers a new link:spark-rpc-RpcEndpoint.adoc[RpcEndpoint] called *LocalSchedulerBackendEndpoint* that is backed by link:spark-LocalEndpoint.adoc[LocalEndpoint]. This is announced on link:spark-LiveListenerBus.adoc[LiveListenerBus] as `driver` (using link:spark-SparkListener.adoc#SparkListenerExecutorAdded[SparkListenerExecutorAdded] message).

The application ids are in the format of `local-[current time millis]`.

It communicates with link:spark-LocalEndpoint.adoc[LocalEndpoint] using <<messages, RPC messages>>.

The default parallelism is controlled using link:spark-rdd-partitions.adoc#spark_default_parallelism[spark.default.parallelism] property.
