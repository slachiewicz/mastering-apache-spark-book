== [[Analyzer]] Analyzer -- Logical Query Plan Analyzer

`Analyzer` is a *logical query plan analyzer* in Spark SQL that <<execute, semantically validates and transforms an unresolved logical plan>> to an *analyzed logical plan* (with proper relational entities) using <<batches, logical evaluation rules>>.

```
Analyzer: Unresolved Logical Plan ==> Analyzed Logical Plan
```

You can access a session-specific `Analyzer` through link:spark-sql-SessionState.adoc#analyzer[SessionState].

[source, scala]
----
val spark: SparkSession = ...
spark.sessionState.analyzer
----

You can access the analyzed logical plan of a `Dataset` using link:spark-sql-dataset-operators.adoc#explain[explain] (with `extended` flag enabled) or SQL's `EXPLAIN EXTENDED` operators.

[source, scala]
----
// sample Dataset
val inventory = spark.range(5)
  .withColumn("new_column", 'id + 5 as "plus5")

// Using explain operator (with extended flag enabled)
scala> inventory.explain(extended = true)
== Parsed Logical Plan ==
'Project [*, ('id + 5) AS plus5#81 AS new_column#82]
+- Range (0, 5, step=1, splits=Some(8))

== Analyzed Logical Plan ==
id: bigint, new_column: bigint
Project [id#78L, (id#78L + cast(5 as bigint)) AS new_column#82L]
+- Range (0, 5, step=1, splits=Some(8))

== Optimized Logical Plan ==
Project [id#78L, (id#78L + 5) AS new_column#82L]
+- Range (0, 5, step=1, splits=Some(8))

== Physical Plan ==
*Project [id#78L, (id#78L + 5) AS new_column#82L]
+- *Range (0, 5, step=1, splits=8)
----

Alternatively, you can also access the analyzed logical plan through ``QueryExecution``'s link:spark-sql-QueryExecution.adoc#analyzed[analyzed] attribute (that together with `numberedTreeString` method is a very good "debugging" tool).

[source, scala]
----
// Here with numberedTreeString to...please your eyes :)
scala> println(inventory.queryExecution.analyzed.numberedTreeString)
00 Project [id#78L, (id#78L + cast(5 as bigint)) AS new_column#82L]
01 +- Range (0, 5, step=1, splits=Some(8))
----

`Analyzer` defines <<extendedResolutionRules, extendedResolutionRules>> extension point for additional logical evaluation rules that a custom `Analyzer` can use to extend the <<Resolution, Resolution>> batch. The rules are added at the end of the `Resolution` batch.

NOTE: link:spark-sql-SessionState.adoc[SessionState] uses its own `Analyzer` with custom <<extendedResolutionRules, extendedResolutionRules>>, <<postHocResolutionRules, postHocResolutionRules>>, and <<extendedCheckRules, extendedCheckRules>> extension methods.

`Analyzer` is <<creating-instance, created>> while its owning link:spark-sql-SessionState.adoc#apply[SessionState] is.

[[internal-registries]]
.Analyzer's Internal Registries and Counters (in alphabetical order)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[extendedResolutionRules]] `extendedResolutionRules`
| Additional link:spark-sql-catalyst-RuleExecutor.adoc#Rule[rules] for <<Resolution, Resolution>> batch. Empty by default

| [[fixedPoint]] `fixedPoint`
| `FixedPoint` with <<maxIterations, maxIterations>> for <<Hints, Hints>>, <<Substitution, Substitution>>, <<Resolution, Resolution>> and <<Cleanup, Cleanup>> batches.

Set when `Analyzer` <<creating-instance, is created>> (and can be defined explicitly or through link:spark-sql-CatalystConf.adoc#optimizerMaxIterations[optimizerMaxIterations] configuration setting.

| [[postHocResolutionRules]] `postHocResolutionRules`
| The only link:spark-sql-catalyst-RuleExecutor.adoc#Rule[rules] in <<Post-Hoc-Resolution, Post-Hoc Resolution>> batch if defined (that are executed in one pass, i.e. `Once` strategy). Empty by default
|===

`Analyzer` is used by `QueryExecution` to link:spark-sql-QueryExecution.adoc#analyzed[resolve the managed `LogicalPlan`] (and, as a sort of follow-up, link:spark-sql-QueryExecution.adoc#assertAnalyzed[assert that a structured query has already been properly analyzed], i.e. no failed or unresolved or somehow broken logical plan operators and expressions exist).

[TIP]
====
Enable `TRACE` or `DEBUG` logging levels for the respective session-specific loggers to see what happens inside `Analyzer`.

* `pass:[org.apache.spark.sql.internal.SessionState$$anon$1]`

* `pass:[org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1]` when link:spark-sql-SparkSession.adoc#enableHiveSupport[Hive support is enabled]

Add the following line to `conf/log4j.properties`:

```
# with no Hive support
log4j.logger.org.apache.spark.sql.internal.SessionState$$anon$1=TRACE

# with Hive support enabled
log4j.logger.org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1=DEBUG
```

Refer to link:spark-logging.adoc[Logging].

---

The reason for such weird-looking logger names is that `analyzer` attribute is created as an anonymous subclass of `Analyzer` class in the respective `SessionStates`.
====

=== [[execute]] Executing Logical Evaluation Rules -- `execute` Method

`Analyzer` is a link:spark-sql-catalyst-RuleExecutor.adoc[RuleExecutor] that defines the <<batches, logical evaluation rules>> (i.e. resolving, removing, and in general modifying it), e.g.

* Resolves unresolved <<ResolveRelations, relations>> and <<ResolveFunctions, functions>> (including `UnresolvedGenerators`) using provided <<catalog, SessionCatalog>>
* ...

[[batches]]
.Analyzer's Batches and Logical Evaluation Rules (in the order of execution)
[cols="2,1,3,3",options="header",width="100%"]
|===
^.^| Batch Name
^.^| Strategy
| Rules
| Description

.2+^.^| [[Hints]] Hints
.2+^.^| <<fixedPoint, FixedPoint>>
| [[ResolveBroadcastHints]] ResolveBroadcastHints
| Adds a link:spark-sql-LogicalPlan-BroadcastHint.adoc[BroadcastHint] unary operator to a logical plan for `BROADCAST`, `BROADCASTJOIN` and `MAPJOIN` broadcast hints for link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation] and link:spark-sql-LogicalPlan-SubqueryAlias.adoc[SubqueryAlias] logical plans.

| RemoveAllHints
| Removes all the hints (valid or not).

^.^| Simple Sanity Check
^.^| `Once`
| [[LookupFunctions]] LookupFunctions
| Checks whether a function identifier (referenced by an link:spark-sql-LogicalPlan-UnresolvedFunction.adoc[UnresolvedFunction]) link:spark-sql-SessionCatalog.adoc#functionExists[exists in the function registry]. Throws a `NoSuchFunctionException` if not.

.4+^.^| [[Substitution]] Substitution
.4+^.^| <<fixedPoint, FixedPoint>>
| CTESubstitution
| Resolves `With` operators (and substitutes named common table expressions -- CTEs)

| [[WindowsSubstitution]] link:spark-sql-Analyzer-WindowsSubstitution.adoc[WindowsSubstitution]
| Substitutes link:spark-sql-Expression-WindowExpression.adoc#UnresolvedWindowExpression[UnresolvedWindowExpression] with link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] for link:spark-sql-LogicalPlan-WithWindowDefinition.adoc[WithWindowDefinition] logical operators.

| EliminateUnions
| Eliminates `Union` of one child into that child

| SubstituteUnresolvedOrdinals
| Replaces ordinals in `Sort` and `Aggregate` operators with `UnresolvedOrdinal`

.26+^.^| [[Resolution]] Resolution
.26+^.^| <<fixedPoint, FixedPoint>>
| ResolveTableValuedFunctions
| Replaces `UnresolvedTableValuedFunction` with table-valued function.

| [[ResolveRelations]] ResolveRelations
| Resolves `InsertIntoTable` and link:spark-sql-LogicalPlan-UnresolvedRelation.adoc[UnresolvedRelation] operators

| ResolveReferences
|
| ResolveCreateNamedStruct
|
| ResolveDeserializer
|
| ResolveNewInstance
|
| ResolveUpCast
|

| [[ResolveGroupingAnalytics]] ResolveGroupingAnalytics
a|

Resolves grouping expressions up in a logical plan tree:

* `Cube`, `Rollup` and link:spark-sql-LogicalPlan-GroupingSets.adoc[GroupingSets] expressions
* `Filter` with `Grouping` or `GroupingID` expressions
* `Sort` with `Grouping` or `GroupingID` expressions

Expects that all children of a logical operator are already resolved (and given it belongs to a fixed-point batch it will likely happen at some iteration).

Fails analysis when `grouping__id` Hive function is used.

```
scala> sql("select grouping__id").queryExecution.analyzed
org.apache.spark.sql.AnalysisException: grouping__id is deprecated; use grouping_id() instead;
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$$anonfun$apply$6.applyOrElse(Analyzer.scala:451)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGroupingAnalytics$$anonfun$apply$6.applyOrElse(Analyzer.scala:448)
```

NOTE: `ResolveGroupingAnalytics` is only for grouping functions resolution while <<ResolveAggregateFunctions, ResolveAggregateFunctions>> is responsible for resolving the other aggregates.

| [[ResolvePivot]] ResolvePivot
| Resolves link:spark-sql-LogicalPlan-Pivot.adoc[Pivot] logical operator to `Project` with an link:spark-sql-LogicalPlan-Aggregate.adoc[Aggregate] unary logical operator (for supported data types in aggregates) or just a single `Aggregate`.

| ResolveOrdinalInOrderByAndGroupBy
|
| ResolveMissingReferences
|
| [[ExtractGenerator]] ExtractGenerator
|
| ResolveGenerate
|
| [[ResolveFunctions]] ResolveFunctions
a| Resolves functions using link:spark-sql-SessionCatalog.adoc#lookupFunction[SessionCatalog]:

* link:spark-sql-Expression-Generator.adoc#UnresolvedGenerator[UnresolvedGenerator] to a link:spark-sql-Expression-Generator.adoc[Generator]

* link:spark-sql-LogicalPlan-UnresolvedFunction.adoc[UnresolvedFunction] to a link:spark-sql-Expression-AggregateExpression.adoc[AggregateExpression] (with link:spark-sql-Expression-AggregateFunction.adoc[AggregateFunction]) or link:spark-sql-Expression-AggregateWindowFunction.adoc[AggregateWindowFunction]

If `Generator` is not found, `ResolveFunctions` reports the error:

[options="wrap"]
----
[name] is expected to be a generator. However, its class is [className], which is not a generator.
----

| [[ResolveAliases]] ResolveAliases
a| Replaces `UnresolvedAlias` link:spark-sql-Expression.adoc[expressions] with concrete aliases:

* link:spark-sql-Expression.adoc#NamedExpression[NamedExpressions]
* `MultiAlias` (for `GeneratorOuter` and `Generator`)
* `Alias` (for `Cast` and `ExtractValue`)

| ResolveSubquery
|
| [[ResolveWindowOrder]] ResolveWindowOrder
|
| [[ResolveWindowFrame]] link:spark-sql-Analyzer-ResolveWindowFrame.adoc[ResolveWindowFrame]
| Resolves link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] expressions

| ResolveNaturalAndUsingJoin
|

| [[ExtractWindowExpressions]] ExtractWindowExpressions
|

| [[GlobalAggregates]] GlobalAggregates
| Resolves (aka _replaces_) `Project` operators with link:spark-sql-Expression-AggregateExpression.adoc[AggregateExpression] that are not link:spark-sql-Expression-WindowExpression.adoc[WindowExpression] with `Aggregate` unary logical operators.

| [[ResolveAggregateFunctions]] ResolveAggregateFunctions
a| Resolves aggregate functions in `Filter` and `Sort` operators

NOTE: `ResolveAggregateFunctions` skips (i.e. does not resolve) grouping functions that are resolved by <<ResolveGroupingAnalytics, ResolveGroupingAnalytics>> rule.

| [[TimeWindowing]] TimeWindowing
a| Resolves link:spark-sql-Expression-TimeWindow.adoc[TimeWindow] expressions to `Filter` with link:spark-sql-LogicalPlan-Expand.adoc[Expand] logical operators.

NOTE: Multiple distinct time window expressions are not currently supported (and yield a `AnalysisException`)

| ResolveInlineTables
| Resolves `UnresolvedInlineTable` with `LocalRelation`

| TypeCoercion.typeCoercionRules
|
| <<extendedResolutionRules, extendedResolutionRules>>
|

^.^| [[Post-Hoc-Resolution]] Post-Hoc Resolution
^.^| `Once`
| <<postHocResolutionRules, postHocResolutionRules>>
|

^.^| View
^.^| `Once`
| AliasViewChild
|

^.^| Nondeterministic
^.^| `Once`
| PullOutNondeterministic
|

^.^| UDF
^.^| `Once`
| [[HandleNullInputsForUDF]] link:spark-sql-Analyzer-HandleNullInputsForUDF.adoc[HandleNullInputsForUDF]
|

^.^| FixNullability
^.^| `Once`
| FixNullability
|

^.^| ResolveTimeZone
^.^| `Once`
| ResolveTimeZone
| Replaces `TimeZoneAwareExpression` with no timezone with one with link:spark-sql-CatalystConf.adoc#sessionLocalTimeZone[session-local time zone].

^.^| [[Cleanup]] Cleanup
^.^| <<fixedPoint, FixedPoint>>
| CleanupAliases
|
|===

TIP: Consult the https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L116-L167[sources of `Analyzer`] for the up-to-date list of the evaluation rules.

=== [[creating-instance]] Creating Analyzer Instance

`Analyzer` takes the following when created:

* [[catalog]] link:spark-sql-SessionCatalog.adoc[SessionCatalog]
* [[conf]] link:spark-sql-CatalystConf.adoc[CatalystConf]
* [[maxIterations]] Number of iterations before <<fixedPoint, FixedPoint>> rule batches have converged (i.e. <<Hints, Hints>>, <<Substitution, Substitution>>, <<Resolution, Resolution>> and <<Cleanup, Cleanup>>)

`Analyzer` initializes the <<internal-registries, internal registries and counters>>.

NOTE: `Analyzer` can also be created without specifying the <<maxIterations, maxIterations>> which is then configured using link:spark-sql-CatalystConf.adoc#optimizerMaxIterations[optimizerMaxIterations] configuration setting.

=== [[resolver]] `resolver` Method

[source, scala]
----
resolver: Resolver
----

`resolver` requests <<conf, CatalystConf>> for link:spark-sql-CatalystConf.adoc#resolver[Resolver].

NOTE: `Resolver` is a mere function of two `String` parameters that returns `true` if both refer to the same entity (i.e. for case insensitive equality).
