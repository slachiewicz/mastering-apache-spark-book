== [[SchedulerBackend]] SchedulerBackend -- Pluggable Task Scheduling Systems

`SchedulerBackend` is the <<contract, abstraction>> of <<implementations, task scheduling systems>> in Apache Spark that can <<reviveOffers, revive resource offers>> from various cluster managers (e.g. built-in Spark Standalone, Hadoop YARN, Kubernetes, Apache Mesos, and <<local/spark-local.adoc#, Spark local>>).

`SchedulerBackend` simply abstracts away the differences between cluster managers with regards to resource offers and task scheduling modes.

NOTE: Being a scheduler backend system assumes a http://mesos.apache.org/[Apache Mesos]-like scheduling model in which "an application" gets *resource offers* as machines become available so it is possible to launch tasks on them. Once required resource allocation is obtained, the scheduler backend can start executors.

[[contract]]
.SchedulerBackend Contract
[cols="1m,3",options="header",width="100%"]
|===
| Method
| Description

| applicationAttemptId
a| [[applicationAttemptId]]

[source, scala]
----
applicationAttemptId(): Option[String]
----

*Execution attempt ID* of the Spark application

Default: `None` (undefined)

Used exclusively when `TaskSchedulerImpl` is requested for the <<spark-TaskSchedulerImpl.adoc#applicationAttemptId, execution attempt ID of a Spark application>>

| applicationId
a| [[applicationId]][[appId]]

[source, scala]
----
applicationId(): String
----

*Unique identifier* of the Spark Application

Default: `spark-application-[currentTimeMillis]`

Used exclusively when `TaskSchedulerImpl` is requested for the <<spark-TaskSchedulerImpl.adoc#applicationId, unique identifier of a Spark application>>

| defaultParallelism
a| [[defaultParallelism]]

[source, scala]
----
defaultParallelism(): Int
----

*Default parallelism*, i.e. a hint for the number of tasks in stages while sizing jobs

Used exclusively when `TaskSchedulerImpl` is requested for the <<spark-TaskSchedulerImpl.adoc#defaultParallelism, default parallelism>>

| getDriverLogUrls
a| [[getDriverLogUrls]]

[source, scala]
----
getDriverLogUrls: Option[Map[String, String]]
----

*Driver log URLs*

Default: `None` (undefined)

Used exclusively when `SparkContext` is requested to <<spark-SparkContext.adoc#postApplicationStart, postApplicationStart>>

| isReady
a| [[isReady]]

[source, scala]
----
isReady(): Boolean
----

Controls whether the <<spark-SchedulerBackend.adoc#, SchedulerBackend>> is ready (`true`) or not (`false`)

Default: `true`

Used exclusively when `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#waitBackendReady, wait until scheduling backend is ready>>

| killTask
a| [[killTask]]

[source, scala]
----
killTask(
  taskId: Long,
  executorId: String,
  interruptThread: Boolean,
  reason: String): Unit
----

Kills a given task

Default: Throws an `UnsupportedOperationException`

Used when:

* `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#killTaskAttempt, killTaskAttempt>> and <<spark-TaskSchedulerImpl.adoc#killAllTaskAttempts, killAllTaskAttempts>>

* `TaskSetManager` is requested to <<spark-TaskSetManager.adoc#handleSuccessfulTask, handle a successful task attempt>>

| maxNumConcurrentTasks
a| [[maxNumConcurrentTasks]]

[source, scala]
----
maxNumConcurrentTasks(): Int
----

*Maximum number of concurrent tasks* that can be launched now

Used exclusively when `SparkContext` is requested to <<spark-SparkContext.adoc#maxNumConcurrentTasks, maxNumConcurrentTasks>>

| reviveOffers
a| [[reviveOffers]]

[source, scala]
----
reviveOffers(): Unit
----

Handles resource allocation offers (from the scheduling system)

Used when `TaskSchedulerImpl` is requested to:

* <<spark-TaskSchedulerImpl.adoc#submitTasks, Submit tasks (from a TaskSet)>>

* <<spark-TaskSchedulerImpl.adoc#statusUpdate, Handle a task status update>>

* <<spark-TaskSchedulerImpl.adoc#handleFailedTask, Notify the TaskSetManager that a task has failed>>

* <<spark-TaskSchedulerImpl.adoc#checkSpeculatableTasks, Check for speculatable tasks>>

* <<spark-TaskSchedulerImpl.adoc#executorLost, Handle a lost executor event>>

| start
a| [[start]]

[source, scala]
----
start(): Unit
----

Starts the <<spark-SchedulerBackend.adoc#, SchedulerBackend>>

Used exclusively when `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#start, start>>

| stop
a| [[stop]]

[source, scala]
----
stop(): Unit
----

Stops the <<spark-SchedulerBackend.adoc#, SchedulerBackend>>

Used when:

* `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#stop, stop>>

* `MesosCoarseGrainedSchedulerBackend` is requested to <<spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.adoc#stopSchedulerBackend, stopSchedulerBackend>>

|===

[[implementations]]
.SchedulerBackends (Direct Implementations and Extensions)
[cols="1,3",options="header",width="100%"]
|===
| SchedulerBackend
| Description

| <<spark-CoarseGrainedSchedulerBackend.adoc#, CoarseGrainedSchedulerBackend>>
| [[CoarseGrainedSchedulerBackend]] Base `SchedulerBackend` for coarse-grained scheduling systems

| <<local/spark-LocalSchedulerBackend.adoc#, LocalSchedulerBackend>>
| [[LocalSchedulerBackend]] Spark local

| MesosFineGrainedSchedulerBackend
| [[MesosFineGrainedSchedulerBackend]] Fine-grained scheduling system for Apache Mesos

|===

NOTE: `org.apache.spark.scheduler.SchedulerBackend` is a `private[spark]` Scala trait in Spark.
