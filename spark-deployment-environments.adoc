== Deployment Environments -- Run Modes

Spark Deployment Environments (aka Run Modes):

* link:local/spark-local.adoc[local]
* link:spark-cluster.adoc[clustered]
** link:spark-standalone.adoc[Spark Standalone]
** link:spark-mesos/spark-mesos.adoc[Spark on Apache Mesos]
** link:yarn/README.adoc[Spark on Hadoop YARN]

A Spark application is composed of the driver and executors that can run locally (on a single JVM) or using cluster resources (like CPU, RAM and disk that are managed by a cluster manager).

NOTE: You can specify where to run the driver using the link:spark-deploy-mode.adoc[deploy mode] (using `--deploy-mode` option of spark-submit or `spark.submit.deployMode` Spark property).

=== [[master-urls]] Master URLs

Spark supports the following *master URLs* (see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L2583-L2592[private object SparkMasterRegex]):

* `local`, `local[N]` and `local[{asterisk}]` for link:local/spark-local.adoc#masterURL[Spark local]
* `local[N, maxRetries]` for link:local/spark-local.adoc#masterURL[Spark local-with-retries]
* `local-cluster[N, cores, memory]` for simulating a Spark cluster of `N` executors (threads), `cores` CPUs and `memory` locally (aka _Spark local-cluster_)
* `spark://host:port,host1:port1,...` for connecting to link:spark-standalone.adoc[Spark Standalone cluster(s)]
* `mesos://` for link:spark-mesos/spark-mesos.adoc[Spark on Mesos cluster]
* `yarn` for link:yarn/README.adoc[Spark on YARN]

You can specify the master URL of a Spark application as follows:

1. link:spark-submit.adoc[spark-submit's `--master` command-line option],

2. link:spark-SparkConf.adoc#spark.master[`spark.master` Spark property],

3. When creating a  link:spark-SparkContext.adoc#getOrCreate[`SparkContext` (using `setMaster` method)],

4. When creating a link:spark-sql-sparksession-builder.adoc[`SparkSession` (using `master` method of the builder interface)].
