== [[ContextCleaner]] `ContextCleaner` -- Spark Application Garbage Collector

`ContextCleaner` is a Spark service that is responsible for <<keepCleaning, application-wide cleanup>> of <<registerShuffleForCleanup, shuffles>>, <<registerRDDForCleanup, RDDs>>, <<registerBroadcastForCleanup, broadcasts>>, <<registerAccumulatorForCleanup, accumulators>> and <<registerRDDCheckpointDataForCleanup, checkpointed RDDs>> that is aimed at reducing the memory requirements of long-running data-heavy Spark applications.

`ContextCleaner` runs on the driver. It is created and immediately started when link:spark-sparkcontext-creating-instance-internals.adoc#_cleaner[`SparkContext` starts] (and <<spark_cleaner_referenceTracking, `spark.cleaner.referenceTracking` Spark property>> is enabled, which it is by default). It is stopped when link:spark-SparkContext.adoc#stop[`SparkContext` is stopped].

[[internal-registries]]
.ContextCleaner's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[referenceBuffer]] `referenceBuffer`
|

Used when ???

| [[referenceQueue]] `referenceQueue`
|

Used when ???

| [[listeners]] `listeners`
|

Used when ???
|===

It uses a daemon *Spark Context Cleaner* thread that cleans RDD, shuffle, and broadcast states (using `keepCleaning` method).

link:spark-rdd-ShuffleDependency.adoc[ShuffleDependencies] <<registerShuffleForCleanup, register themselves for cleanup>>.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.ContextCleaner` logger to see what happens in `ContextCleaner`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.ContextCleaner=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[doCleanupRDD]] `doCleanupRDD` Method

CAUTION: FIXME

=== [[keepCleaning]] `keepCleaning` Internal Method

[source, scala]
----
keepCleaning(): Unit
----

`keepCleaning` runs indefinitely until <<stop, `ContextCleaner` is stopped>>. It...FIXME

You should see the following DEBUG message in the logs:

```
DEBUG Got cleaning task [task]
```

NOTE: `keepCleaning` is exclusively used in <<cleaningThread, Spark Context Cleaner Cleaning Thread>> that is started once when <<start, `ContextCleaner` is started>>.

=== [[cleaningThread]] Spark Context Cleaner Cleaning Thread -- `cleaningThread` Attribute

CAUTION: FIXME

The name of the daemon thread is *Spark Context Cleaner*.

```
$ jstack -l [sparkPID] | grep "Spark Context Cleaner"
"Spark Context Cleaner" #80 daemon prio=5 os_prio=31 tid=0x00007fc304677800 nid=0xa103 in Object.wait() [0x0000000120371000]
```

NOTE: `cleaningThread` is started as a daemon thread when <<start, `ContextCleaner` starts>>.

=== [[registerRDDCheckpointDataForCleanup]] `registerRDDCheckpointDataForCleanup` Method

CAUTION: FIXME

=== [[registerBroadcastForCleanup]] `registerBroadcastForCleanup` Method

CAUTION: FIXME

=== [[registerRDDForCleanup]] `registerRDDForCleanup` Method

CAUTION: FIXME

=== [[registerAccumulatorForCleanup]] `registerAccumulatorForCleanup` Method

CAUTION: FIXME

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[creating-instance]] Creating `ContextCleaner` Instance

`ContextCleaner` takes a link:spark-SparkContext.adoc[SparkContext].

`ContextCleaner` <<internal-registries, initializes the internal registries and counters>>.

=== [[start]] Starting `ContextCleaner` -- `start` Method

[source, scala]
----
start(): Unit
----

`start` starts <<cleaningThread, cleaning thread>> and an action to request the JVM garbage collector (using `System.gc()`) every <<spark_cleaner_periodicGC_interval, spark.cleaner.periodicGC.interval>> interval.

NOTE: The action to request the JVM GC is scheduled on <<periodicGCService, `periodicGCService` executor service>>.

=== [[periodicGCService]] `periodicGCService` Single-Thread Executor Service

`periodicGCService` is an internal single-thread http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[executor service] with the name *context-cleaner-periodic-gc* to request the JVM garbage collector.

[NOTE]
====
Requests for JVM GC are scheduled every <<spark_cleaner_periodicGC_interval, spark.cleaner.periodicGC.interval>> interval.

The periodic runs are started when <<start, `ContextCleaner` starts>> and stopped when <<stop, `ContextCleaner` stops>>.
====

=== [[registerShuffleForCleanup]] Registering `ShuffleDependency` for Cleanup -- `registerShuffleForCleanup` Method

[source, scala]
----
registerShuffleForCleanup(shuffleDependency: ShuffleDependency[_, _, _]): Unit
----

`registerShuffleForCleanup` registers a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] for cleanup.

Internally, `registerShuffleForCleanup` simply executes <<registerForCleanup, registerForCleanup>> for the input `ShuffleDependency`.

NOTE: `registerShuffleForCleanup` is exclusively used when link:spark-rdd-ShuffleDependency.adoc#creating-instance[`ShuffleDependency` is created].

=== [[registerForCleanup]] Registering Object Reference For Cleanup -- `registerForCleanup` Internal Method

[source, scala]
----
registerForCleanup(objectForCleanup: AnyRef, task: CleanupTask): Unit
----

Internally, `registerForCleanup` adds the input `objectForCleanup` to <<referenceBuffer, `referenceBuffer` internal queue>>.

NOTE: Despite the widest-possible `AnyRef` type of the input `objectForCleanup`, the type is really `CleanupTaskWeakReference` which is a custom Java's https://docs.oracle.com/javase/8/docs/api/java/lang/ref/WeakReference.html[java.lang.ref.WeakReference].

=== [[doCleanupShuffle]] Removing Shuffle Blocks From `MapOutputTrackerMaster` and `BlockManagerMaster` -- `doCleanupShuffle` Method

[source, scala]
----
doCleanupShuffle(shuffleId: Int, blocking: Boolean): Unit
----

`doCleanupShuffle` performs a shuffle cleanup which is to remove the shuffle from the current link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] and link:spark-BlockManagerMaster.adoc[BlockManagerMaster]. `doCleanupShuffle` also notifies link:spark-CleanerListener.adoc[CleanerListeners].

Internally, when executed, you should see the following DEBUG message in the logs:

```
DEBUG Cleaning shuffle [id]
```

`doCleanupShuffle` link:spark-service-mapoutputtracker.adoc#unregisterShuffle[unregisters the input `shuffleId` from `MapOutputTrackerMaster`].

NOTE: `doCleanupShuffle` uses link:spark-SparkEnv.adoc#mapOutputTracker[`SparkEnv` to access the current `MapOutputTracker`].

`doCleanupShuffle` link:spark-BlockManagerMaster.adoc#removeShuffle[removes the shuffle blocks of the input `shuffleId` from `BlockManagerMaster`].

NOTE: `doCleanupShuffle` uses link:spark-SparkEnv.adoc#blockManager[`SparkEnv` to access the current `BlockManagerMaster`].

`doCleanupShuffle` informs all registered `CleanerListener` listeners (from <<listeners, `listeners` internal queue>>) that link:spark-CleanerListener.adoc#shuffleCleaned[the input `shuffleId` was cleaned].

In the end, you should see the following DEBUG message in the logs:

```
DEBUG Cleaned shuffle [id]
```

In case of any exception, you should see the following ERROR message in the logs and the exception itself.

```
ERROR Error cleaning shuffle [id]
```

NOTE: `doCleanupShuffle` is executed when <<keepCleaning, `ContextCleaner` cleans a shuffle reference>> and (interestingly) while fitting a `ALSModel` (in Spark MLlib).

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property | Default Value | Description

| [[spark_cleaner_periodicGC_interval]] `spark.cleaner.periodicGC.interval`
| `30min`
| Controls how often to trigger a garbage collection.

| [[spark_cleaner_referenceTracking]] `spark.cleaner.referenceTracking`
| `true`
| Controls whether a <<creating-instance, `ContextCleaner` should be created>> when a link:spark-SparkContext.adoc#creating-instance[`SparkContext` initializes].

| [[spark_cleaner_referenceTracking_blocking]] `spark.cleaner.referenceTracking.blocking`
| `true`
| Controls whether the cleaning thread should block on cleanup tasks (other than shuffle, which is controlled by <<spark_cleaner_referenceTracking_blocking_shuffle, spark.cleaner.referenceTracking.blocking.shuffle>> Spark property).

It is `true` as a workaround to https://issues.apache.org/jira/browse/SPARK-3015[SPARK-3015 Removing broadcast in quick successions causes Akka timeout].

| [[spark_cleaner_referenceTracking_blocking_shuffle]] `spark.cleaner.referenceTracking.blocking.shuffle`
| `false`
| Controls whether the cleaning thread should block on shuffle cleanup tasks.

It is `false` as a workaround to https://issues.apache.org/jira/browse/SPARK-3139[SPARK-3139 Akka timeouts from ContextCleaner when cleaning shuffles].

| [[spark_cleaner_referenceTracking_cleanCheckpoints]] `spark.cleaner.referenceTracking.cleanCheckpoints`
| `false`
| Controls whether to clean checkpoint files if the reference is out of scope.
