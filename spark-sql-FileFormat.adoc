== [[FileFormat]] FileFormat

`FileFormat` is the <<contract, contract>> in Spark SQL to...FIXME

[[contract]]
[source, scala]
----
package org.apache.spark.sql.execution.datasources

trait FileFormat {
  // only required methods that have no implementation
  // the others follow
  def inferSchema(
    sparkSession: SparkSession,
    options: Map[String, String],
    files: Seq[FileStatus]): Option[StructType]
  def prepareWrite(
    sparkSession: SparkSession,
    job: Job,
    options: Map[String, String],
    dataSchema: StructType): OutputWriterFactory
}
----

.(Subset of) FileFormat Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[inferSchema]] `inferSchema`
| Used when...

| [[prepareWrite]] `prepareWrite`
| Used exclusively when `FileFormatWriter` is requested to link:spark-sql-FileFormatWriter.adoc#write[write a query result].
|===

[[supportBatch]]
`supportBatch`...FIXME

[[vectorTypes]]
`vectorTypes`...FIXME

[[isSplitable]]
`isSplitable`...FIXME

[[buildReader]]
`buildReader`...FIXME

=== [[buildReaderWithPartitionValues]] `buildReaderWithPartitionValues` Method

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): PartitionedFile => Iterator[InternalRow]
----

`buildReaderWithPartitionValues`...FIXME

NOTE: `buildReaderWithPartitionValues` is used exclusively when `FileSourceScanExec` is requested for link:spark-sql-SparkPlan-FileSourceScanExec.adoc#inputRDDs[input RDDs].
