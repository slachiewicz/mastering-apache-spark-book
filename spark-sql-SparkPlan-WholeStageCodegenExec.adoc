== [[WholeStageCodegenExec]] WholeStageCodegenExec Unary Physical Operator for Java Code Generation

`WholeStageCodegenExec` is a link:spark-sql-SparkPlan.adoc#UnaryExecNode[unary physical operator] (i.e. with one <<child, child>> physical operator) that, together with link:spark-sql-SparkPlan-InputAdapter.adoc[InputAdapter], lays the foundation for link:spark-sql-whole-stage-codegen.adoc[Whole-Stage Java Code Generation] for a *codegened pipeline* of a physical query plan.

`WholeStageCodegenExec` is created when link:spark-sql-CollapseCodegenStages.adoc[CollapseCodegenStages] physical optimization rule transforms a link:spark-sql-SparkPlan.adoc[physical plan] and link:spark-sql-SQLConf.adoc#spark.sql.codegen.wholeStage[spark.sql.codegen.wholeStage] Spark SQL property is enabled.

NOTE: link:spark-sql-SQLConf.adoc#spark.sql.codegen.wholeStage[spark.sql.codegen.wholeStage] property is enabled by default.

`WholeStageCodegenExec` supports link:spark-sql-CodegenSupport.adoc[Java code generation] and marks a node in a query plan that, once <<doExecute, executed>>, triggers code generation.

[[generateTreeString]]
`WholeStageCodegenExec` is marked with `*` prefix in the tree output of a physical plan.

NOTE: As `WholeStageCodegenExec` is created as a result of `CollapseCodegenStages` optimization rule, you should use link:spark-sql-QueryExecution.adoc#executedPlan[executedPlan] phase of a query execution to have `WholeStageCodegenExec` in a plan (that you can only notice by the star prefix in plan output).

[source, scala]
----
val q = spark.range(9)
val plan = q.queryExecution.executedPlan
// Note the star prefix of Range that marks WholeStageCodegenExec
// As a matter of fact, there are two physical operators in play here
// i.e. WholeStageCodegenExec with Range as the child
scala> println(plan.numberedTreeString)
00 *Range (0, 9, step=1, splits=8)

// Let's unwrap Range physical operator
// and access the parent WholeStageCodegenExec
import org.apache.spark.sql.execution.WholeStageCodegenExec
val wsce = plan.asInstanceOf[WholeStageCodegenExec]

// Trigger code generation of the entire query plan tree
val (ctx, code) = wsce.doCodeGen

// CodeFormatter can pretty-print the code
import org.apache.spark.sql.catalyst.expressions.codegen.CodeFormatter
scala> println(CodeFormatter.format(code))
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
 * Codegend pipeline for
 * Range (0, 9, step=1, splits=8)
 */
/* 006 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
...
----

When <<doExecute, executed>>, `WholeStageCodegenExec` gives <<pipelineTime, pipelineTime>> performance metric.

[[metrics]]
.WholeStageCodegenExec's Performance Metrics
[cols="1,2,2",options="header",width="100%"]
|===
| Key
| Name (in web UI)
| Description

| [[pipelineTime]] `pipelineTime`
| (empty)
| Time of how long the whole-stage codegend pipeline has been running (i.e. the elapsed time since the underlying `BufferedRowIterator` had been created and the internal rows were all consumed).
|===

.WholeStageCodegenExec in web UI (Details for Query)
image::images/spark-sql-WholeStageCodegenExec-webui.png[align="center"]

TIP: Use link:spark-sql-Dataset.adoc#explain[explain] operator to know the physical plan of a query and find out whether or not `WholeStageCodegen` is in use.

[source, scala]
----
val q = spark.range(10).where('id === 4)
// Note the stars in the output that are for codegened operators
scala> q.explain
== Physical Plan ==
*Filter (id#0L = 4)
+- *Range (0, 10, step=1, splits=8)
----

TIP: Consider using link:spark-sql-debugging-execution.adoc[Debugging Query Execution facility] to deep dive into whole stage codegen.

[source, scala]
----
scala> q.queryExecution.debug.codegen
Found 1 WholeStageCodegen subtrees.
== Subtree 1 / 1 ==
*Filter (id#5L = 4)
+- *Range (0, 10, step=1, splits=8)
----

NOTE: link:spark-sql-SparkPlan.adoc[Physical plans] that support code generation extend link:spark-sql-CodegenSupport.adoc[CodegenSupport].

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.WholeStageCodegenExec` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.WholeStageCodegenExec=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[doExecute]] Executing WholeStageCodegenExec -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is a part of link:spark-sql-SparkPlan.adoc#doExecute[SparkPlan Contract] to produce the result of a structured query as an `RDD` of link:spark-sql-InternalRow.adoc[internal binary rows].

`doExecute` <<doCodeGen, generates the Java code>> that is link:spark-sql-CodeGenerator.adoc#compile[compiled] right afterwards.

If compilation fails and link:spark-sql-settings.adoc#spark.sql.codegen.fallback[spark.sql.codegen.fallback] is enabled, you should see the following WARN message in the logs and `doExecute` returns the link:spark-sql-SparkPlan.adoc#execute[result of executing the child physical operator].

```
WARN WholeStageCodegenExec: Whole-stage codegen disabled for this plan:
[tree]
```

If however code generation and compilation went well, `doExecute` branches off per the number of link:spark-sql-CodegenSupport.adoc#inputRDDs[input RDDs].

NOTE: `doExecute` only supports up to two link:spark-sql-CodegenSupport.adoc#inputRDDs[input RDDs].

CAUTION: FIXME

=== [[doCodeGen]] Generating Java Code for Child Subtree -- `doCodeGen` Method

[source, scala]
----
doCodeGen(): (CodegenContext, CodeAndComment)
----

CAUTION: FIXME

You should see the following DEBUG message in the logs:

```
DEBUG WholeStageCodegenExec:
[cleanedSource]
```

NOTE: `doCodeGen` is used when `WholeStageCodegenExec` <<doExecute, doExecute>> (and for link:spark-sql-debugging-execution.adoc#debugCodegen[debugCodegen]).

=== [[doConsume]] Generating Java Source Code for Whole-Stage Consume Path Code Generation -- `doConsume` Method

[source, scala]
----
doConsume(ctx: CodegenContext, input: Seq[ExprCode], row: ExprCode): String
----

NOTE: `doConsume` is a part of link:spark-sql-CodegenSupport.adoc#doConsume[CodegenSupport Contract] to generate plain Java source code for link:spark-sql-whole-stage-codegen.adoc#consume-path[whole-stage "consume" path code generation].

`doConsume` generates a Java source code that:

1. Takes (from the input `row`) the code to evaluate a Catalyst expression on an input `InternalRow`
1. Takes (from the input `row`) the term for a value of the result of the evaluation
  i. Adds `.copy()` to the term if <<needCopyResult, needCopyResult>> is turned on
1. Wraps the term inside `append()` code block

[source, scala]
----
import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext
val ctx = new CodegenContext()

import org.apache.spark.sql.catalyst.expressions.codegen.ExprCode
val exprCode = ExprCode(code = "my_code", isNull = "false", value = "my_value")

// wsce defined above, i.e at the top of the page
val consumeCode = wsce.doConsume(ctx, input = Seq(), row = exprCode)
scala> println(consumeCode)
my_code
append(my_value);
----
