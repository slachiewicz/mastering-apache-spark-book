== [[Task]] Task

`Task` (aka _command_) is the *smallest individual unit of execution* that computes a single <<partitionId, RDD partition>>.

.Tasks Are Runtime Representation of RDD Partitions
image::images/spark-rdd-partitions-job-stage-tasks.png[align="center"]

[[creating-instance]]
`Task` is described by the following:

* [[stageId]] ID of the <<spark-scheduler-Stage.adoc#, stage>> (*Stage ID*)
* [[stageAttemptId]] Stage (execution) attempt ID
* [[partitionId]] ID of the <<spark-rdd-Partition.adoc#, partition>> (*Partition ID*)
* [[localProperties]] Local properties
* [[serializedTaskMetrics]] Serialized <<spark-executor-TaskMetrics.adoc#, TaskMetrics>> (`Array[Byte]`)
* [[jobId]] Optional ID of the <<spark-scheduler-ActiveJob.adoc#, job>> (default: `None`)
* [[appId]] Optional ID of the Spark application (default: `None`)
* [[appAttemptId]] Optional ID of the Spark application's (execution) attempt ID (default: `None`)
* [[isBarrier]] `isBarrier` flag that is to say whether the task belongs to a barrier stage (default: `false`)

Tasks are <<creating-instance, created>> when `DAGScheduler` is requested to <<spark-scheduler-DAGScheduler.adoc#submitMissingTasks, submit missing tasks of a stage>>.

NOTE: `Task` is a Scala abstract class and cannot be <<creating-instance, created>> directly. It is created indirectly for the <<implementations, concrete Tasks>>.

`Task` can be <<runTask, run>> (possibly on <<preferredLocations, preferred executor>>).

Tasks are link:spark-Executor.adoc#launchTask[launched on executors] and <<run, ran when `TaskRunner` starts>>.

In other words, a task is a computation on the records in a RDD partition in a stage of a RDD in a Spark job.

NOTE: In Scala `Task` is actually `Task[T]` in which `T` is the type of the result of a task (i.e. the type of the value computed).

[[implementations]]
.Tasks
[cols="1,3",options="header",width="100%"]
|===
| Task
| Description

| <<spark-scheduler-ResultTask.adoc#, ResultTask>>
| [[ResultTask]] Computes a <<spark-scheduler-ResultStage.adoc#, ResultStage>> and gives the result back to the driver

| <<spark-scheduler-ShuffleMapTask.adoc#, ShuffleMapTask>>
| [[ShuffleMapTask]] Computes a <<spark-scheduler-ShuffleMapStage.adoc#, ShuffleMapStage>>

|===

The very last stage in a Spark job consists of multiple link:spark-scheduler-ResultTask.adoc[ResultTasks], while earlier stages can only be link:spark-scheduler-ShuffleMapTask.adoc[ShuffleMapTasks].

CAUTION: FIXME You could have a Spark job with link:spark-scheduler-ShuffleMapTask.adoc[ShuffleMapTask] being the last.

[[internal-registries]]
.`Task` Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[context]] `context`
| Used when ???

| [[epoch]] `epoch`
| Set for a `Task` when link:spark-scheduler-TaskSetManager.adoc#creating-instance[`TaskSetManager` is created] and later used when link:spark-Executor-TaskRunner.adoc#run[`TaskRunner` runs] and when `DAGScheduler` link:spark-scheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[handles a `ShuffleMapTask` successful completion].

| [[_executorDeserializeTime]] `_executorDeserializeTime`
| Used when ???

| [[_executorDeserializeCpuTime]] `_executorDeserializeCpuTime`
| Used when ???

| [[_killed]] `_killed`
| Used when ???

| [[metrics]] `metrics`
| link:spark-executor-TaskMetrics.adoc[TaskMetrics]

Created lazily when <<creating-instance, `Task` is created>> from <<serializedTaskMetrics, serializedTaskMetrics>>.

Used when ???

| [[taskMemoryManager]] `taskMemoryManager`
| link:spark-memory-TaskMemoryManager.adoc[TaskMemoryManager] that manages the memory allocated by the task.

Used when ???

| [[taskThread]] `taskThread`
| Used when ???

|===

A task can only belong to one stage and operate on a single partition. All tasks in a stage must be completed before the stages that follow can start.

Tasks are spawned one by one for each stage and partition.

CAUTION: FIXME What are `stageAttemptId` and `taskAttemptId`?

=== [[contract]] Task Contract

[source, scala]
----
def runTask(context: TaskContext): T
def preferredLocations: Seq[TaskLocation] = Nil
----

NOTE: `Task` is a `private[spark]` contract.

.Task Contract
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| [[runTask]] `runTask`
| Used when a <<run, task runs>>.

| [[preferredLocations]] `preferredLocations`
| Collection of link:spark-TaskLocation.adoc[TaskLocations].

Used exclusively when `TaskSetManager` link:spark-scheduler-TaskSetManager.adoc#addPendingTask[registers a task as pending execution] and link:spark-scheduler-TaskSetManager.adoc#dequeueSpeculativeTask[dequeueSpeculativeTask].

Empty by default and so no task location preferences are defined that says the task could be launched on any executor.

Defined by the custom tasks, i.e. link:spark-scheduler-ShuffleMapTask.adoc#preferredLocations[ShuffleMapTask] and link:spark-scheduler-ResultTask.adoc#preferredLocations[ResultTask].
|===

=== [[run]] Running Task Thread -- `run` Method

[source, scala]
----
run(
  taskAttemptId: Long,
  attemptNumber: Int,
  metricsSystem: MetricsSystem): T
----

`run` link:spark-BlockManager.adoc#registerTask[registers the task (identified as `taskAttemptId`) with the local `BlockManager`].

NOTE: `run` uses link:spark-SparkEnv.adoc#blockManager[`SparkEnv` to access the current `BlockManager`].

`run` link:spark-TaskContextImpl.adoc#creating-instance[creates a `TaskContextImpl`] that in turn becomes the task's link:spark-TaskContext.adoc#setTaskContext[TaskContext].

NOTE: `run` is a `final` method and so must not be overriden.

`run` checks <<_killed, _killed>> flag and, if enabled, <<kill, kills the task>> (with `interruptThread` flag disabled).

`run` creates a Hadoop `CallerContext` and sets it.

`run` <<runTask, runs the task>>.

NOTE: This is the moment when the custom ``Task``'s <<runTask, runTask>> is executed.

In the end, `run` link:spark-TaskContextImpl.adoc#markTaskCompleted[notifies `TaskContextImpl` that the task has completed] (regardless of the final outcome -- a success or a failure).

In case of any exceptions, `run` link:spark-TaskContextImpl.adoc#markTaskFailed[notifies `TaskContextImpl` that the task has failed]. `run` link:spark-MemoryStore.adoc#releaseUnrollMemoryForThisTask[requests `MemoryStore` to release unroll memory for this task] (for both `ON_HEAP` and `OFF_HEAP` memory modes).

NOTE: `run` uses link:spark-SparkEnv.adoc#blockManager[`SparkEnv` to access the current `BlockManager`] that it uses to access link:spark-BlockManager.adoc#memoryStore[MemoryStore].

`run` link:spark-MemoryManager.adoc[requests `MemoryManager` to notify any tasks waiting for execution memory to be freed to wake up and try to acquire memory again].

`run` link:spark-TaskContext.adoc#unset[unsets the task's `TaskContext`].

NOTE: `run` uses link:spark-SparkEnv.adoc#memoryManager[`SparkEnv` to access the current `MemoryManager`].

NOTE: `run` is used exclusively when `TaskRunner` is requested to <<spark-Executor-TaskRunner.adoc#run, run>> (when `Executor` is requested to <<spark-Executor.adoc#launchTask, launch a task (on "Executor task launch worker" thread pool sometime in the future)>>).

. The `Task` instance has just been deserialized from `taskBytes` that were sent over the wire to an executor. `localProperties` and link:spark-memory-TaskMemoryManager.adoc[TaskMemoryManager] are already assigned.

=== [[states]][[TaskState]] Task States

A task can be in one of the following states (as described by `TaskState` enumeration):

* `LAUNCHING`
* `RUNNING` when the task is being started.
* `FINISHED` when the task finished with the serialized result.
* `FAILED` when the task fails, e.g. when link:spark-TaskRunner-FetchFailedException.adoc[FetchFailedException], `CommitDeniedException` or any `Throwable` occurs
* `KILLED` when an executor kills a task.
* `LOST`

States are the values of `org.apache.spark.TaskState`.

NOTE: Task status updates are sent from executors to the driver through link:spark-ExecutorBackend.adoc[ExecutorBackend].

Task is finished when it is in one of `FINISHED`, `FAILED`, `KILLED`, `LOST`.

`LOST` and `FAILED` states are considered failures.

TIP: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState].

=== [[collectAccumulatorUpdates]] Collect Latest Values of (Internal and External) Accumulators -- `collectAccumulatorUpdates` Method

[source, scala]
----
collectAccumulatorUpdates(taskFailed: Boolean = false): Seq[AccumulableInfo]
----

`collectAccumulatorUpdates` collects the latest values of internal and external accumulators from a task (and returns the values as a collection of link:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo]).

Internally, `collectAccumulatorUpdates` link:spark-TaskContextImpl.adoc#taskMetrics[takes `TaskMetrics`].

NOTE: `collectAccumulatorUpdates` uses <<context, TaskContextImpl>> to access the task's `TaskMetrics`.

`collectAccumulatorUpdates` collects the latest values of:

* link:spark-executor-TaskMetrics.adoc#internalAccums[internal accumulators] whose current value is not the zero value and the `RESULT_SIZE` accumulator (regardless whether the value is its zero or not).

* link:spark-executor-TaskMetrics.adoc#externalAccums[external accumulators] when `taskFailed` is disabled (`false`) or which link:spark-accumulators.adoc#countFailedValues[should be included on failures].

`collectAccumulatorUpdates` returns an empty collection when <<context, TaskContextImpl>> is not initialized.

NOTE: `collectAccumulatorUpdates` is used when link:spark-Executor-TaskRunner.adoc#run[`TaskRunner` runs a task] (and sends a task's final results back to the driver).

=== [[kill]] Killing Task -- `kill` Method

[source, scala]
----
kill(interruptThread: Boolean)
----

`kill` marks the task to be killed, i.e. it sets the internal `_killed` flag to `true`.

`kill` calls link:spark-TaskContextImpl.adoc#markInterrupted[TaskContextImpl.markInterrupted] when `context` is set.

If `interruptThread` is enabled and the internal `taskThread` is available, `kill` interrupts it.

CAUTION: FIXME When could `context` and `interruptThread` not be set?
