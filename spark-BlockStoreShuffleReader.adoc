== [[BlockStoreShuffleReader]] BlockStoreShuffleReader

`BlockStoreShuffleReader` is the one and only link:spark-ShuffleReader.adoc[ShuffleReader] that fetches and <<read, reads the partitions>> (in range [`startPartition`, `endPartition`)) from a shuffle by requesting them from other nodes' block stores.

`BlockStoreShuffleReader` is <<creating-instance, created>> when the link:spark-SortShuffleManager.adoc#getReader[default `SortShuffleManager` is requested for a `ShuffleReader`] (for a `ShuffleHandle`).

=== [[creating-instance]] Creating BlockStoreShuffleReader Instance

`BlockStoreShuffleReader` takes:

1. link:spark-BaseShuffleHandle.adoc[BaseShuffleHandle]
2. `startPartition` and `endPartition` partition indices
3. link:spark-taskscheduler-taskcontext.adoc[TaskContext]
4. (optional) link:spark-SerializerManager.adoc[SerializerManager]
5. (optional) link:spark-blockmanager.adoc[BlockManager]
6. (optional) link:spark-service-mapoutputtracker.adoc[MapOutputTracker]

NOTE: `BlockStoreShuffleReader` uses link:spark-sparkenv.adoc[`SparkEnv` to define the optional `SerializerManager`, `BlockManager` and `MapOutputTracker`].

=== [[read]] Reading Combined Key-Value Records For Reduce Task (using ShuffleBlockFetcherIterator) -- `read` Method

[source, scala]
----
read(): Iterator[Product2[K, C]]
----

NOTE: `read` is part of link:spark-ShuffleReader.adoc[ShuffleReader contract].

Internally, `read` first link:spark-ShuffleBlockFetcherIterator.adoc#creating-instance[creates a `ShuffleBlockFetcherIterator`] (passing in the values of <<spark_reducer_maxSizeInFlight, spark.reducer.maxSizeInFlight>>, <<spark_reducer_maxReqsInFlight, spark.reducer.maxReqsInFlight>> and <<spark_shuffle_detectCorrupt, spark.shuffle.detectCorrupt>> Spark properties).

NOTE: `read` uses link:spark-blockmanager.adoc#shuffleClient[`BlockManager` to access `ShuffleClient`] to create `ShuffleBlockFetcherIterator`.

NOTE: `read` uses link:spark-service-mapoutputtracker.adoc#getMapSizesByExecutorId[`MapOutputTracker` to find the BlockManagers with the shuffle blocks and sizes] to create `ShuffleBlockFetcherIterator`.

`read` creates a new link:spark-SerializerInstance.adoc[SerializerInstance] (using link:spark-rdd-ShuffleDependency.adoc#serializer[`Serializer` from ShuffleDependency]).

`read` creates a key/value iterator by `deserializeStream` every shuffle block stream.

`read` updates the link:spark-taskscheduler-taskcontext.adoc#taskMetrics[context task metrics] for each record read.

NOTE: `read` uses `CompletionIterator` (to count the records read) and `InterruptibleIterator` (to support task cancellation).

If the link:spark-rdd-ShuffleDependency.adoc#aggregator[`ShuffleDependency` has an `Aggregator` defined], `read` wraps the current iterator inside an iterator defined by link:spark-Aggregator.adoc#combineCombinersByKey[Aggregator.combineCombinersByKey] (for link:spark-rdd-ShuffleDependency.adoc#mapSideCombine[`mapSideCombine` enabled]) or link:spark-Aggregator.adoc#combineValuesByKey[Aggregator.combineValuesByKey] otherwise.

NOTE: `run` reports an exception when link:spark-rdd-ShuffleDependency.adoc#aggregator[`ShuffleDependency` has no `Aggregator` defined] with link:spark-rdd-ShuffleDependency.adoc#mapSideCombine[`mapSideCombine` flag enabled].

For link:spark-rdd-ShuffleDependency.adoc#keyOrdering[`keyOrdering` defined in `ShuffleDependency`], `run` does the following:

1. link:spark-ExternalSorter.adoc#creating-instance[Creates an `ExternalSorter`]
2. link:spark-ExternalSorter.adoc#insertAll[Inserts all the records] into the `ExternalSorter`
3. Updates context `TaskMetrics`
4. Returns a `CompletionIterator` for the `ExternalSorter`

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_reducer_maxSizeInFlight]] `spark.reducer.maxSizeInFlight`
| `48m`
| Maximum size (in bytes) of map outputs to fetch simultaneously from each reduce task.

Since each output requires a new buffer to receive it, this represents a fixed memory overhead per reduce task, so keep it small unless you have a large amount of memory.

Used when <<read, `BlockStoreShuffleReader` creates a `ShuffleBlockFetcherIterator` to read records>>.

| [[spark_reducer_maxReqsInFlight]] `spark.reducer.maxReqsInFlight`
| (unlimited)
| The maximum number of remote requests to fetch blocks at any given point.

When the number of hosts in the cluster increases, it might lead to very large number of in-bound connections to one or more nodes, causing the workers to fail under load. By allowing it to limit the number of fetch requests, this scenario can be mitigated.

Used when <<read, `BlockStoreShuffleReader` creates a `ShuffleBlockFetcherIterator` to read records>>.

| [[spark_shuffle_detectCorrupt]] `spark.shuffle.detectCorrupt`
| `true`
| Controls whether to detect any corruption in fetched blocks.

Used when <<read, `BlockStoreShuffleReader` creates a `ShuffleBlockFetcherIterator` to read records>>.

|===
