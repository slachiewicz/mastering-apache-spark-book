== [[SparkEnv]] SparkEnv -- Spark Runtime Environment

*Spark Runtime Environment* (`SparkEnv`) is the runtime environment with Spark's public services that interact with each other to establish a distributed computing platform for a Spark application.

Spark Runtime Environment is represented by a <<SparkEnv, SparkEnv>> object that holds all the required runtime services for a running Spark application with separate environments for the <<createDriverEnv, driver>> and <<createExecutorEnv, executors>>.

The idiomatic way in Spark to access the current `SparkEnv` when on the driver or executors is to use <<get, get>> method.

[source, scala]
----
import org.apache.spark._
scala> SparkEnv.get
res0: org.apache.spark.SparkEnv = org.apache.spark.SparkEnv@49322d04
----

.`SparkEnv` Services
[cols="1,1,2",options="header",width="100%"]
|===
| Property | Service | Description
| [[rpcEnv]] rpcEnv | link:spark-rpc.adoc[RpcEnv] |
| [[serializer]] serializer | Serializer |
| [[closureSerializer]] closureSerializer | link:spark-Serializer.adoc[Serializer] |
| [[serializerManager]] serializerManager | SerializerManager |
| [[mapOutputTracker]] <<MapOutputTracker, mapOutputTracker>> | link:spark-service-mapoutputtracker.adoc[MapOutputTracker] |
| [[shuffleManager]] <<ShuffleManager, shuffleManager>> | link:spark-ShuffleManager.adoc[ShuffleManager] |
| [[broadcastManager]] broadcastManager | BroadcastManager |
| [[blockManager]] <<BlockManager, blockManager>> | link:spark-blockmanager.adoc[BlockManager] |
| securityManager | SecurityManager |
| [[metricsSystem]] metricsSystem | link:spark-MetricsSystem.adoc[MetricsSystem] |
| [[memoryManager]] memoryManager | link:spark-MemoryManager.adoc[MemoryManager] |
| outputCommitCoordinator | OutputCommitCoordinator |
| [[conf]] conf | link:spark-SparkConf.adoc[SparkConf]
|===

[[internal-properties]]
.SparkEnv's Internal Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| [[isStopped]] `isStopped`
| Disabled, i.e. `false`
| Used to mark `SparkEnv` stopped. FIXME

| [[driverTmpDir]] `driverTmpDir`
|
|

|===

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.SparkEnv` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.SparkEnv=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[SparkEnv]] `SparkEnv` Factory Object

=== [[create]] Creating "Base" `SparkEnv` -- `create` Method

[source, scala]
----
create(
  conf: SparkConf,
  executorId: String,
  hostname: String,
  port: Int,
  isDriver: Boolean,
  isLocal: Boolean,
  numUsableCores: Int,
  listenerBus: LiveListenerBus = null,
  mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv
----

`create` is a internal helper method to create a "base" `SparkEnv` regardless of the target environment, i.e. a driver or an executor.

.``create``'s Input Arguments and Their Usage
[cols="1,2",options="header",width="100%"]
|===
| Input Argument | Usage
| `bindAddress` | Used to create link:spark-rpc.adoc[RpcEnv] and link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService].

| `advertiseAddress` | Used to create link:spark-rpc.adoc[RpcEnv] and link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService].

| `numUsableCores` | Used to create link:spark-MemoryManager.adoc[MemoryManager], link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService] and link:spark-blockmanager.adoc#creating-instance[BlockManager].
|===

When executed, `create` creates a `Serializer` (based on <<spark_serializer, spark.serializer>> setting). You should see the following `DEBUG` message in the logs:

```
DEBUG SparkEnv: Using serializer: [serializer]
```

It creates another `Serializer` (based on <<spark_closure_serializer, spark.closure.serializer>>).

[[ShuffleManager]]
It creates a link:spark-ShuffleManager.adoc[ShuffleManager] based on link:spark-ShuffleManager.adoc#spark_shuffle_manager[spark.shuffle.manager] Spark property.

[[MemoryManager]]
It creates a link:spark-MemoryManager.adoc[MemoryManager] based on <<spark_memory_useLegacyMode, spark.memory.useLegacyMode>> setting (with link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] being the default and `numCores` the input `numUsableCores`).

[[NettyBlockTransferService]]
`create` creates a link:spark-NettyBlockTransferService.adoc#creating-instance[NettyBlockTransferService]. It uses link:spark-driver.adoc#spark_driver_blockManager_port[`spark.driver.blockManager.port` for the port on the driver] and link:spark-blockmanager.adoc#spark_blockManager_port[`spark.blockManager.port` for the port on executors].

CAUTION: FIXME A picture with `SparkEnv`, `NettyBlockTransferService` and the ports "armed".

[[BlockManagerMaster]]
`create` creates a link:spark-BlockManagerMaster.adoc#creating-instance[BlockManagerMaster] object with the `BlockManagerMaster` RPC endpoint reference (by <<registerOrLookupEndpoint, registering or looking it up by name>> and link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMasterEndpoint]), the input link:spark-SparkConf.adoc[SparkConf], and the input `isDriver` flag.

.Creating BlockManager for the Driver
image::images/sparkenv-driver-blockmanager.png[align="center"]

NOTE: `create` registers the *BlockManagerMaster* RPC endpoint for the driver and looks it up for executors.

.Creating BlockManager for Executor
image::images/sparkenv-executor-blockmanager.png[align="center"]

[[BlockManager]]
It creates a link:spark-blockmanager.adoc#creating-instance[BlockManager] (using the above <<BlockManagerMaster, BlockManagerMaster>>, <<NettyBlockTransferService, NettyBlockTransferService>> and other services).

`create` creates a link:spark-service-broadcastmanager.adoc[BroadcastManager].

[[MapOutputTracker]]
`create` creates a link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] or link:spark-service-MapOutputTrackerWorker.adoc[MapOutputTrackerWorker] for the driver and executors, respectively.

NOTE: The choice of the real implementation of link:spark-service-mapoutputtracker.adoc[MapOutputTracker] is based on whether the input `executorId` is *driver* or not.

[[MapOutputTrackerMasterEndpoint]]
`create` <<registerOrLookupEndpoint, registers or looks up `RpcEndpoint`>> as *MapOutputTracker*. It registers link:spark-service-MapOutputTrackerMasterEndpoint.adoc[MapOutputTrackerMasterEndpoint] on the driver and creates a RPC endpoint reference on executors. The RPC endpoint reference gets assigned as the link:spark-service-mapoutputtracker.adoc#trackerEndpoint[MapOutputTracker RPC endpoint].

CAUTION: FIXME

It creates a CacheManager.

It creates a MetricsSystem for a driver and a worker separately.

It initializes `userFiles` temporary directory used for downloading dependencies for a driver while this is the executor's current working directory for an executor.

An OutputCommitCoordinator is created.

NOTE: `create` is called by <<createDriverEnv, createDriverEnv>> and <<createExecutorEnv, createExecutorEnv>>.

=== [[registerOrLookupEndpoint]] Registering or Looking up RPC Endpoint by Name -- `registerOrLookupEndpoint` Method

[source, scala]
----
registerOrLookupEndpoint(name: String, endpointCreator: => RpcEndpoint)
----

`registerOrLookupEndpoint` registers or looks up a RPC endpoint by `name`.

If called from the driver, you should see the following INFO message in the logs:

```
INFO SparkEnv: Registering [name]
```

And the RPC endpoint is registered in the RPC environment.

Otherwise, it obtains a RPC endpoint reference by `name`.

=== [[createDriverEnv]] Creating SparkEnv for Driver -- `createDriverEnv` Method

[source, scala]
----
createDriverEnv(
  conf: SparkConf,
  isLocal: Boolean,
  listenerBus: LiveListenerBus,
  numCores: Int,
  mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv
----

`createDriverEnv` creates a `SparkEnv` execution environment for the driver.

.Spark Environment for driver
image::images/sparkenv-driver.png[align="center"]

`createDriverEnv` accepts an instance of link:spark-SparkConf.adoc[SparkConf], link:spark-deployment-environments.adoc[whether it runs in local mode or not], link:spark-LiveListenerBus.adoc[LiveListenerBus], the number of cores to use for execution in local mode or `0` otherwise, and a link:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator] (default: none).

`createDriverEnv` ensures that link:spark-driver.adoc#spark_driver_host[spark.driver.host] and link:spark-driver.adoc#spark_driver_port[spark.driver.port] settings are defined.

It then passes the call straight on to the <<create, create helper method>> (with `driver` executor id, `isDriver` enabled, and the input parameters).

NOTE: `createDriverEnv` is exclusively used by link:spark-sparkcontext-creating-instance-internals.adoc#createSparkEnv[SparkContext to create a `SparkEnv`] (while a link:spark-SparkContext.adoc#creating-instance[SparkContext is being created for the driver]).

=== [[createExecutorEnv]] Creating SparkEnv for Executor -- `createExecutorEnv` Method

[source, scala]
----
createExecutorEnv(
  conf: SparkConf,
  executorId: String,
  hostname: String,
  port: Int,
  numCores: Int,
  ioEncryptionKey: Option[Array[Byte]],
  isLocal: Boolean): SparkEnv
----

`createExecutorEnv` creates an *executor's (execution) environment* that is the Spark execution environment for an executor.

.Spark Environment for executor
image::images/sparkenv-executor.png[align="center"]

NOTE: `createExecutorEnv` is a `private[spark]` method.

`createExecutorEnv` simply <<create, creates the base `SparkEnv`>> (passing in all the input parameters) and <<set, sets it as the current `SparkEnv`>>.

NOTE: The number of cores `numCores` is configured using `--cores` command-line option of `CoarseGrainedExecutorBackend` and is specific to a cluster manager.

NOTE: `createExecutorEnv` is used when link:spark-CoarseGrainedExecutorBackend.adoc#run[`CoarseGrainedExecutorBackend` runs] and link:spark-executor-backends-MesosExecutorBackend.adoc#registered[`MesosExecutorBackend` registers a Spark executor].

=== [[get]] Getting Current SparkEnv -- `get` Method

[source, scala]
----
get: SparkEnv
----

`get` returns the current `SparkEnv`.

[source, scala]
----
import org.apache.spark._
scala> SparkEnv.get
res0: org.apache.spark.SparkEnv = org.apache.spark.SparkEnv@49322d04
----

=== [[stop]] Stopping SparkEnv -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` checks <<isStopped, isStopped>> internal flag and does nothing when enabled.

NOTE: `stop` is a `private[spark]` method.

Otherwise, `stop` turns `isStopped` flag on, stops all `pythonWorkers` and requests the following services to stop:

1. link:spark-service-mapoutputtracker.adoc#stop[MapOutputTracker]
2. link:spark-ShuffleManager.adoc#stop[ShuffleManager]
3. link:spark-service-broadcastmanager.adoc#stop[BroadcastManager]
4. link:spark-blockmanager.adoc#stop[BlockManager]
5. link:spark-BlockManagerMaster.adoc#stop[BlockManagerMaster]
6. link:spark-MetricsSystem.adoc#stop[MetricsSystem]
7. link:spark-service-outputcommitcoordinator.adoc#stop[OutputCommitCoordinator]

`stop` link:spark-rpc.adoc#shutdown[requests `RpcEnv` to shut down] and link:spark-rpc.adoc#awaitTermination[waits till it terminates].

Only on the driver, `stop` deletes the <<driverTmpDir, temporary directory>>. You can see the following WARN message in the logs if the deletion fails.

```
WARN Exception while deleting Spark temp dir: [path]
```

NOTE: `stop` is used when link:spark-SparkContext.adoc#stop[`SparkContext` stops] (on the driver) and link:spark-Executor.adoc#stop[`Executor` stops].

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property | Default Value | Description

| [[spark_serializer]] `spark.serializer`
| `org.apache.spark.serializer.JavaSerializer`
| link:spark-Serializer.adoc[Serializer]

TIP: Enable DEBUG logging level for `org.apache.spark.SparkEnv` logger to see the current value.

```
DEBUG SparkEnv: Using serializer: [serializer]
```

| [[spark_closure_serializer]] `spark.closure.serializer`
| `org.apache.spark.serializer.JavaSerializer`
| link:spark-Serializer.adoc[Serializer]

| [[spark_memory_useLegacyMode]] `spark.memory.useLegacyMode`
| `false`
| Controls what type of the link:spark-MemoryManager.adoc[MemoryManager] to use. When enabled (i.e. `true`) it is the legacy `StaticMemoryManager` while link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] otherwise (i.e. `false`).

|===
