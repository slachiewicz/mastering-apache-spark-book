== [[MapOutputTrackerMaster]] MapOutputTrackerMaster -- MapOutputTracker For Driver

`MapOutputTrackerMaster` is the link:spark-service-mapoutputtracker.adoc[MapOutputTracker] for the driver.

A `MapOutputTrackerMaster` is the source of truth for link:spark-MapStatus.adoc[MapStatus] objects (map output locations) per shuffle id (as recorded from link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTasks]).

NOTE: `MapOutputTrackerMaster` uses Java's thread-safe https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentHashMap.html[java.util.concurrent.ConcurrentHashMap] for link:spark-service-mapoutputtracker.adoc#mapStatuses[`mapStatuses` internal cache].

NOTE: There is currently a hardcoded limit of map and reduce tasks above which Spark does not assign preferred locations aka locality preferences based on map output sizes -- `1000` for map and reduce each.

`MapOutputTrackerMaster` uses `MetadataCleaner` with `MetadataCleanerType.MAP_OUTPUT_TRACKER` as `cleanerType` and <<cleanup, cleanup>> function to drop entries in `mapStatuses`.

[[internal-registries]]
.MapOutputTrackerMaster Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[cachedSerializedBroadcast]] `cachedSerializedBroadcast`
| Internal registry of...FIXME

Used when...FIXME

| [[cachedSerializedStatuses]] `cachedSerializedStatuses`
| Internal registry of serialized link:spark-MapStatus.adoc[shuffle map output statuses] (as `Array[Byte]`) per...FIXME

Used when...FIXME

| [[cacheEpoch]] `cacheEpoch`
| Internal registry with...FIXME

Used when...FIXME

| [[shuffleIdLocks]] `shuffleIdLocks`
| Internal registry of locks for shuffle ids.

Used when...FIXME

| [[mapOutputRequests]] `mapOutputRequests`
| Internal queue with `GetMapOutputMessage` requests for map output statuses.

Used when `MapOutputTrackerMaster` <<post, posts `GetMapOutputMessage` messages to>> and <<run, take one head element off this queue>>.

NOTE: `mapOutputRequests` uses Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/LinkedBlockingQueue.html[java.util.concurrent.LinkedBlockingQueue].

|===

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.MapOutputTrackerMaster` logger to see what happens in `MapOutputTrackerMaster`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.MapOutputTrackerMaster=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[removeBroadcast]] `removeBroadcast` Method

CAUTION: FIXME

=== [[clearCachedBroadcast]] `clearCachedBroadcast` Method

CAUTION: FIXME

=== [[post]] `post` Method

CAUTION: FIXME

=== [[stop]] `stop` Method

CAUTION: FIXME

=== [[unregisterMapOutput]] `unregisterMapOutput` Method

CAUTION: FIXME

=== [[cleanup]] cleanup Function for MetadataCleaner

`cleanup(cleanupTime: Long)` method removes old entries in `mapStatuses` and `cachedSerializedStatuses` that have timestamp earlier than `cleanupTime`.

It uses `org.apache.spark.util.TimeStampedHashMap.clearOldValues` method.

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.util.TimeStampedHashMap` logger to see what happens in TimeStampedHashMap.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.util.TimeStampedHashMap=DEBUG
```
====

You should see the following DEBUG message in the logs for entries being removed:

```
DEBUG Removing key [entry.getKey]
```

=== [[creating-instance]] Creating MapOutputTrackerMaster Instance

`MapOutputTrackerMaster` takes the following when created:

1. link:spark-SparkConf.adoc[SparkConf]
2. `broadcastManager` -- link:spark-service-broadcastmanager.adoc[BroadcastManager]
3. `isLocal` -- flag to control whether `MapOutputTrackerMaster` runs in local or on a cluster.

`MapOutputTrackerMaster` initializes the <<internal-registries, internal registries and counters>> and <<threadpool, starts map-output-dispatcher threads>>.

NOTE: `MapOutputTrackerMaster` is created when link:spark-SparkEnv.adoc#MapOutputTracker[`SparkEnv` is created].

=== [[threadpool]] `threadpool` Thread Pool with `map-output-dispatcher` Threads

[source, scala]
----
threadpool: ThreadPoolExecutor
----

`threadpool` is a daemon fixed thread pool registered with *map-output-dispatcher* thread name prefix.

`threadpool` uses <<spark_shuffle_mapOutput_dispatcher_numThreads, spark.shuffle.mapOutput.dispatcher.numThreads>> (default: `8`) for the number of <<MessageLoop, `MessageLoop` dispatcher threads>> to process received `GetMapOutputMessage` messages.

NOTE: The dispatcher threads are started immediately when <<creating-instance, `MapOutputTrackerMaster` is created>>.

NOTE: `threadpool` is shut down when <<stop, `MapOutputTrackerMaster` stops>>.

=== [[getPreferredLocationsForShuffle]] Finding Preferred BlockManagers with Most Shuffle Map Outputs (For ShuffleDependency and Partition) -- `getPreferredLocationsForShuffle` Method

[source, scala]
----
getPreferredLocationsForShuffle(dep: ShuffleDependency[_, _, _], partitionId: Int): Seq[String]
----

`getPreferredLocationsForShuffle` finds the locations (i.e. link:spark-BlockManager.adoc[BlockManagers]) with the most map outputs for the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and link:spark-rdd-Partition.adoc[Partition].

NOTE: `getPreferredLocationsForShuffle` is simply <<getLocationsWithLargestOutputs, getLocationsWithLargestOutputs>> with a guard condition.

Internally, `getPreferredLocationsForShuffle` checks whether <<spark_shuffle_reduceLocality_enabled, `spark.shuffle.reduceLocality.enabled` Spark property>> is enabled (it is by default) with the number of partitions of the link:spark-rdd-ShuffleDependency.adoc#rdd[RDD of the input `ShuffleDependency`] and partitions in the link:spark-rdd-ShuffleDependency.adoc#partitioner[partitioner of the input `ShuffleDependency`] both being less than `1000`.

NOTE: The thresholds for the number of partitions in the RDD and of the partitioner when computing the preferred locations are `1000` and are not configurable.

If the condition holds, `getPreferredLocationsForShuffle` <<getLocationsWithLargestOutputs, finds locations with the largest number of shuffle map outputs>> for the input `ShuffleDependency` and `partitionId` (with the number of partitions in the partitioner of the input `ShuffleDependency` and `0.2`) and returns the hosts of the preferred `BlockManagers`.

NOTE: `0.2` is the fraction of total map output that must be at a location to be considered as a preferred location for a reduce task. It is not configurable.

NOTE: `getPreferredLocationsForShuffle` is used when link:spark-rdd-ShuffledRDD.adoc#getPreferredLocations[ShuffledRDD] and link:spark-sql-ShuffledRowRDD.adoc#getPreferredLocations[ShuffledRowRDD] ask for preferred locations for a partition.

=== [[incrementEpoch]] Incrementing Epoch -- `incrementEpoch` Method

[source, scala]
----
incrementEpoch(): Unit
----

`incrementEpoch` increments the internal link:spark-service-mapoutputtracker.adoc#epoch[epoch].

You should see the following DEBUG message in the logs:

```
DEBUG MapOutputTrackerMaster: Increasing epoch to [epoch]
```

NOTE: `incrementEpoch` is used when `MapOutputTrackerMaster` <<registerMapOutputs, registers map outputs>> (with `changeEpoch` flag enabled -- it is disabled by default) and <<unregisterMapOutput, unregisters map outputs>> (for a shuffle, mapper and block manager), and when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` is notified that an executor got lost] (with `filesLost` flag enabled).

=== [[getLocationsWithLargestOutputs]] Finding Locations with Largest Number of Shuffle Map Outputs -- `getLocationsWithLargestOutputs` Method

[source, scala]
----
getLocationsWithLargestOutputs(
  shuffleId: Int,
  reducerId: Int,
  numReducers: Int,
  fractionThreshold: Double): Option[Array[BlockManagerId]]
----

`getLocationsWithLargestOutputs` returns link:spark-BlockManager.adoc#BlockManagerId[BlockManagerId]s with the largest size (of all the shuffle blocks they manage) above the input `fractionThreshold` (given the total size of all the shuffle blocks for the shuffle across all link:spark-BlockManager.adoc[BlockManagers]).

NOTE: `getLocationsWithLargestOutputs` may return no `BlockManagerId` if their shuffle blocks do not total up above the input `fractionThreshold`.

NOTE: The input `numReducers` is not used.

Internally, `getLocationsWithLargestOutputs` queries the <<mapStatuses, mapStatuses>> internal cache for the input `shuffleId`.

[NOTE]
====
One entry in `mapStatuses` internal cache is a link:spark-MapStatus.adoc[MapStatus] array indexed by partition id.

`MapStatus` includes link:spark-MapStatus.adoc#contract[information about the `BlockManager` (as `BlockManagerId`) and estimated size of the reduce blocks].
====

`getLocationsWithLargestOutputs` iterates over the `MapStatus` array and builds an interim mapping between link:spark-BlockManager.adoc#BlockManagerId[BlockManagerId] and the cumulative sum of shuffle blocks across link:spark-BlockManager.adoc[BlockManagers].

NOTE: `getLocationsWithLargestOutputs` is used exclusively when <<getPreferredLocationsForShuffle, `MapOutputTrackerMaster` finds the preferred locations (BlockManagers and hence executors) for a shuffle>>.

=== [[containsShuffle]] Requesting Tracking Status of Shuffle Map Output -- `containsShuffle` Method

[source, scala]
----
containsShuffle(shuffleId: Int): Boolean
----

`containsShuffle` checks if the input `shuffleId` is registered in the <<cachedSerializedStatuses, cachedSerializedStatuses>> or link:spark-service-mapoutputtracker.adoc#mapStatuses[mapStatuses] internal caches.

NOTE: `containsShuffle` is used exclusively when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage`] (for link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and link:spark-dagscheduler-jobs.adoc[ActiveJob]).

=== [[registerShuffle]] Registering ShuffleDependency -- `registerShuffle` Method

[source, scala]
----
registerShuffle(shuffleId: Int, numMaps: Int): Unit
----

`registerShuffle` registers the input `shuffleId` in the link:spark-service-mapoutputtracker.adoc#mapStatuses[mapStatuses] internal cache.

NOTE: The number of link:spark-MapStatus.adoc[MapStatus] entries in the new array in `mapStatuses` internal cache is exactly the input `numMaps`.

`registerShuffle` adds a lock in the <<shuffleIdLocks, `shuffleIdLocks` internal registry>> (without using it).

If the `shuffleId` has already been registered, `registerShuffle` throws a `IllegalArgumentException` with the following message:

```
Shuffle ID [id] registered twice
```

NOTE: `registerShuffle` is used exclusively when link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage`] (for link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and link:spark-dagscheduler-jobs.adoc[ActiveJob]).

=== [[registerMapOutputs]] Registering Map Outputs for Shuffle (Possibly with Epoch Change) -- `registerMapOutputs` Method

[source, scala]
----
registerMapOutputs(
  shuffleId: Int,
  statuses: Array[MapStatus],
  changeEpoch: Boolean = false): Unit
----

`registerMapOutputs` registers the input `statuses` (as the shuffle map output) with the input `shuffleId` in the link:spark-service-mapoutputtracker.adoc#mapStatuses[mapStatuses] internal cache.

`registerMapOutputs` <<incrementEpoch, increments epoch>> if the input `changeEpoch` is enabled (it is not by default).

[NOTE]
====
`registerMapOutputs` is used when `DAGScheduler` handles link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleTaskCompletion-Success-ShuffleMapTask[successful `ShuffleMapTask` completion] and link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[executor lost events].

In both cases, the input `changeEpoch` is enabled.
====

=== [[getSerializedMapOutputStatuses]] Finding Serialized Map Output Statuses (And Possibly Broadcasting Them) -- `getSerializedMapOutputStatuses` Method

[source, scala]
----
getSerializedMapOutputStatuses(shuffleId: Int): Array[Byte]
----

`getSerializedMapOutputStatuses` <<checkCachedStatuses, finds cached serialized map statuses>> for the input `shuffleId`.

If found, `getSerializedMapOutputStatuses` returns the cached serialized map statuses.

Otherwise, `getSerializedMapOutputStatuses` acquires the <<shuffleIdLocks, shuffle lock>> for `shuffleId` and <<checkCachedStatuses, finds cached serialized map statuses>> again since some other thread could not update the <<cachedSerializedStatuses, cachedSerializedStatuses>> internal cache.

`getSerializedMapOutputStatuses` returns the serialized map statuses if found.

If not, `getSerializedMapOutputStatuses` link:spark-service-mapoutputtracker.adoc#serializeMapStatuses[serializes the local array of `MapStatuses`] (from <<checkCachedStatuses, checkCachedStatuses>>).

You should see the following INFO message in the logs:

```
INFO Size of output statuses for shuffle [shuffleId] is [bytes] bytes
```

`getSerializedMapOutputStatuses` saves the serialized map output statuses in <<cachedSerializedStatuses, cachedSerializedStatuses>> internal cache if the <<epoch, epoch>> has not changed in the meantime. `getSerializedMapOutputStatuses` also saves its broadcast version in <<cachedSerializedBroadcast, cachedSerializedBroadcast>> internal cache.

If the <<epoch, epoch>> has changed in the meantime, the serialized map output statuses and their broadcast version are not saved, and you should see the following INFO message in the logs:

```
INFO Epoch changed, not caching!
```

`getSerializedMapOutputStatuses` <<removeBroadcast, removes the broadcast>>.

`getSerializedMapOutputStatuses` returns the serialized map statuses.

NOTE: `getSerializedMapOutputStatuses` is used when <<MessageLoop, `MapOutputTrackerMaster` responds to `GetMapOutputMessage` requests>> and link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates `ShuffleMapStage` for `ShuffleDependency`] (copying the shuffle map output locations from previous jobs to avoid unnecessarily regenerating data).

==== [[checkCachedStatuses]] Finding Cached Serialized Map Statuses -- `checkCachedStatuses` Internal Method

[source, scala]
----
checkCachedStatuses(): Boolean
----

`checkCachedStatuses` is an internal helper method that <<getSerializedMapOutputStatuses, getSerializedMapOutputStatuses>> uses to do some bookkeeping (when the <<epoch, epoch>> and <<cacheEpoch, cacheEpoch>> differ) and set local `statuses`, `retBytes` and `epochGotten` (that `getSerializedMapOutputStatuses` uses).

Internally, `checkCachedStatuses` acquires the link:spark-service-mapoutputtracker.adoc#epochLock[`epochLock` lock] and checks the status of <<epoch, epoch>> to <<cacheEpoch, cached `cacheEpoch`>>.

If `epoch` is younger (i.e. greater), `checkCachedStatuses` clears <<cachedSerializedStatuses, cachedSerializedStatuses>> internal cache, <<clearCachedBroadcast, cached broadcasts>> and sets `cacheEpoch` to be `epoch`.

`checkCachedStatuses` gets the serialized map output statuses for the `shuffleId` (of the owning <<getSerializedMapOutputStatuses, getSerializedMapOutputStatuses>>).

When the serialized map output status is found, `checkCachedStatuses` saves it in a local `retBytes` and returns `true`.

When not found, you should see the following DEBUG message in the logs:

```
DEBUG cached status not found for : [shuffleId]
```

`checkCachedStatuses` uses link:spark-service-mapoutputtracker.adoc#mapStatuses[mapStatuses] internal cache to get map output statuses for the `shuffleId` (of the owning <<getSerializedMapOutputStatuses, getSerializedMapOutputStatuses>>) or falls back to an empty array and sets it to a local `statuses`. `checkCachedStatuses` sets the local `epochGotten` to the current <<epoch, epoch>> and returns `false`.

=== [[MessageLoop]][[run]] `MessageLoop` Dispatcher Thread

`MessageLoop` is a dispatcher thread that, once started, runs indefinitely until <<PoisonPill, PoisonPill>> arrives.

`MessageLoop` takes `GetMapOutputMessage` messages off <<mapOutputRequests, mapOutputRequests>> internal queue (waiting if necessary until a message becomes available).

Unless `PoisonPill` is processed, you should see the following DEBUG message in the logs:

```
DEBUG Handling request to send map output locations for shuffle [shuffleId] to [hostPort]
```

`MessageLoop` replies back with <<getSerializedMapOutputStatuses, serialized map output statuses for the `shuffleId`>> (from the incoming `GetMapOutputMessage` message).

NOTE: `MessageLoop` is created and executed immediately when <<creating-instance, `MapOutputTrackerMaster` is created>>.

=== [[PoisonPill]] PoisonPill Message

`PoisonPill` is a `GetMapOutputMessage` (with `-99` as `shuffleId`) that indicates that <<MessageLoop, MessageLoop>> should exit its message loop.

`PoisonPill` is posted when <<stop, `MapOutputTrackerMaster` stops>>.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_shuffle_mapOutput_dispatcher_numThreads]] `spark.shuffle.mapOutput.dispatcher.numThreads`
| `8`
| FIXME

| [[spark_shuffle_mapOutput_minSizeForBroadcast]] `spark.shuffle.mapOutput.minSizeForBroadcast`
| `512k`
| FIXME

| [[spark_shuffle_reduceLocality_enabled]] `spark.shuffle.reduceLocality.enabled`
| `true`
| Controls whether to compute locality preferences for reduce tasks.

When enabled (i.e. `true`), `MapOutputTrackerMaster` computes the preferred hosts on which to run a given map output partition in a given shuffle, i.e. the nodes that the most outputs for that partition are on.
|===
