== [[KafkaSourceProvider]] KafkaSourceProvider

`KafkaSourceProvider` is an link:spark-sql-DataSourceRegister.adoc[interface to register] Apache Kafka as a data source.

`KafkaSourceProvider` is a link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider] and link:spark-sql-RelationProvider.adoc[RelationProvider].

`KafkaSourceProvider` is registered under `kafka` alias.

[source, scala]
----
// start Spark application like spark-shell with the following package
// --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT
scala> val fromKafkaTopic1 = spark.
  read.
  format("kafka").
  option("subscribe", "topic1").  // subscribe, subscribepattern, or assign
  option("kafka.bootstrap.servers", "localhost:9092").
  load("gauge_one")
----

`KafkaSourceProvider` <<sourceSchema, uses a fixed schema>> (and makes sure that a user did not set a custom one).

[source, scala]
----
import org.apache.spark.sql.types.StructType
val schema = new StructType().add($"id".int)
scala> spark
  .read
  .format("kafka")
  .option("subscribe", "topic1")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .schema(schema) // <-- defining a custom schema is not supported
  .load
org.apache.spark.sql.AnalysisException: kafka does not allow user-specified schemas.;
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:307)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)
  ... 48 elided
----

=== [[createRelation-RelationProvider]] Creating KafkaRelation -- `createRelation` Method (from RelationProvider)

[source, scala]
----
createRelation(
  sqlContext: SQLContext,
  parameters: Map[String, String]): BaseRelation
----

NOTE: `createRelation` is a part of link:spark-sql-RelationProvider.adoc#createRelation[RelationProvider Contract] used exclusively when `DataSource` is requested to create a `BaseRelation` (`resolveRelation` method) for reading or writing.

`createRelation` creates a link:spark-sql-streaming-KafkaRelation.adoc[KafkaRelation].

Internally, `createRelation` first <<validateBatchOptions, validates batch options>> and collects all the *kafka.*-prefixed parameters.

`createRelation` then creates a `KafkaOffsetRangeLimit` per `startingoffsets` option with `EarliestOffsetRangeLimit` being the default.

NOTE: `startingoffsets` can be `latest`, `earliest`, a JSON or undefined.

NOTE: `createRelation` asserts that `startingoffsets` is not `latest`.

`createRelation` creates a `KafkaOffsetRangeLimit` per `endingoffsets` option with `LatestOffsetRangeLimit` being the default.

NOTE: `createRelation` asserts that `endingoffsets` is not `earliest`.

In the end, `createRelation` creates a link:spark-sql-streaming-KafkaRelation.adoc#creating-instance[KafkaRelation] with the subscription strategy (per `assign`, `subscribe`, `subscribepattern` options), with `failondataloss` option, and the parameters and offsets (as calculated above).

=== [[validateBatchOptions]] `validateBatchOptions` Internal Method

[source, scala]
----
validateBatchOptions(caseInsensitiveParams: Map[String, String]): Unit
----

`validateBatchOptions`...FIXME

NOTE: `validateBatchOptions` is used exclusively when `KafkaSourceProvider` is requested to <<createRelation, create a KafkaRelation>>.

=== [[createRelation-CreatableRelationProvider]] `createRelation` Method (from CreatableRelationProvider)

[source, scala]
----
createRelation(
  sqlContext: SQLContext,
  mode: SaveMode,
  parameters: Map[String, String],
  df: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `createRelation` is a part of link:spark-sql-CreatableRelationProvider.adoc#contract[CreatableRelationProvider Contract].

=== [[createSource]] `createSource` Method

[source, scala]
----
createSource(
  sqlContext: SQLContext,
  metadataPath: String,
  schema: Option[StructType],
  providerName: String,
  parameters: Map[String, String]): Source
----

CAUTION: FIXME

NOTE: `createSource` is a part of Structured Streaming's `StreamSourceProvider` Contract.

=== [[sourceSchema]] `sourceSchema` Method

[source, scala]
----
sourceSchema(
  sqlContext: SQLContext,
  schema: Option[StructType],
  providerName: String,
  parameters: Map[String, String]): (String, StructType)
----

CAUTION: FIXME

[source, scala]
----
val fromKafka = spark.read.format("kafka")...
scala> fromKafka.printSchema
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
----

NOTE: `sourceSchema` is a part of Structured Streaming's `StreamSourceProvider` Contract.
