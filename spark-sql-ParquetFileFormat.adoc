== [[ParquetFileFormat]] ParquetFileFormat

[[shortName]]
`ParquetFileFormat` is a link:spark-sql-FileFormat.adoc[FileFormat] for *parquet* data source format (i.e. link:spark-sql-DataSourceRegister.adoc#shortName[registers itself to handle files in parquet format] and convert them to Spark SQL rows).

NOTE: `parquet` is the link:spark-sql-DataFrameReader.adoc#source[default data source format] in Spark SQL.

[source, scala]
----
// All the following queries are equivalent
// schema has to be specified manually
import org.apache.spark.sql.types.StructType
val schema = StructType($"id".int :: Nil)

spark.read.schema(schema).format("parquet").load("parquet-datasets")
spark.read.schema(schema).parquet("parquet-datasets")
spark.read.schema(schema).load("parquet-datasets")
----

[[isSplitable]]
`ParquetFileFormat` is splitable, i.e. FIXME

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[prepareWrite]] `prepareWrite` Method

[source, scala]
----
prepareWrite(
  sparkSession: SparkSession,
  job: Job,
  options: Map[String, String],
  dataSchema: StructType): OutputWriterFactory
----

NOTE: `prepareWrite` is a part of link:spark-sql-FileFormat.adoc#prepareWrite[FileFormat Contract] to...FIXME.

`prepareWrite`...FIXME

=== [[inferSchema]] `inferSchema` Method

[source, scala]
----
inferSchema(
  sparkSession: SparkSession,
  parameters: Map[String, String],
  files: Seq[FileStatus]): Option[StructType]
----

NOTE: `inferSchema` is a part of link:spark-sql-FileFormat.adoc#inferSchema[FileFormat Contract] to...FIXME.

`inferSchema`...FIXME

=== [[buildReaderWithPartitionValues]] `buildReaderWithPartitionValues` Method

[source, scala]
----
buildReaderWithPartitionValues(
  sparkSession: SparkSession,
  dataSchema: StructType,
  partitionSchema: StructType,
  requiredSchema: StructType,
  filters: Seq[Filter],
  options: Map[String, String],
  hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow]
----

NOTE: `buildReaderWithPartitionValues` is a part of link:spark-sql-FileFormat.adoc#buildReaderWithPartitionValues[FileFormat Contract] to...FIXME.

`buildReaderWithPartitionValues`...FIXME
