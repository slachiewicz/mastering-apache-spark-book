== [[SparkSession]] SparkSession -- The Entry Point to Spark SQL

`SparkSession` is the entry point to Spark SQL. It is the very first object you have to create while developing Spark SQL applications using the fully-typed link:spark-sql-Dataset.adoc[Dataset] (or untyped ``Row``-based link:spark-sql-DataFrame.adoc[DataFrame]) data abstractions.

NOTE: `SparkSession` has merged link:spark-sql-SQLContext.adoc[SQLContext] and link:spark-sql-hive-integration.adoc[HiveContext] in one object in Spark *2.0*.

You use the <<builder, SparkSession.builder>> method to create an instance of `SparkSession`.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .getOrCreate
----

And stop the current `SparkSession` using <<stop, stop>> method.

[source, scala]
----
spark.stop
----

You can have as many `SparkSessions` as you want in a single Spark application. The common use case is to keep relational entities separate per `SparkSession` (see <<catalog, catalog>> attribute).

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

Internally, `SparkSession` requires a link:spark-SparkContext.adoc[SparkContext] and an optional link:spark-sql-SparkSession-SharedState.adoc[SharedState] (that represents the shared state across `SparkSession` instances).

[[methods]]
.SparkSession's Class and Instance Methods
[cols="1,2",options="header",width="100%"]
|===
| Method | Description
| <<builder, builder>> | "Opens" a builder to get or create a `SparkSession` instance
| <<version, version>> | Returns the current version of Spark.
| <<implicits, implicits>> | Use `import spark.implicits._` to import the implicits conversions and create `Datasets` from (almost arbitrary) Scala objects.
| <<emptyDataset, emptyDataset[T]>> | Creates an empty `Dataset[T]`.
| <<range, range>> | Creates a `Dataset[Long]`.
| <<sql, sql>> | Executes a SQL query (and returns a `DataFrame`).
| <<udf, udf>> | Access to user-defined functions (UDFs).
| <<table, table>> | Creates a `DataFrame` from a table.
| <<catalog, catalog>> | Access to the catalog of the entities of structured queries
| <<read, read>> | Access to `DataFrameReader` to read a `DataFrame` from external files and storage systems.
| <<conf, conf>> | Access to the current runtime configuration.
| <<readStream, readStream>> | Access to `DataStreamReader` to read streaming datasets.
| <<streams, streams>> | Access to `StreamingQueryManager` to manage structured streaming queries.
| <<newSession, newSession>> | Creates a new `SparkSession`.
| <<stop, stop>> | Stops the `SparkSession`.
|===

[TIP]
====
Use link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir] Spark property to change the location of Hive's `hive.metastore.warehouse.dir` property, i.e. the location of the Hive local/embedded metastore database (using Derby).

Refer to link:spark-sql-SparkSession-SharedState.adoc[SharedState] to learn about (the low-level details of) Spark SQL support for Apache Hive.

See also the official https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin[Hive Metastore Administration] document.
====

[[attributes]]
.SparkSession's (Lazily-Initialized) Attributes (in alphabetical order)
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Type
| Description

| [[sessionState]] `sessionState`
| link:spark-sql-SessionState.adoc[SessionState]
a|

Internally, `sessionState` link:spark-sql-SessionState.adoc#clone[clones] the optional <<parentSessionState, parent `SessionState`>> (if given when <<creating-instance, creating SparkSession>>) or <<instantiateSessionState, creates a new `SessionState`>> using link:spark-sql-BaseSessionStateBuilder.adoc[BaseSessionStateBuilder] as defined by link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] property:

* *in-memory* (default) for `org.apache.spark.sql.internal.SessionStateBuilder`
* *hive* for `org.apache.spark.sql.hive.HiveSessionStateBuilder`

| [[sharedState]] `sharedState`
| link:spark-sql-SparkSession-SharedState.adoc[SharedState]
|
|===

NOTE: <<baseRelationToDataFrame, baseRelationToDataFrame>> acts as a mechanism to plug `BaseRelation` object hierarchy in into link:spark-sql-LogicalPlan.adoc[LogicalPlan] object hierarchy that `SparkSession` uses to bridge them.

=== [[creating-instance]] Creating SparkSession Instance

CAUTION: FIXME

=== [[internalCreateDataFrame]] `internalCreateDataFrame` Internal Method

[source, scala]
----
internalCreateDataFrame(
  catalystRows: RDD[InternalRow],
  schema: StructType,
  isStreaming: Boolean = false): DataFrame
----

`internalCreateDataFrame`...FIXME

NOTE: `internalCreateDataFrame` is used when...FIXME

=== [[builder]] Creating SparkSession Using Builder Pattern -- `builder` Method

[source, scala]
----
builder(): Builder
----

`builder` creates a new link:spark-sql-sparksession-builder.adoc[Builder] that you use to build a fully-configured `SparkSession` using a _fluent API_.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----

TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.

=== [[version]] Accessing Version of Spark -- `version` Method

[source, scala]
----
version: String
----

`version` returns the version of Apache Spark in use.

Internally, `version` uses `spark.SPARK_VERSION` value that is the `version` property in `spark-version-info.properties` properties file on CLASSPATH.

=== [[implicits]] Implicit Conversions -- `implicits` object

The `implicits` object is a helper class with the Scala implicit methods (aka _conversions_) to convert Scala objects to link:spark-sql-Dataset.adoc[Datasets], link:spark-sql-DataFrame.adoc[DataFrames] and link:spark-sql-Column.adoc[Columns]. It also defines link:spark-sql-Encoder.adoc[Encoders] for Scala's "primitive" types, e.g. `Int`, `Double`, `String`, and their products and collections.

[NOTE]
====
Import the implicits by `import spark.implicits._`.

[source, scala]
----
val spark = SparkSession.builder.getOrCreate()
import spark.implicits._
----
====

`implicits` object offers support for creating `Dataset` from `RDD` of any type (for which an link:spark-sql-Encoder.adoc[encoder] exists in scope), or case classes or tuples, and `Seq`.

`implicits` object also offers conversions from Scala's `Symbol` or `$` to `Column`.

It also offers conversions from `RDD` or `Seq` of `Product` types (e.g. case classes or tuples) to `DataFrame`. It has direct conversions from `RDD` of `Int`, `Long` and `String` to `DataFrame` with a single column name `_1`.

NOTE: It is only possible to call `toDF` methods on `RDD` objects of `Int`, `Long`, and `String` "primitive" types.

=== [[emptyDataset]] Creating Empty Dataset -- `emptyDataset` method

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

`emptyDataset` creates an empty link:spark-sql-Dataset.adoc[Dataset] (assuming that future records being of type `T`).

[source, scala]
----
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
----

`emptyDataset` creates a  link:spark-sql-LogicalPlan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[createDataset]] Creating Dataset from Local Collections and RDDs -- `createDataset` methods

[source, scala]
----
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
----

`createDataset` is an experimental API to create a link:spark-sql-Dataset.adoc[Dataset] from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

[source, scala]
----
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
----

`createDataset` creates a link:spark-sql-LogicalPlan-LocalRelation.adoc[`LocalRelation` logical query plan] (for the input `data` collection) or `LogicalRDD` (for the input `RDD[T]`).

[TIP]
====
You'd be better off using link:spark-sql-Dataset.adoc#implicits[Scala implicits and `toDS` method] instead (that does this conversion automatically for you).

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

scala> val one = Seq(1).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
----
====

Internally, `createDataset` first looks up the implicit link:spark-sql-ExpressionEncoder.adoc[expression encoder] in scope to access the ``AttributeReference``s (of the link:spark-sql-schema.adoc[schema]).

NOTE: Only unresolved link:spark-sql-ExpressionEncoder.adoc[expression encoders] are currently supported.

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of link:spark-sql-InternalRow.adoc[InternalRows]. With the references and rows, `createDataset` returns a link:spark-sql-Dataset.adoc[Dataset] with a link:spark-sql-LogicalPlan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[range]] Creating Dataset With Single Long Column -- `range` methods

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

`range` family of methods create a link:spark-sql-Dataset.adoc[Dataset] of `Long` numbers.

[source, scala]
----
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
----

NOTE: The three first variants (that do not specify `numPartitions` explicitly) use link:spark-SparkContext.adoc#defaultParallelism[SparkContext.defaultParallelism] for the number of partitions `numPartitions`.

Internally, `range` creates a new `Dataset[Long]` with `Range` link:spark-sql-LogicalPlan.adoc[logical plan] and `Encoders.LONG` link:spark-sql-Encoder.adoc[encoder].

=== [[emptyDataFrame]]  Creating Empty DataFrame --  `emptyDataFrame` method

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` creates an empty `DataFrame` (with no rows and columns).

It calls <<createDataFrame, createDataFrame>> with an empty `RDD[Row]` and an empty schema link:spark-sql-StructType.adoc[StructType(Nil)].

=== [[createDataFrame]] Creating DataFrames from RDDs with Explicit Schema -- `createDataFrame` method

[source, scala]
----
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
----

`createDataFrame` creates a `DataFrame` using `RDD[Row]` and the input `schema`. It is assumed that the rows in `rowRDD` all match the `schema`.

=== [[sql]] Executing SQL Queries (aka SQL Mode) -- `sql` Method

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement and creates a link:spark-sql-DataFrame.adoc[DataFrame].

[NOTE]
====
`sql` is imported in link:spark-shell.adoc[spark-shell] so you can execute SQL statements as if `sql` were a part of the environment.

```
scala> spark.version
res0: String = 2.2.0-SNAPSHOT

scala> :imports
 1) import spark.implicits._       (72 terms, 43 are implicit)
 2) import spark.sql               (1 terms)
```
====

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, `sql` requests the link:spark-sql-SessionState.adoc#sqlParser[current `ParserInterface`] to link:spark-sql-ParserInterface.adoc#parsePlan[execute a SQL query] that gives a link:spark-sql-LogicalPlan.adoc[LogicalPlan].

NOTE: `sql` uses `SessionState` link:spark-sql-SessionState.adoc#sqlParser[to access the current `ParserInterface`].

`sql` then creates a link:spark-sql-DataFrame.adoc[DataFrame] using the current `SparkSession` (itself) and the link:spark-sql-LogicalPlan.adoc[LogicalPlan].

[TIP]
====
link:spark-sql-spark-sql.adoc[spark-sql] is the main SQL environment in Spark to work with pure SQL statements (where you do not have to use Scala to execute them).

```
spark-sql> show databases;
default
Time taken: 0.028 seconds, Fetched 1 row(s)
```
====

=== [[udf]] Accessing UDF Registration Interface -- `udf` Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute gives access to link:spark-sql-UDFRegistration.adoc[UDFRegistration] that allows registering link:spark-sql-udfs.adoc[user-defined functions] for SQL-based queries.

[source, scala]
----
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
----

Internally, it is simply an alias for link:spark-sql-SessionState.adoc#udfRegistration[SessionState.udfRegistration].

=== [[table]] Creating DataFrame for Table -- `table` method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` creates a link:spark-sql-DataFrame.adoc[DataFrame] from records in the `tableName` table (if exists).

[source, scala]
----
val df = spark.table("mytable")
----

=== [[catalog]] Accessing Metastore -- `catalog` Attribute

[source, scala]
----
catalog: Catalog
----

`catalog` attribute is a (lazy) interface to the current metastore, i.e. link:spark-sql-Catalog.adoc[data catalog] (of relational entities like databases, tables, functions, table columns, and temporary views).

TIP: All methods in `Catalog` return `Datasets`.

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

Internally, `catalog` creates a link:spark-sql-CatalogImpl.adoc[CatalogImpl] (that uses the current `SparkSession`).

=== [[read]] Accessing DataFrameReader -- `read` method

[source, scala]
----
read: DataFrameReader
----

`read` method returns a link:spark-sql-DataFrameReader.adoc[DataFrameReader] that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Runtime Configuration -- `conf` attribute

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current runtime configuration (as `RuntimeConfig`) that wraps link:spark-sql-SQLConf.adoc[SQLConf].

CAUTION: FIXME

=== [[readStream]] `readStream` method

[source, scala]
----
readStream: DataStreamReader
----

`readStream` returns a new link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader].

=== [[streams]] `streams` Attribute

[source, scala]
----
streams: StreamingQueryManager
----

`streams` attribute gives access to link:spark-sql-streaming-StreamingQueryManager.adoc[StreamingQueryManager] (through link:spark-sql-SessionState.adoc#streamingQueryManager[SessionState]).

[source, scala]
----
val spark: SparkSession = ...
spark.streams.active.foreach(println)
----

=== [[streamingQueryManager]] `streamingQueryManager` Attribute

`streamingQueryManager` is...

=== [[listenerManager]] `listenerManager` Attribute

`listenerManager` is...

=== [[ExecutionListenerManager]] `ExecutionListenerManager`

`ExecutionListenerManager` is...

=== [[functionRegistry]] `functionRegistry` Attribute

`functionRegistry` is...

=== [[experimentalMethods]] `experimentalMethods` Attribute

[source, scala]
----
experimental: ExperimentalMethods
----

`experimentalMethods` is an extension point with link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods] that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

NOTE: `experimental` is used in link:spark-sql-SparkPlanner.adoc[SparkPlanner] and link:spark-sql-SparkOptimizer.adoc[SparkOptimizer]. Hive and link:spark-structured-streaming.adoc[Structured Streaming] use it for their own extra strategies and optimization rules.

=== [[newSession]] `newSession` method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates (starts) a new `SparkSession` (with the current link:spark-SparkContext.adoc[SparkContext] and link:spark-sql-SparkSession-SharedState.adoc[SharedState]).

[source, scala]
----
scala> println(sc.version)
2.0.0-SNAPSHOT

scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[stop]] Stopping SparkSession -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the `SparkSession`, i.e. link:spark-SparkContext.adoc#stop[stops the underlying `SparkContext`].

=== [[baseRelationToDataFrame]] Create DataFrame from BaseRelation -- `baseRelationToDataFrame` Method

[source, scala]
----
baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame
----

Internally, `baseRelationToDataFrame` creates a link:spark-sql-DataFrame.adoc[DataFrame] from the input link:spark-sql-BaseRelation.adoc[BaseRelation] wrapped inside link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation].

NOTE: link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] is an logical plan adapter for `BaseRelation` (so `BaseRelation` can be part of a link:spark-sql-LogicalPlan.adoc[logical plan]).

[NOTE]
====
`baseRelationToDataFrame` is used when:

* `DataFrameReader` link:spark-sql-DataFrameReader.adoc#load[loads data from a data source that supports multiple paths]
* `DataFrameReader` link:spark-sql-DataFrameReader.adoc#jdbc[loads data from an external table using JDBC]
* `TextInputCSVDataSource` creates a base `Dataset` (of Strings)
* `TextInputJsonDataSource` creates a base `Dataset` (of Strings)
====

=== [[instantiateSessionState]] Building SessionState -- `instantiateSessionState` Internal Method

[source, scala]
----
instantiateSessionState(className: String, sparkSession: SparkSession): SessionState
----

`instantiateSessionState` finds the `className` that is then used to link:spark-sql-BaseSessionStateBuilder.adoc#creating-instance[create] and immediatelly link:spark-sql-BaseSessionStateBuilder.adoc#build[build] a `BaseSessionStateBuilder`.

`instantiateSessionState` reports a `IllegalArgumentException` while constructing a `SessionState`:

```
Error while instantiating '[className]'
```

NOTE: `instantiateSessionState` is used exclusively when `SparkSession` <<sessionState, is requested for `SessionState`>> (and one is not available yet).
