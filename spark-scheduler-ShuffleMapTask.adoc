== [[ShuffleMapTask]] ShuffleMapTask -- Task for ShuffleMapStage

`ShuffleMapTask` is a concrete <<spark-scheduler-Task.adoc#, Task>> that <<runTask, computes a MapStatus>>, i.e. writes the result of executing a <<taskBinary, serialized task code>> over the records (in a <<partition, RDD partition>>) to the link:spark-shuffle-ShuffleManager.adoc[shuffle system] and returns information about the link:spark-BlockManager.adoc[BlockManager] and estimated size of the result shuffle blocks.

`ShuffleMapTask` is created exclusively when `DAGScheduler` is requested to <<spark-scheduler-DAGScheduler.adoc#submitMissingTasks, submitting missing tasks of a ShuffleMapStage>>.

[NOTE]
====
Spark uses <<spark-broadcast.adoc#, broadcast variables>> to send (serialized) tasks to executors.

<<runTask, runTask>> expects that the <<taskBinary, serialized task binary>> should be a tuple of a <<spark-rdd-RDD.adoc#, RDD>> and a <<spark-rdd-ShuffleDependency.adoc#, ShuffleDependency>>.
====

[[logging]]
[TIP]
====
Enable `ALL` logging level for `org.apache.spark.scheduler.ShuffleMapTask` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.ShuffleMapTask=ALL
```

Refer to <<spark-logging.adoc#, Logging>>.
====

=== [[creating-instance]] Creating ShuffleMapTask Instance

`ShuffleMapTask` takes the following to be created:

* [[stageId]] <<spark-scheduler-Stage.adoc#, Stage>> ID
* [[stageAttemptId]] Stage attempt ID
* [[taskBinary]] <<spark-broadcast.adoc#, Broadcast variable>> with the serialized code to execute (`Broadcast[Array[Byte]]`)
* [[partition]] <<spark-rdd-Partition.adoc#, RDD partition>>
* [[locs]] link:spark-TaskLocation.adoc[TaskLocations] (`Seq[TaskLocation]`)
* [[localProperties]] Task-specific local properties
* [[serializedTaskMetrics]] Serialized task metrics (`Array[Byte]`)
* [[jobId]] Optional <<spark-scheduler-ActiveJob.adoc#, job>> ID (default: `None`)
* [[appId]] Optional application ID (default: `None`)
* [[appAttemptId]] Optional application attempt ID (default: `None`)
* [[isBarrier]] `isBarrier` flag (default: `false`)

`ShuffleMapTask` calculates <<preferredLocs, preferredLocs>> internal attribute that is the input `locs` if defined. Otherwise, it is empty.

NOTE: <<preferredLocs, preferredLocs>> and <<locs, locs>> are transient values and so they are not sent over the wire with the task.

`ShuffleMapTask` initializes the <<internal-registries, internal registries and counters>>.

=== [[runTask]] Running Task -- `runTask` Method

[source, scala]
----
runTask(context: TaskContext): MapStatus
----

NOTE: `runTask` is part of link:spark-scheduler-Task.adoc#runTask[Task contract] to run a task.

`runTask` writes the result (_records_) of executing the <<taskBinary, serialized task code>> over the records (in the <<partition, RDD partition>>) to the link:spark-shuffle-ShuffleManager.adoc[shuffle system] and returns a link:spark-scheduler-MapStatus.adoc[MapStatus] (with the link:spark-BlockManager.adoc[BlockManager] and an estimated size of the result shuffle blocks).

Internally, `runTask` requests the <<spark-SparkEnv.adoc#, SparkEnv>> for the new instance of <<spark-SparkEnv.adoc#closureSerializer, closure serializer>> and requests it to <<spark-Serializer.adoc#deserialize, deserialize>> the <<taskBinary, taskBinary>> (into a tuple of a <<spark-rdd-RDD.adoc#, RDD>> and a <<spark-rdd-ShuffleDependency.adoc#, ShuffleDependency>>).

`runTask` measures the <<spark-scheduler-Task.adoc#_executorDeserializeTime, thread>> and <<spark-scheduler-Task.adoc#_executorDeserializeCpuTime, CPU>> deserialization times.

`runTask` requests the <<spark-SparkEnv.adoc#, SparkEnv>> for the <<spark-SparkEnv.adoc#shuffleManager, ShuffleManager>> and requests it for a <<spark-shuffle-ShuffleManager.adoc#getWriter, ShuffleWriter>> (for the <<spark-rdd-ShuffleDependency.adoc#shuffleHandle, ShuffleHandle>>, the <<spark-scheduler-Task.adoc#partitionId, RDD partition>>, and the <<spark-TaskContext.adoc#, TaskContext>>).

`runTask` then requests the <<rdd, RDD>> for the <<spark-rdd-RDD.adoc#iterator, records>> (of the <<partition, partition>>) that the `ShuffleWriter` is requested to <<spark-shuffle-ShuffleWriter.adoc#write, write out>> (to the shuffle system).

In the end, `runTask` requests the `ShuffleWriter` to <<spark-shuffle-ShuffleWriter.adoc#stop, stop>> (with the `success` flag on) and returns the <<spark-scheduler-MapStatus.adoc#, shuffle map output status>>.

NOTE: This is the moment in ``Task``'s lifecycle (and its corresponding RDD) when a link:spark-rdd.adoc#iterator[RDD partition is computed] and in turn becomes a sequence of records (i.e. real data) on an executor.

In case of any exceptions, `runTask` requests the `ShuffleWriter` to <<spark-shuffle-ShuffleWriter.adoc#stop, stop>> (with the `success` flag off) and (re)throws the exception.

`runTask` may also print out the following DEBUG message to the logs when the `ShuffleWriter` could not be <<spark-shuffle-ShuffleWriter.adoc#stop, stopped>>.

```
DEBUG Could not stop writer
```

=== [[preferredLocations]] `preferredLocations` Method

[source, scala]
----
preferredLocations: Seq[TaskLocation]
----

NOTE: `preferredLocations` is part of link:spark-scheduler-Task.adoc#preferredLocations[Task contract] to...FIXME

`preferredLocations` simply returns <<preferredLocs, preferredLocs>> internal property.

=== [[internal-registries]] Internal Properties

.ShuffleMapTask's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| preferredLocs
| [[preferredLocs]] link:spark-TaskLocation.adoc[TaskLocations] that are the unique entries in the given <<locs, locs>> with the only rule that when `locs` is not defined, it is empty, and no task location preferences are defined.

Initialized when `ShuffleMapTask` is <<creating-instance, created>>

Used exclusively when `ShuffleMapTask` is requested for the <<preferredLocations, preferred locations>>

|===
