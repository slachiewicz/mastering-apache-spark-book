== [[UnsafeShuffleWriter]] UnsafeShuffleWriter -- ShuffleWriter for SerializedShuffleHandle

`UnsafeShuffleWriter` is a concrete link:spark-shuffle-ShuffleWriter.adoc[ShuffleWriter] that is used to <<write, write records>> (key-value pairs).

`UnsafeShuffleWriter` is <<creating-instance, created>> exclusively when `SortShuffleManager` is requested for a <<spark-shuffle-SortShuffleManager.adoc#getWriter, ShuffleWriter>> for a <<spark-shuffle-SerializedShuffleHandle.adoc#, SerializedShuffleHandle>>.

`UnsafeShuffleWriter` can use a <<transferToEnabled, specialized NIO-based merge procedure>> that avoids extra serialization/deserialization when <<spark.file.transferTo, spark.file.transferTo>> configuration property (default: `true`) is enabled.

[[DEFAULT_INITIAL_SORT_BUFFER_SIZE]]
[[initialSortBufferSize]]
`UnsafeShuffleWriter` uses the <<initialSortBufferSize, initial buffer size for sorting>> (default: `4096`) when creating a <<sorter, ShuffleExternalSorter>> (when requested to <<open, open>>).

TIP: Use <<spark-configuration-properties.adoc#spark.shuffle.sort.initialBufferSize, spark.shuffle.sort.initialBufferSize>> configuration property to change the default buffer size.

[[inputBufferSizeInBytes]]
`UnsafeShuffleWriter` uses the <<spark-configuration-properties.adoc#spark.shuffle.file.buffer, spark.shuffle.file.buffer>> configuration property (default: `32k`) for...FIXME

[[outputBufferSizeInBytes]]
`UnsafeShuffleWriter` uses the <<spark-configuration-properties.adoc#spark.shuffle.unsafe.file.output.buffer, spark.shuffle.unsafe.file.output.buffer>> configuration property (default: `32k`) for...FIXME

[[logging]]
[TIP]
====
Enable `ALL` logging levels for `org.apache.spark.shuffle.sort.UnsafeShuffleWriter` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=ALL
```

Refer to <<spark-logging.adoc#, Logging>>.
====

=== [[mergeSpillsWithTransferTo]] `mergeSpillsWithTransferTo` Method

CAUTION: FIXME

=== [[forceSorterToSpill]] `forceSorterToSpill` Method

CAUTION: FIXME

=== [[mergeSpills]] `mergeSpills` Method

CAUTION: FIXME

=== [[updatePeakMemoryUsed]] `updatePeakMemoryUsed` Method

CAUTION: FIXME

=== [[write]] Writing Records to Shuffle System -- `write` Method

[source, java]
----
void write(Iterator<Product2<K, V>> records)
----

NOTE: `write` is part of the <<spark-shuffle-ShuffleWriter.adoc#write, ShuffleWriter Contract>> to write records to a shuffle system.

Internally, `write` traverses the input sequence of records (for a RDD partition) and <<insertRecordIntoSorter, insertRecordIntoSorter>> one by one. When all the records have been processed, `write` <<closeAndWriteOutput, closes internal resources and writes spill files merged>>.

In the end, `write` link:spark-shuffle-ShuffleExternalSorter.adoc#cleanupResources[requests `ShuffleExternalSorter` to clean after itself].

CAUTION: FIXME

=== [[stop]] Stopping ShuffleWriter -- `stop` Method

[source, scala]
----
Option<MapStatus> stop(boolean success)
----

NOTE: `stop` is part of the <<spark-shuffle-ShuffleWriter.adoc#stop, ShuffleWriter Contract>> to stop the <<spark-shuffle-ShuffleWriter.adoc#, ShuffleWriter>>.

`stop`...FIXME

=== [[creating-instance]] Creating UnsafeShuffleWriter Instance

`UnsafeShuffleWriter` takes the following to be created:

* [[blockManager]] <<spark-BlockManager.adoc#, BlockManager>>
* [[shuffleBlockResolver]] <<spark-shuffle-IndexShuffleBlockResolver.adoc#, IndexShuffleBlockResolver>>
* [[memoryManager]] <<spark-memory-TaskMemoryManager.adoc#, TaskMemoryManager>>
* [[handle]] <<spark-shuffle-SerializedShuffleHandle.adoc#, SerializedShuffleHandle>>
* [[mapId]] Map ID
* [[taskContext]] <<spark-TaskContext.adoc#, TaskContext>>
* [[sparkConf]] <<spark-SparkConf.adoc#, SparkConf>>

`UnsafeShuffleWriter` requests the <<handle, SerializedShuffleHandle>> for the <<spark-shuffle-BaseShuffleHandle.adoc#dependency, ShuffleDependency>> that is then requested for the <<spark-rdd-ShuffleDependency.adoc#partitioner, Partitioner>> and, in the end, for the <<spark-rdd-Partitioner.adoc#numPartitions, number of partitions>>. `UnsafeShuffleWriter` makes sure that the number of shuffle output partitions is below <<spark-shuffle-SortShuffleManager.adoc#MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE, `(1 << 24)` partition identifiers that can be encoded>> and throws an `IllegalArgumentException` if not met:

```
UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions
```

NOTE: The number of shuffle output partitions is first enforced when link:spark-shuffle-SortShuffleManager.adoc#canUseSerializedShuffle[`SortShuffleManager` checks if `SerializedShuffleHandle` can be used for `ShuffleHandle`] (that eventually leads to `UnsafeShuffleWriter`).

In the end, `UnsafeShuffleWriter` <<open, creates a ShuffleExternalSorter and a SerializationStream>>.

=== [[open]] Opening UnsafeShuffleWriter (Creating ShuffleExternalSorter and SerializationStream) -- `open` Internal Method

[source, java]
----
void open() throws IOException
----

`open` makes sure that the internal reference to link:spark-shuffle-ShuffleExternalSorter.adoc[ShuffleExternalSorter] (as `sorter`) is not defined and link:spark-shuffle-ShuffleExternalSorter.adoc#creating-instance[creates one itself].

`open` creates a new byte array output stream (as `serBuffer`) with the buffer capacity of `1M`.

`open` creates a new link:spark-SerializationStream.adoc[SerializationStream] for the new byte array output stream using link:spark-SerializerInstance.adoc[SerializerInstance].

NOTE: `SerializerInstance` was defined when <<creating-instance, `UnsafeShuffleWriter` was created>> (and is exactly the one used to link:spark-rdd-ShuffleDependency.adoc#creating-instance[create the `ShuffleDependency`]).

NOTE: `open` is used exclusively when `UnsafeShuffleWriter` is <<creating-instance, created>>.

=== [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter -- `insertRecordIntoSorter` Method

[source, java]
----
void insertRecordIntoSorter(Product2<K, V> record)
throws IOException
----

`insertRecordIntoSorter` link:spark-rdd-Partitioner.adoc#getPartition[calculates the partition for the key of the input `record`].

NOTE: `Partitioner` is defined when <<creating-instance, `UnsafeShuffleWriter` is created>>.

`insertRecordIntoSorter` then writes the key and the value of the input `record` to link:spark-SerializationStream.adoc[SerializationStream] and calculates the size of the serialized buffer.

NOTE: `SerializationStream` is created when <<open, `UnsafeShuffleWriter` opens>>.

In the end, `insertRecordIntoSorter` link:spark-shuffle-ShuffleExternalSorter.adoc#insertRecord[inserts the serialized buffer to `ShuffleExternalSorter`] (as `Platform.BYTE_ARRAY_OFFSET` ).

NOTE: `ShuffleExternalSorter` is created when <<open, `UnsafeShuffleWriter` opens>>.

NOTE: `insertRecordIntoSorter` is used exclusively when <<write, `UnsafeShuffleWriter` writes records>>.

=== [[closeAndWriteOutput]] Closing Internal Resources and Writing Spill Files Merged -- `closeAndWriteOutput` Method

[source, java]
----
void closeAndWriteOutput() throws IOException
----

`closeAndWriteOutput` first <<updatePeakMemoryUsed, updates peak memory used>>.

`closeAndWriteOutput` removes the internal `ByteArrayOutputStream` and link:spark-SerializationStream.adoc[SerializationStream].

`closeAndWriteOutput` requests link:spark-shuffle-ShuffleExternalSorter.adoc#closeAndGetSpills[`ShuffleExternalSorter` to close itself and return `SpillInfo` metadata].

`closeAndWriteOutput` removes the internal `ShuffleExternalSorter`.

`closeAndWriteOutput` requests `IndexShuffleBlockResolver` for the data file for the `shuffleId` and `mapId`.

`closeAndWriteOutput` creates a temporary file to <<mergeSpills, merge spill files>>, deletes them afterwards, and requests `IndexShuffleBlockResolver` to write index file and commit.

`closeAndWriteOutput` creates a link:spark-scheduler-MapStatus.adoc[MapStatus] with the link:spark-BlockManager.adoc#shuffleServerId[location of the executor's `BlockManager`] and partition lengths in the merged file.

If there is an issue with deleting spill files, you should see the following ERROR message in the logs:

```
ERROR Error while deleting spill file [path]
```

If there is an issue with deleting the temporary file, you should see the following ERROR message in the logs:

```
ERROR Error while deleting temp file [path]
```

NOTE: `closeAndWriteOutput` is used exclusively when <<write, `UnsafeShuffleWriter` writes records>>.

=== [[internal-registries]] Internal Properties

.UnsafeShuffleWriter's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| partitioner
a| [[partitioner]]

| serializer
a| [[serializer]]

| shuffleId
a| [[shuffleId]]

| sorter
a| [[sorter]] link:spark-shuffle-ShuffleExternalSorter.adoc[ShuffleExternalSorter]

Initialized when `UnsafeShuffleWriter` is requested to <<open, open>> (which is when <<creating-instance, created>>) and destroyed when requested to <<closeAndWriteOutput, close internal resources and writes spill files merged>>.

Used when `UnsafeShuffleWriter` <<insertRecordIntoSorter, inserts a record into `ShuffleExternalSorter`>>, <<write, writes records>>, <<forceSorterToSpill, forceSorterToSpill>>, <<updatePeakMemoryUsed, updatePeakMemoryUsed>>, <<closeAndWriteOutput, closes internal resources and writes spill files merged>>, <<stop, stops>>.

| transferToEnabled
a| [[transferToEnabled]]

| writeMetrics
a| [[writeMetrics]]
|===
