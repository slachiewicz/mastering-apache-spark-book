== [[BlockManager]] BlockManager -- Key-Value Store for Blocks

`BlockManager` is a key-value store for blocks of data (simply _blocks_) in Spark. `BlockManager` acts as a local cache that runs on every "node" in a Spark application, i.e. the link:spark-driver.adoc[driver] and link:spark-Executor.adoc[executors] (and is created when link:spark-sparkenv.adoc#BlockManager[`SparkEnv` is created]).

`BlockManager` provides interface for uploading and fetching blocks both locally and remotely using various stores, i.e. <<stores, memory, disk, and off-heap>>.

When <<creating-instance, `BlockManager` is created>>, it creates its own private instances of link:spark-DiskBlockManager.adoc[DiskBlockManager], link:spark-BlockInfoManager.adoc[BlockInfoManager], link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] (that it immediately wires together, i.e. `BlockInfoManager` with `MemoryStore` and `DiskStore` with `DiskBlockManager`).

The common idiom in Spark to access a `BlockManager` regardless of a location, i.e. the driver or executors, is through link:spark-sparkenv.adoc#get[SparkEnv]:

[source, scala]
----
SparkEnv.get.blockManager
----

`BlockManager` is a link:spark-blockdatamanager.adoc[BlockDataManager], i.e. manages the storage for blocks that can represent cached RDD partitions, intermediate shuffle outputs, broadcasts, etc. It is also a <<BlockEvictionHandler, BlockEvictionHandler>> that drops a block from memory and storing it on a disk if applicable.

*Cached blocks* are blocks with non-zero sum of memory and disk sizes.

TIP: Use link:spark-webui.adoc[Web UI], esp. link:spark-webui-storage.adoc[Storage] and link:spark-webui-executors.adoc[Executors] tabs, to monitor the memory used.

TIP: Use link:spark-submit.adoc[spark-submit]'s command-line options, i.e. link:spark-submit.adoc#driver-memory[--driver-memory] for the driver and link:spark-submit.adoc#executor-memory[--executor-memory] for executors or their equivalents as Spark properties, i.e. link:spark-submit.adoc#spark.executor.memory[spark.executor.memory] and link:spark-submit.adoc#spark_driver_memory[spark.driver.memory], to control the memory for storage memory.

A <<creating-instance, `BlockManager` is created>> when a link:spark-sparkenv.adoc#create[Spark application starts] and must be <<initialize, initialized>> before it is fully operable.

When <<externalShuffleServiceEnabled, External Shuffle Service is enabled>>, `BlockManager` uses link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] to read other executors' shuffle files.

[[metrics]]
`BlockManager` uses link:spark-blockmanager-BlockManagerSource.adoc[BlockManagerSource] to report metrics under the name *BlockManager*.

[[internal-properties]]
.BlockManager's Internal Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| [[diskBlockManager]] `diskBlockManager`
| FIXME
| link:spark-DiskBlockManager.adoc[DiskBlockManager] for...FIXME

| [[maxMemory]] `maxMemory`
| Total available link:spark-MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and link:spark-MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] memory for storage (in bytes)
| Total maximum value that `BlockManager` can ever possibly use (that depends on <<memoryManager, MemoryManager>> and may vary over time).
|===

[TIP]
====
Enable `INFO`, `DEBUG` or `TRACE` logging level for `org.apache.spark.storage.BlockManager` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManager=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

[TIP]
====
You may want to shut off WARN messages being printed out about the current state of blocks using the following line to cut the noise:

```
log4j.logger.org.apache.spark.storage.BlockManager=OFF
```
====

=== [[getLocations]] `getLocations` Method

CAUTION: FIXME

=== [[blockIdsToHosts]] `blockIdsToHosts` Method

CAUTION: FIXME

=== [[getLocationBlockIds]] `getLocationBlockIds` Method

CAUTION: FIXME

=== [[getPeers]] `getPeers` Method

CAUTION: FIXME

=== [[releaseAllLocksForTask]] `releaseAllLocksForTask` Method

CAUTION: FIXME

=== [[memoryStore]] `memoryStore` Property

CAUTION: FIXME

=== [[stop]] Stopping BlockManager -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop`...FIXME

NOTE: `stop` is used exclusively when `SparkEnv` is requested to link:spark-sparkenv.adoc#stop[stop].

=== [[putSingle]] `putSingle` Method

CAUTION: FIXME

NOTE: `putSingle` is used when link:spark-TorrentBroadcast.adoc#readBroadcastBlock[`TorrentBroadcast` reads the blocks of a broadcast variable and stores them in a local `BlockManager`].

=== [[getMatchingBlockIds]] Getting IDs of Existing Blocks (For a Given Filter) -- `getMatchingBlockIds` Method

[source, scala]
----
getMatchingBlockIds(filter: BlockId => Boolean): Seq[BlockId]
----

`getMatchingBlockIds`...FIXME

NOTE: `getMatchingBlockIds` is used exclusively when `BlockManagerSlaveEndpoint` is requested to link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#GetMatchingBlockIds[handle a GetMatchingBlockIds message].

=== [[getLocalValues]] `getLocalValues` Method

[source, scala]
----
getLocalValues(blockId: BlockId): Option[BlockResult]
----

`getLocalValues`...FIXME

Internally, when `getLocalValues` is executed, you should see the following DEBUG message in the logs:

```
DEBUG BlockManager: Getting local block [blockId]
```

`getLocalValues` link:spark-BlockInfoManager.adoc#lockForReading[obtains a read lock for `blockId`].

When no `blockId` block was found, you should see the following DEBUG message in the logs and `getLocalValues` returns "nothing" (i.e. `NONE`).

```
DEBUG Block [blockId] was not found
```

When the `blockId` block was found, you should see the following DEBUG message in the logs:

```
DEBUG Level for block [blockId] is [level]
```

If `blockId` block has memory level and link:spark-MemoryStore.adoc#contains[is registered in `MemoryStore`], `getLocalValues` returns a <<BlockResult, BlockResult>> as `Memory` read method and with a `CompletionIterator` for an interator:

1. link:spark-MemoryStore.adoc#getValues[Values iterator from `MemoryStore` for `blockId`] for "deserialized" persistence levels.
2. Iterator from link:spark-SerializerManager.adoc#dataDeserializeStream[`SerializerManager` after the data stream has been deserialized] for the `blockId` block and link:spark-MemoryStore.adoc#getBytes[the bytes for `blockId` block] for "serialized" persistence levels.

NOTE: `getLocalValues` is used when link:spark-TorrentBroadcast.adoc#readBroadcastBlock[`TorrentBroadcast` reads the blocks of a broadcast variable and stores them in a local `BlockManager`].

CAUTION: FIXME

=== [[getRemoteValues]] `getRemoteValues` Internal Method

[source, scala]
----
getRemoteValues[T: ClassTag](blockId: BlockId): Option[BlockResult]
----

`getRemoteValues`...FIXME

=== [[get]] Retrieving Block from Local or Remote Block Managers -- `get` Method

[source, scala]
----
get[T](blockId: BlockId): Option[BlockResult]
----

`get` attempts to get the `blockId` block from a local block manager first before querying remote block managers.

Internally, `get` tries to <<getLocalValues, get `blockId` block from the local `BlockManager`>>. If the `blockId` block was found, you should see the following INFO message in the logs and `get` returns the local <<BlockResult, BlockResult>>.

```
INFO Found block [blockId] locally
```

If however the `blockId` block was not found locally, `get` tries to <<getRemoteValues, get the block from remote ``BlockManager``s>>. If the `blockId` block was retrieved from a remote `BlockManager`, you should see the following INFO message in the logs and `get` returns the remote <<BlockResult, BlockResult>>.

```
INFO Found block [blockId] remotely
```

In the end, `get` returns "nothing" (i.e. `NONE`) when the `blockId` block was not found either in the local `BlockManager` or any remote `BlockManager`.

NOTE: `get` is used when `BlockManager` is requested to <<getOrElseUpdate, `getOrElseUpdate` a block>>, <<getSingle, getSingle>> and to link:spark-rdd-blockrdd.adoc#[compute a `BlockRDD`].

=== [[getSingle]] `getSingle` Method

CAUTION: FIXME

=== [[getOrElseUpdate]] `getOrElseUpdate` Method

CAUTION: FIXME

[source, scala]
----
getOrElseUpdate[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[T],
  makeIterator: () => Iterator[T]): Either[BlockResult, Iterator[T]]
----

`getOrElseUpdate`...FIXME

=== [[getLocalBytes]] Getting Local Block Data As Bytes -- `getLocalBytes` Method

CAUTION: FIXME

=== [[getRemoteBytes]] `getRemoteBytes` Method

CAUTION: FIXME

=== [[getBlockData]] Finding Shuffle Block Data -- `getBlockData` Method

CAUTION: FIXME

=== [[removeBlockInternal]] `removeBlockInternal` Method

CAUTION: FIXME

=== [[externalShuffleServiceEnabled]] Is External Shuffle Service Enabled? -- `externalShuffleServiceEnabled` Flag

When the link:spark-ExternalShuffleService.adoc[External Shuffle Service] is enabled for a Spark application, `BlockManager` uses link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] to read other executors' shuffle files.

CAUTION: FIXME How is `shuffleClient` used?

=== [[stores]] Stores

A *Store* is the place where blocks are held.

There are the following possible stores:

* link:spark-MemoryStore.adoc[MemoryStore] for memory storage level.
* link:spark-DiskStore.adoc[DiskStore] for disk storage level.
* `ExternalBlockStore` for OFF_HEAP storage level.

=== [[putBlockData]] Storing Block Data Locally -- `putBlockData` Method

[source, scala]
----
putBlockData(
  blockId: BlockId,
  data: ManagedBuffer,
  level: StorageLevel,
  classTag: ClassTag[_]): Boolean
----

`putBlockData` simply <<putBytes, stores `blockId` locally>> (given the given storage `level`).

NOTE: `putBlockData` is part of link:spark-blockdatamanager.adoc[BlockDataManager contract].

Internally, `putBlockData` wraps `ChunkedByteBuffer` around `data` buffer's NIO `ByteBuffer` and calls <<putBytes, putBytes>>.

NOTE: `putBlockData` is used when `NettyBlockRpcServer` link:spark-NettyBlockRpcServer.adoc[handles a `UploadBlock` message].

=== [[putBytes]] Storing Block Bytes Locally -- `putBytes` Method

[source, scala]
----
putBytes(
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  tellMaster: Boolean = true): Boolean
----

`putBytes` stores the `blockId` block (with `bytes` bytes and `level` storage level).

`putBytes` simply passes the call on to the internal <<doPutBytes, doPutBytes>>.

NOTE: `putBytes` is executed when link:spark-executor-TaskRunner.adoc#run-result-sent-via-blockmanager[`TaskRunner` sends a task result via `BlockManager`], <<putBlockData, `BlockManager` puts a block locally>> and in link:spark-TorrentBroadcast.adoc[TorrentBroadcast].

==== [[doPutBytes]] `doPutBytes` Internal Method

[source, scala]
----
def doPutBytes[T](
  blockId: BlockId,
  bytes: ChunkedByteBuffer,
  level: StorageLevel,
  classTag: ClassTag[T],
  tellMaster: Boolean = true,
  keepReadLock: Boolean = false): Boolean
----

`doPutBytes` calls the internal helper <<doPut, doPut>> with a function that accepts a `BlockInfo` and does the uploading.

Inside the function, if the link:spark-rdd-StorageLevel.adoc[storage `level`]'s replication is greater than 1, it immediately starts <<replicate, replication>> of the `blockId` block on a separate thread (from `futureExecutionContext` thread pool). The replication uses the input `bytes` and `level` storage level.

For a memory storage level, the function checks whether the storage `level` is deserialized or not. For a deserialized storage `level`, ``BlockManager``'s link:spark-SerializerManager.adoc#dataDeserializeStream[`SerializerManager` deserializes `bytes` into an iterator of values] that link:spark-MemoryStore.adoc#putIteratorAsValues[`MemoryStore` stores]. If however the storage `level` is not deserialized, the function requests link:spark-MemoryStore.adoc#putBytes[`MemoryStore` to store the bytes]

If the put did not succeed and the storage level is to use disk, you should see the following WARN message in the logs:

```
WARN BlockManager: Persisting block [blockId] to disk instead.
```

And link:spark-DiskStore.adoc#putBytes[`DiskStore` stores the bytes].

NOTE: link:spark-DiskStore.adoc[DiskStore] is requested to store the bytes of a block with memory and disk storage level only when link:spark-MemoryStore.adoc[MemoryStore] has failed.

If the storage level is to use disk only, link:spark-DiskStore.adoc#putBytes[`DiskStore` stores the bytes].

`doPutBytes` requests <<getCurrentBlockStatus, current block status>> and if the block was successfully stored, and the driver should know about it (`tellMaster`), the function <<reportBlockStatus, reports the current storage status of the block to the driver>>. The link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current `TaskContext` metrics are updated with the updated block status] (only when executed inside a task where `TaskContext` is available).

You should see the following DEBUG message in the logs:

```
DEBUG BlockManager: Put block [blockId] locally took [time] ms
```

The function waits till the earlier asynchronous replication finishes for a block with replication level greater than `1`.

The final result of `doPutBytes` is the result of storing the block successful or not (as computed earlier).

NOTE: `doPutBytes` is called exclusively from <<putBytes, `putBytes` method>>.

=== [[replicate]] `replicate` Internal Method

CAUTION: FIXME

=== [[maybeCacheDiskValuesInMemory]] `maybeCacheDiskValuesInMemory` Method

CAUTION: FIXME

=== [[doPutIterator]] `doPutIterator` Method

CAUTION: FIXME

=== [[doPut]] `doPut` Internal Method

[source, scala]
----
doPut[T](
  blockId: BlockId,
  level: StorageLevel,
  classTag: ClassTag[_],
  tellMaster: Boolean,
  keepReadLock: Boolean)(putBody: BlockInfo => Option[T]): Option[T]
----

`doPut` is an internal helper method for <<doPutBytes, doPutBytes>> and <<doPutIterator, doPutIterator>>.

`doPut` executes the input `putBody` function with a link:spark-BlockInfo.adoc[BlockInfo] being a new `BlockInfo` object (with `level` storage level) that link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[`BlockInfoManager` managed to create a write lock for].

If the block has already been created (and link:spark-BlockInfoManager.adoc#lockNewBlockForWriting[`BlockInfoManager` did not manage to create a write lock for]), the following WARN message is printed out to the logs:

```
WARN Block [blockId] already exists on this machine; not re-adding it
```

`doPut` <<releaseLock, releases the read lock for the block>> when `keepReadLock` flag is disabled and returns `None` immediately.

If however the write lock has been given, `doPut` executes `putBody`.

If the result of `putBody` is `None` the block is considered saved successfully.

For successful save and `keepReadLock` enabled, link:spark-BlockInfoManager.adoc#downgradeLock[`BlockInfoManager` is requested to downgrade an exclusive write lock for `blockId` to a shared read lock].

For successful save and `keepReadLock` disabled, link:spark-BlockInfoManager.adoc#unlock[`BlockInfoManager` is requested to release lock on `blockId`].

For unsuccessful save, <<removeBlockInternal, the block is removed from memory and disk stores>> and the following WARN message is printed out to the logs:

```
WARN Putting block [blockId] failed
```

Ultimately, the following DEBUG message is printed out to the logs:

```
DEBUG Putting block [blockId] [withOrWithout] replication took [usedTime] ms
```

=== [[removeBlock]] Removing Block From Memory and Disk -- `removeBlock` Method

[source, scala]
----
removeBlock(blockId: BlockId, tellMaster: Boolean = true): Unit
----

`removeBlock` removes the `blockId` block from the link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

When executed, it prints out the following DEBUG message to the logs:

```
DEBUG Removing block [blockId]
```

It requests link:spark-BlockInfoManager.adoc[BlockInfoManager] for lock for writing for the `blockId` block. If it receives none, it prints out the following WARN message to the logs and quits.

```
WARN Asked to remove block [blockId], which does not exist
```

Otherwise, with a write lock for the block, the block is removed from link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] (see link:spark-MemoryStore.adoc#remove[Removing Block in `MemoryStore`] and link:spark-DiskStore.adoc#remove[Removing Block in `DiskStore`]).

If both removals fail, it prints out the following WARN message:

```
WARN Block [blockId] could not be removed as it was not found in either the disk, memory, or external block store
```

The block is removed from link:spark-BlockInfoManager.adoc[BlockInfoManager].

It then <<getCurrentBlockStatus, calculates the current block status>> that is used to <<reportBlockStatus, report the block status to the driver>> (if the input `tellMaster` and the info's `tellMaster` are both enabled, i.e. `true`) and the link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

NOTE: It is used to <<removeRdd, remove RDDs>> and <<removeBroadcast, broadcast>> as well as in link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveBlock[`BlockManagerSlaveEndpoint` while handling `RemoveBlock` messages].

=== [[removeRdd]] Removing RDD Blocks -- `removeRdd` Method

[source, scala]
----
removeRdd(rddId: Int): Int
----

`removeRdd` removes all the blocks that belong to the `rddId` RDD.

It prints out the following INFO message to the logs:

```
INFO Removing RDD [rddId]
```

It then requests RDD blocks from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>> (without informing the driver).

The number of blocks removed is the final result.

NOTE: It is used by link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveRdd[`BlockManagerSlaveEndpoint` while handling `RemoveRdd` messages].

=== [[removeBroadcast]] Removing Broadcast Blocks -- `removeBroadcast` Method

[source, scala]
----
removeBroadcast(broadcastId: Long, tellMaster: Boolean): Int
----

`removeBroadcast` removes all the blocks of the input `broadcastId` broadcast.

Internally, it starts by printing out the following DEBUG message to the logs:

```
DEBUG Removing broadcast [broadcastId]
```

It then requests all the link:spark-blockdatamanager.adoc#BroadcastBlockId[BroadcastBlockId] objects that belong to the `broadcastId` broadcast from link:spark-BlockInfoManager.adoc[BlockInfoManager] and <<removeBlock, removes them (from memory and disk)>>.

The number of blocks removed is the final result.

NOTE: It is used by link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveBroadcast[`BlockManagerSlaveEndpoint` while handling `RemoveBroadcast` messages].

=== [[getStatus]] Getting Block Status -- `getStatus` Method

CAUTION: FIXME

=== [[creating-instance]] Creating BlockManager Instance

`BlockManager` takes the following when created:

* `executorId` (for the driver and executors)
* link:spark-rpc.adoc[RpcEnv]
* [[master]] link:spark-BlockManagerMaster.adoc[BlockManagerMaster]
* link:spark-SerializerManager.adoc[SerializerManager]
* link:spark-SparkConf.adoc[SparkConf]
* [[memoryManager]] link:spark-MemoryManager.adoc[MemoryManager]
* link:spark-service-mapoutputtracker.adoc[MapOutputTracker]
* link:spark-ShuffleManager.adoc[ShuffleManager]
* link:spark-blocktransferservice.adoc[BlockTransferService]
* `SecurityManager`

NOTE: `executorId` is `SparkContext.DRIVER_IDENTIFIER`, i.e. `driver` for the driver and the value of link:spark-CoarseGrainedExecutorBackend.adoc#executor-id[--executor-id] command-line argument for link:spark-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend] executors or link:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend].

CAUTION: FIXME Elaborate on the executor backends and executor ids.

When created, `BlockManager` sets <<externalShuffleServiceEnabled, externalShuffleServiceEnabled>> internal flag per link:spark-ExternalShuffleService.adoc#spark.shuffle.service.enabled[spark.shuffle.service.enabled] Spark property.

`BlockManager` then creates an instance of link:spark-DiskBlockManager.adoc[DiskBlockManager] (requesting `deleteFilesOnStop` when an external shuffle service is not in use).

`BlockManager` creates an instance of link:spark-BlockInfoManager.adoc[BlockInfoManager] (as `blockInfoManager`).

`BlockManager` creates *block-manager-future* daemon cached thread pool with 128 threads maximum (as `futureExecutionContext`).

`BlockManager` creates a link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

link:spark-MemoryManager.adoc[MemoryManager] gets the link:spark-MemoryStore.adoc[MemoryStore] object assigned.

`BlockManager` calculates the maximum memory to use (as `maxMemory`) by requesting the maximum link:spark-MemoryManager.adoc#maxOnHeapStorageMemory[on-heap] and link:spark-MemoryManager.adoc#maxOffHeapStorageMemory[off-heap] storage memory from the assigned `MemoryManager`.

NOTE: link:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager] is the default `MemoryManager` (as of Spark 1.6).

`BlockManager` calculates the port used by the external shuffle service (as `externalShuffleServicePort`).

NOTE: It is computed specially in Spark on YARN.

CAUTION: FIXME Describe the YARN-specific part.

`BlockManager` creates a client to read other executors' shuffle files (as `shuffleClient`). If the external shuffle service is used an link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] is created or the input link:spark-blocktransferservice.adoc[BlockTransferService] is used.

`BlockManager` sets <<spark.block.failures.beforeLocationRefresh, the maximum number of failures before this block manager refreshes the block locations from the driver>> (as `maxFailuresBeforeLocationRefresh`).

`BlockManager` registers link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint] with the input link:spark-rpc.adoc[RpcEnv], itself, and link:spark-service-mapoutputtracker.adoc[MapOutputTracker] (as `slaveEndpoint`).

=== [[shuffleClient]] `shuffleClient`

CAUTION: FIXME

(that is assumed to be a link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient])

=== [[shuffleServerId]] `shuffleServerId`

CAUTION: FIXME

=== [[initialize]] Initializing BlockManager -- `initialize` Method

[source, scala]
----
initialize(appId: String): Unit
----

`initialize` initializes a `BlockManager` on the driver and executors (see link:spark-SparkContext.adoc#creating-instance[Creating SparkContext Instance] and link:spark-Executor.adoc#creating-instance[Creating Executor Instance], respectively).

NOTE: The method must be called before a `BlockManager` can be considered fully operable.

`initialize` does the following in order:

1. Initializes link:spark-blocktransferservice.adoc#init[BlockTransferService]
2. Initializes the internal shuffle client, be it link:spark-shuffleclient.adoc#ExternalShuffleClient[ExternalShuffleClient] or link:spark-blocktransferservice.adoc[BlockTransferService].
3. link:spark-BlockManagerMaster.adoc#registerBlockManager[Registers itself with the driver's `BlockManagerMaster`] (using the `id`, `maxMemory` and its `slaveEndpoint`).
+
The `BlockManagerMaster` reference is passed in when the <<creating-instance, `BlockManager` is created>> on the driver and executors.
4. Sets <<shuffleServerId, shuffleServerId>> to an instance of <<BlockManagerId, BlockManagerId>> given an executor id, host name and port for link:spark-blocktransferservice.adoc[BlockTransferService].
5. It creates the address of the server that serves this executor's shuffle files (using <<shuffleServerId, shuffleServerId>>)

CAUTION: FIXME Review the initialize procedure again

CAUTION: FIXME Describe `shuffleServerId`. Where is it used?

If the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, the following INFO appears in the logs:

```
INFO external shuffle service port = [externalShuffleServicePort]
```

It link:spark-BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's BlockManagerMaster] passing the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint].

Ultimately, if the initialization happens on an executor and the <<externalShuffleServiceEnabled, External Shuffle Service is used>>, it <<registerWithExternalShuffleServer, registers to the shuffle service>>.

NOTE: `initialize` is called when the link:spark-sparkcontext-creating-instance-internals.adoc#BlockManager-initialization[driver is launched (and `SparkContext` is created)] and when an link:spark-Executor.adoc#creating-instance[`Executor` is created] (for link:spark-CoarseGrainedExecutorBackend.adoc#RegisteredExecutor[CoarseGrainedExecutorBackend] and link:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]).

==== [[registerWithExternalShuffleServer]] Registering Executor's BlockManager with External Shuffle Server -- `registerWithExternalShuffleServer` Method

[source, scala]
----
registerWithExternalShuffleServer(): Unit
----

`registerWithExternalShuffleServer` is an internal helper method to register the `BlockManager` for an executor with an link:spark-ExternalShuffleService.adoc[external shuffle server].

NOTE: It is executed when a <<initialize, `BlockManager` is initialized on an executor and an external shuffle service is used>>.

When executed, you should see the following INFO message in the logs:

```
INFO Registering executor with local external shuffle service.
```

It uses <<shuffleClient, shuffleClient>> to link:spark-shuffleclient.adoc#ExternalShuffleClient-registerWithShuffleServer[register the block manager] using <<shuffleServerId, shuffleServerId>> (i.e. the host, the port and the executorId) and a `ExecutorShuffleInfo`.

NOTE: The `ExecutorShuffleInfo` uses `localDirs` and `subDirsPerLocalDir` from link:spark-DiskBlockManager.adoc[DiskBlockManager] and the class name of the constructor link:spark-ShuffleManager.adoc[ShuffleManager].

It tries to register at most 3 times with 5-second sleeps in-between.

NOTE: The maximum number of attempts and the sleep time in-between are hard-coded, i.e. they are not configured.

Any issues while connecting to the external shuffle service are reported as ERROR messages in the logs:

```
ERROR Failed to connect to external shuffle server, will retry [#attempts] more times after waiting 5 seconds...
```

=== [[reregister]] Re-registering BlockManager with Driver and Reporting Blocks -- `reregister` Method

[source, scala]
----
reregister(): Unit
----

When executed, `reregister` prints the following INFO message to the logs:

```
INFO BlockManager: BlockManager [blockManagerId] re-registering with master
```

`reregister` then link:spark-BlockManagerMaster.adoc#registerBlockManager[registers itself to the driver's `BlockManagerMaster`] (just as it was when <<initialize, BlockManager was initializing>>). It passes the <<BlockManagerId, BlockManagerId>>, the maximum memory (as `maxMemory`), and the link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint].

`reregister` will then report all the local blocks to the link:spark-BlockManagerMaster.adoc[BlockManagerMaster].

You should see the following INFO message in the logs:

```
INFO BlockManager: Reporting [blockInfoManager.size] blocks to the master.
```

For each block metadata (in link:spark-BlockInfoManager.adoc[BlockInfoManager]) it <<getCurrentBlockStatus, gets block current status>> and <<tryToReportBlockStatus, tries to send it to the BlockManagerMaster>>.

If there is an issue communicating to the link:spark-BlockManagerMaster.adoc[BlockManagerMaster], you should see the following ERROR message in the logs:

```
ERROR BlockManager: Failed to report [blockId] to master; giving up.
```

After the ERROR message, `reregister` stops reporting.

NOTE: `reregister` is called when a link:spark-Executor.adoc#heartbeats-and-active-task-metrics[`Executor` was informed to re-register while sending heartbeats].

=== [[getCurrentBlockStatus]] Calculate Current Block Status -- `getCurrentBlockStatus` Method

[source, scala]
----
getCurrentBlockStatus(blockId: BlockId, info: BlockInfo): BlockStatus
----

`getCurrentBlockStatus` returns the current `BlockStatus` of the `BlockId` block (with the block's current link:spark-rdd-StorageLevel.adoc[StorageLevel], memory and disk sizes). It uses link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] for size and other information.

NOTE: Most of the information to build `BlockStatus` is already in `BlockInfo` except that it may not necessarily reflect the current state per link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore].

Internally, it uses the input link:spark-BlockInfo.adoc[BlockInfo] to know about the block's storage level. If the storage level is not set (i.e. `null`), the returned `BlockStatus` assumes the link:spark-rdd-StorageLevel.adoc[default `NONE` storage level] and the memory and disk sizes being `0`.

If however the storage level is set, `getCurrentBlockStatus` uses link:spark-MemoryStore.adoc[MemoryStore] and link:spark-DiskStore.adoc[DiskStore] to check whether the block is stored in the storages or not and request for their sizes in the storages respectively (using their `getSize` or assume `0`).

NOTE: It is acceptable that the `BlockInfo` says to use memory or disk yet the block is not in the storages (yet or anymore). The method will give current status.

NOTE: `getCurrentBlockStatus` is used when <<reregister, executor's BlockManager is requested to report the current status of the local blocks to the master>>, <<doPutBytes, saving a block to a storage>> or <<dropFromMemory, removing a block from memory only>> or <<removeBlock, both, i.e. from memory and disk>>.

=== [[dropFromMemory]] Removing Blocks From Memory Only -- `dropFromMemory` Method

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

When `dropFromMemory` is executed, you should see the following INFO message in the logs:

```
INFO BlockManager: Dropping block [blockId] from memory
```

It then asserts that the `blockId` block is link:spark-BlockInfoManager.adoc#assertBlockIsLockedForWriting[locked for writing].

If the block's link:spark-rdd-StorageLevel.adoc[StorageLevel] uses disks and the internal link:spark-DiskStore.adoc[DiskStore] object (`diskStore`) does not contain the block, it is saved then. You should see the following INFO message in the logs:

```
INFO BlockManager: Writing block [blockId] to disk
```

CAUTION: FIXME Describe the case with saving a block to disk.

The block's memory size is fetched and recorded (using `MemoryStore.getSize`).

The block is link:spark-MemoryStore.adoc#remove[removed from memory] if exists. If not, you should see the following WARN message in the logs:

```
WARN BlockManager: Block [blockId] could not be dropped from memory as it does not exist
```

It then <<getCurrentBlockStatus, calculates the current storage status of the block>> and <<reportBlockStatus, reports it to the driver>>. It only happens when `info.tellMaster`.

CAUTION: FIXME When would `info.tellMaster` be `true`?

A block is considered updated when it was written to disk or removed from memory or both. If either happened, the link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[current TaskContext metrics are updated with the change].

Ultimately, `dropFromMemory` returns the current storage level of the block.

NOTE: `dropFromMemory` is part of the single-method <<BlockEvictionHandler, BlockEvictionHandler>> interface.

=== [[reportAllBlocks]] `reportAllBlocks` Method

CAUTION: FIXME

NOTE: `reportAllBlocks` is called when `BlockManager` is requested to <<reregister, re-register all blocks to the driver>>.

=== [[reportBlockStatus]] Reporting Current Storage Status of Block to Driver -- `reportBlockStatus` Method

[source, scala]
----
reportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Unit
----

`reportBlockStatus` is an internal method for <<tryToReportBlockStatus, reporting a block status to the driver>> and if told to re-register it prints out the following INFO message to the logs:

```
INFO BlockManager: Got told to re-register updating block [blockId]
```

It does asynchronous reregistration (using `asyncReregister`).

In either case, it prints out the following DEBUG message to the logs:

```
DEBUG BlockManager: Told master about block [blockId]
```

NOTE: `reportBlockStatus` is called by <<getBlockData, getBlockData>>, <<doPutBytes, doPutBytes>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, dropFromMemory>> and <<removeBlockInternal, removeBlockInternal>>.

=== [[tryToReportBlockStatus]] Reporting Block Status Update to Driver -- `tryToReportBlockStatus` Internal Method

[source, scala]
----
def tryToReportBlockStatus(
  blockId: BlockId,
  info: BlockInfo,
  status: BlockStatus,
  droppedMemorySize: Long = 0L): Boolean
----

`tryToReportBlockStatus` link:spark-BlockManagerMaster.adoc#updateBlockInfo[reports block status update] to <<master, BlockManagerMaster>> and returns its response.

NOTE: `tryToReportBlockStatus` is used when `BlockManager` <<reportAllBlocks, reportAllBlocks>> or <<reportBlockStatus, reportBlockStatus>>.

=== [[BlockEvictionHandler]] BlockEvictionHandler

`BlockEvictionHandler` is a `private[storage]` Scala trait with a single method <<BlockEvictionHandler-dropFromMemory, dropFromMemory>>.

[source, scala]
----
dropFromMemory(
  blockId: BlockId,
  data: () => Either[Array[T], ChunkedByteBuffer]): StorageLevel
----

NOTE: A `BlockManager` is a `BlockEvictionHandler`.

NOTE: `dropFromMemory` is called when  link:spark-MemoryStore.adoc#evictBlocksToFreeSpace[`MemoryStore` evicts blocks from memory to free space].

=== [[broadcast]] Broadcast Values

When a new broadcast value is created, link:spark-TorrentBroadcast.adoc[TorrentBroadcast] blocks are put in the block manager.

You should see the following `TRACE` message:

```
TRACE Put for block [blockId] took [startTimeMs] to get into synchronized block
```

It puts the data in the memory first and drop to disk if the memory store can't hold it.

```
DEBUG Put block [blockId] locally took [startTimeMs]
```

=== [[BlockManagerId]] BlockManagerId

FIXME

=== [[execution-context]] Execution Context

*block-manager-future* is the execution context for...FIXME

=== Misc

The underlying abstraction for blocks in Spark is a `ByteBuffer` that limits the size of a block to 2GB (`Integer.MAX_VALUE` - see http://stackoverflow.com/q/8076472/1305344[Why does FileChannel.map take up to Integer.MAX_VALUE of data?] and https://issues.apache.org/jira/browse/SPARK-1476[SPARK-1476 2GB limit in spark for blocks]). This has implication not just for managed blocks in use, but also for shuffle blocks (memory mapped blocks are limited to 2GB, even though the API allows for `long`), ser-deser via byte array-backed output streams.

When a non-local executor starts, it initializes a `BlockManager` object using link:spark-SparkConf.adoc#spark.app.id[spark.app.id] Spark property for the id.

=== [[BlockResult]] BlockResult

`BlockResult` is a description of a fetched block with the `readMethod` and `bytes`.

=== [[registerTask]] Registering Task with BlockInfoManager -- `registerTask` Method

[source, scala]
----
registerTask(taskAttemptId: Long): Unit
----

`registerTask` link:spark-BlockInfoManager.adoc#registerTask[registers the input `taskAttemptId` with `BlockInfoManager`].

NOTE: `registerTask` is used exclusively when link:spark-taskscheduler-tasks.adoc#run[`Task` runs].

=== [[getDiskWriter]] Offering DiskBlockObjectWriter To Write Blocks To Disk (For Current BlockManager) -- `getDiskWriter` Method

[source, scala]
----
getDiskWriter(
  blockId: BlockId,
  file: File,
  serializerInstance: SerializerInstance,
  bufferSize: Int,
  writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter
----

`getDiskWriter` link:spark-blockmanager-DiskBlockObjectWriter.adoc#creating-instance[creates a `DiskBlockObjectWriter`] with <<spark_shuffle_sync, spark.shuffle.sync>> Spark property for `syncWrites`.

NOTE: `getDiskWriter` uses the same `serializerManager` that was used to <<creating-instance, create a `BlockManager`>>.

NOTE: `getDiskWriter` is used when link:spark-BypassMergeSortShuffleWriter.adoc#write[`BypassMergeSortShuffleWriter` writes records into one single shuffle block data file], in link:spark-ShuffleExternalSorter.adoc#writeSortedFile[ShuffleExternalSorter], `UnsafeSorterSpillWriter`, link:spark-ExternalSorter.adoc[ExternalSorter], and `ExternalAppendOnlyMap`.

=== [[addUpdatedBlockStatusToTaskMetrics]] Recording Updated BlockStatus In Current Task's TaskMetrics -- `addUpdatedBlockStatusToTaskMetrics` Internal Method

[source, scala]
----
addUpdatedBlockStatusToTaskMetrics(blockId: BlockId, status: BlockStatus): Unit
----

`addUpdatedBlockStatusToTaskMetrics` link:spark-taskscheduler-taskcontext.adoc#get[takes an active `TaskContext`] (if available) and link:spark-taskscheduler-taskmetrics.adoc#incUpdatedBlockStatuses[records updated `BlockStatus` for `Block`] (in the link:spark-taskscheduler-taskcontext.adoc#taskMetrics[task's `TaskMetrics`]).

NOTE: `addUpdatedBlockStatusToTaskMetrics` is used when `BlockManager` <<doPutBytes, doPutBytes>> (for a block that was successfully stored), <<doPut, doPut>>, <<doPutIterator, doPutIterator>>, <<dropFromMemory, removes blocks from memory>> (possibly spilling it to disk) and <<removeBlock, removes block from memory and disk>>.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_blockManager_port]] `spark.blockManager.port`
| `0`
| Port to use for the block manager when a more specific setting for the driver or executors is not provided.

| [[spark_shuffle_sync]] `spark.shuffle.sync`
| `false`
| Controls whether link:spark-blockmanager-DiskBlockObjectWriter.adoc#commitAndGet[`DiskBlockObjectWriter` should force outstanding writes to disk when committing a single atomic block], i.e. all operating system buffers should synchronize with the disk to ensure that all changes to a file are in fact recorded in the storage.

|===
