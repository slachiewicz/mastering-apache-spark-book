== [[Dataset]] Dataset -- Strongly-Typed Structured Query with Encoder

*Dataset* is a strongly-typed data structure in Spark SQL that represents a structured query with link:spark-sql-Encoder.adoc[encoders].

.Dataset's Internals
image::images/spark-sql-Dataset.png[align="center"]

NOTE: Given the picture above, one could say that a `Dataset` is a pair of an link:spark-sql-Encoder.adoc[Encoder] and link:spark-sql-QueryExecution.adoc[QueryExecution] (that in turn is a link:spark-sql-LogicalPlan.adoc[LogicalPlan] in a link:spark-sql-SparkSession.adoc[SparkSession])

Datasets are _lazy_ and structured query expressions are only triggered when an action is invoked. Internally, a `Dataset` represents a link:spark-sql-LogicalPlan.adoc[logical plan] that describes the computation query required to produce the data (in a given link:spark-sql-SparkSession.adoc[session]).

A Dataset is a result of executing a query expression against data storage like files, Hive tables or JDBC databases. The structured query expression can be described by a SQL query, a Column-based SQL expression or a Scala/Java lambda function. And that is why Dataset operations are available in three variants.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = ...

scala> val dataset = spark.range(5)
dataset: org.apache.spark.sql.Dataset[Long] = [id: bigint]

// Variant 1: filter operator accepts a Scala function
dataset.filter(n => n % 2 == 0).count

// Variant 2: filter operator accepts a Column-based SQL expression
dataset.filter('value % 2 === 0).count

// Variant 3: filter operator accepts a SQL query
dataset.filter("value % 2 = 0").count
----

The Dataset API offers declarative and type-safe operators that makes for an improved experience for data processing (comparing to link:spark-sql-DataFrame.adoc[DataFrames] that were a set of index- or column name-based link:spark-sql-Row.adoc[Rows]).

[NOTE]
====
`Dataset` was first introduced in Apache Spark *1.6.0* as an experimental feature, and has since turned itself into a fully supported API.

As of Spark *2.0.0*, link:spark-sql-DataFrame.adoc[DataFrame] - the flagship data abstraction of previous versions of Spark SQL - is currently a _mere_ type alias for `Dataset[Row]`:

[source, scala]
----
type DataFrame = Dataset[Row]
----

See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45[package object sql].
====

`Dataset` offers convenience of RDDs with the performance optimizations of DataFrames and the strong static type-safety of Scala. The last feature of bringing the strong type-safety to link:spark-sql-DataFrame.adoc[DataFrame] makes Dataset so appealing. All the features together give you a more functional programming interface to work with structured data.

[source, scala]
----
scala> spark.range(1).filter('id === 0).explain(true)
== Parsed Logical Plan ==
'Filter ('id = 0)
+- Range (0, 1, splits=8)

== Analyzed Logical Plan ==
id: bigint
Filter (id#51L = cast(0 as bigint))
+- Range (0, 1, splits=8)

== Optimized Logical Plan ==
Filter (id#51L = 0)
+- Range (0, 1, splits=8)

== Physical Plan ==
*Filter (id#51L = 0)
+- *Range (0, 1, splits=8)

scala> spark.range(1).filter(_ == 0).explain(true)
== Parsed Logical Plan ==
'TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], unresolveddeserializer(newInstance(class java.lang.Long))
+- Range (0, 1, splits=8)

== Analyzed Logical Plan ==
id: bigint
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 1, splits=8)

== Optimized Logical Plan ==
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 1, splits=8)

== Physical Plan ==
*Filter <function1>.apply
+- *Range (0, 1, splits=8)
----

It is only with Datasets to have syntax and analysis checks at compile time (that was not possible using link:spark-sql-DataFrame.adoc[DataFrame], regular SQL queries or even RDDs).

Using `Dataset` objects turns `DataFrames` of link:spark-sql-Row.adoc[Row] instances into a `DataFrames` of case classes with proper names and types (following their equivalents in the case classes). Instead of using indices to access respective fields in a DataFrame and cast it to a type, all this is automatically handled by Datasets and checked by the Scala compiler.

Datasets use link:spark-sql-Optimizer.adoc[Catalyst Query Optimizer] and link:spark-sql-tungsten.adoc[Tungsten] to optimize query performance.

A `Dataset` object requires a link:spark-sql-SparkSession.adoc[SparkSession], a link:spark-sql-QueryExecution.adoc[QueryExecution] plan, and an link:spark-sql-Encoder.adoc[Encoder] (for fast serialization to and deserialization from link:spark-sql-InternalRow.adoc[InternalRow]).

If however a link:spark-sql-LogicalPlan.adoc[LogicalPlan] is used to <<creating-instance, create a `Dataset`>>, the logical plan is first link:spark-sql-SessionState.adoc#executePlan[executed] (using the current link:spark-sql-SessionState.adoc#executePlan[SessionState] in the `SparkSession`) that yields the link:spark-sql-QueryExecution.adoc[QueryExecution] plan.

A `Dataset` is <<Queryable, Queryable>> and `Serializable`, i.e. can be saved to a persistent storage.

NOTE: link:spark-sql-SparkSession.adoc[SparkSession] and link:spark-sql-QueryExecution.adoc[QueryExecution] are transient attributes of a `Dataset` and therefore do not participate in Dataset serialization. The only _firmly-tied_ feature of a `Dataset` is the link:spark-sql-Encoder.adoc[Encoder].

You can <<implicits, convert a type-safe Dataset to a "untyped" DataFrame>> or access the link:spark-sql-dataset-operators.adoc#rdd[RDD] that is generated after executing the query. It is supposed to give you a more pleasant experience while transitioning from the legacy RDD-based or DataFrame-based APIs you may have used in the earlier versions of Spark SQL or encourage migrating from Spark Core's RDD API to Spark SQL's Dataset API.

The default storage level for `Datasets` is link:spark-rdd-caching.adoc[MEMORY_AND_DISK] because recomputing the in-memory columnar representation of the underlying table is expensive. You can however link:spark-sql-caching.adoc#persist[persist a `Dataset`].

NOTE: Spark 2.0 has introduced a new query model called link:spark-structured-streaming.adoc[Structured Streaming] for continuous incremental execution of structured queries. That made possible to consider Datasets a static and bounded as well as streaming and unbounded data sets with a single unified API for different execution models.

A `Dataset` is link:spark-sql-dataset-operators.adoc#isLocal[local] if it was created from local collections using link:spark-sql-SparkSession.adoc#emptyDataset[SparkSession.emptyDataset] or link:spark-sql-SparkSession.adoc#createDataset[SparkSession.createDataset] methods and their derivatives like <<toDF,toDF>>. If so, the queries on the Dataset can be optimized and run locally, i.e. without using Spark executors.

NOTE: `Dataset` makes sure that the underlying `QueryExecution` is link:spark-sql-QueryExecution.adoc#analyzed[analyzed] and link:spark-sql-Analyzer-CheckAnalysis.adoc#checkAnalysis[checked].

[[properties]]
[[attributes]]
.Dataset's Properties
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[boundEnc]] `boundEnc`
| link:spark-sql-ExpressionEncoder.adoc[ExpressionEncoder]

Used when...FIXME

| [[exprEnc]] `exprEnc`
| Implicit link:spark-sql-ExpressionEncoder.adoc[ExpressionEncoder]

Used when...FIXME

| [[logicalPlan]] `logicalPlan`
| link:spark-sql-LogicalPlan.adoc[Logical plan]

| [[rdd]] `rdd`
a| (lazily-created) link:spark-rdd.adoc[RDD] of JVM objects of type `T` (as converted from rows in `Dataset` in the link:spark-sql-InternalRow.adoc[internal binary row format]).

[source, scala]
----
rdd: RDD[T]
----

NOTE: `rdd` gives `RDD` with the extra execution step to convert rows from their internal binary row format to JVM objects that will impact the JVM memory as the objects are inside JVM (while were outside before). You should not use `rdd` directly.

Internally, `rdd` first link:spark-sql-CatalystSerde.adoc#deserialize[creates a new logical plan that deserializes] the Dataset's <<logicalPlan, logical plan>>.

[source, scala]
----
val dataset = spark.range(5).withColumn("group", 'id % 2)
scala> dataset.rdd.toDebugString
res1: String =
(8) MapPartitionsRDD[8] at rdd at <console>:26 [] // <-- extra deserialization step
 |  MapPartitionsRDD[7] at rdd at <console>:26 []
 |  MapPartitionsRDD[6] at rdd at <console>:26 []
 |  MapPartitionsRDD[5] at rdd at <console>:26 []
 |  ParallelCollectionRDD[4] at rdd at <console>:26 []

// Compare with a more memory-optimized alternative
// Avoids copies and has no schema
scala> dataset.queryExecution.toRdd.toDebugString
res2: String =
(8) MapPartitionsRDD[11] at toRdd at <console>:26 []
|  MapPartitionsRDD[10] at toRdd at <console>:26 []
|  ParallelCollectionRDD[9] at toRdd at <console>:26 []
----

`rdd` then requests `SessionState` to link:spark-sql-SessionState.adoc#executePlan[execute the logical plan] to get the corresponding link:spark-sql-QueryExecution.adoc#toRdd[RDD of binary rows].

NOTE: `rdd` uses <<sparkSession, SparkSession>> to link:spark-sql-SparkSession.adoc#sessionState[access `SessionState`].

`rdd` then requests the Dataset's <<exprEnc, ExpressionEncoder>> for the link:spark-sql-Expression.adoc#dataType[data type] of the rows (using link:spark-sql-ExpressionEncoder.adoc#deserializer[deserializer] expression) and link:spark-rdd-transformations.adoc#mapPartitions[maps over them (per partition)] to create records of the expected type `T`.

NOTE: `rdd` is at the "boundary" between the internal binary row format and the JVM type of the dataset. Avoid the extra deserialization step to lower JVM memory requirements of your Spark application.

| [[sqlContext]] `sqlContext`
| Lazily-created link:spark-sql-SQLContext.adoc[SQLContext]

Used when...FIXME
|===

=== [[resolve]] `resolve` Internal Method

[source, scala]
----
resolve(colName: String): NamedExpression
----

CAUTION: FIXME

=== [[creating-instance]] Creating Dataset Instance

`Dataset` takes the following when created:

* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]
* [[queryExecution]] link:spark-sql-QueryExecution.adoc[QueryExecution]
* [[encoder]] link:spark-sql-Encoder.adoc[Encoder] for the type `T` of the records

NOTE: You can also create a `Dataset` using link:spark-sql-LogicalPlan.adoc[LogicalPlan] that is immediately link:spark-sql-SessionState.adoc#executePlan[executed using `SessionState`].

Internally, `Dataset` requests <<queryExecution, QueryExecution>> to link:spark-sql-QueryExecution.adoc#assertAnalyzed[analyze itself].

`Dataset` initializes the <<internal-registries, internal registries and counters>>.

=== [[isLocal]] Is Dataset Local? -- `isLocal` Method

[source, scala]
----
isLocal: Boolean
----

`isLocal` flag is enabled (i.e. `true`) when operators like `collect` or `take` could be run locally, i.e. without using executors.

Internally, `isLocal` checks whether the logical query plan of a `Dataset` is link:spark-sql-LogicalPlan-LocalRelation.adoc[LocalRelation].

=== [[isStreaming]] Is Dataset Streaming? -- `isStreaming` method

[source, scala]
----
isStreaming: Boolean
----

`isStreaming` is enabled (i.e. `true`) when the logical plan link:spark-sql-LogicalPlan.adoc#isStreaming[is streaming].

Internally, `isStreaming` takes the Dataset's link:spark-sql-LogicalPlan.adoc[logical plan] and gives link:spark-sql-LogicalPlan.adoc#isStreaming[whether the plan is streaming or not].

=== [[implicits]][[toDS]][[toDF]] Implicit Type Conversions to Datasets -- `toDS` and `toDF` methods

`DatasetHolder` case class offers three methods that do the conversions from `Seq[T]` or `RDD[T]` types to a `Dataset[T]`:

* `toDS(): Dataset[T]`
* `toDF(): DataFrame`
* `toDF(colNames: String*): DataFrame`

NOTE: `DataFrame` is a _mere_ type alias for `Dataset[Row]` since Spark *2.0.0*.

`DatasetHolder` is used by `SQLImplicits` that is available to use after link:spark-sql-SparkSession.adoc#implicits[importing `implicits` object of `SparkSession`].

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

scala> val ds = Seq("I am a shiny Dataset!").toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]

scala> val df = Seq("I am an old grumpy DataFrame!").toDF
df: org.apache.spark.sql.DataFrame = [value: string]

scala> val df = Seq("I am an old grumpy DataFrame!").toDF("text")
df: org.apache.spark.sql.DataFrame = [text: string]

scala> val ds = sc.parallelize(Seq("hello")).toDS
ds: org.apache.spark.sql.Dataset[String] = [value: string]
----

[NOTE]
====
This import of `implicits` object's values is automatically executed in link:spark-shell.adoc[Spark Shell] and so you don't need to do anything but use the conversions.

```
scala> spark.version
res11: String = 2.0.0

scala> :imports
 1) import spark.implicits._  (59 terms, 38 are implicit)
 2) import spark.sql          (1 terms)
```
====

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

case class Token(name: String, productId: Int, score: Double)
val data = Seq(
  Token("aaa", 100, 0.12),
  Token("aaa", 200, 0.29),
  Token("bbb", 200, 0.53),
  Token("bbb", 300, 0.42))

// Transform data to a Dataset[Token]
// It doesn't work with type annotation
// https://issues.apache.org/jira/browse/SPARK-13456
val ds = data.toDS

// ds: org.apache.spark.sql.Dataset[Token] = [name: string, productId: int ... 1 more field]

// Transform data into a DataFrame with no explicit schema
val df = data.toDF

// Transform DataFrame into a Dataset
val ds = df.as[Token]

scala> ds.show
+----+---------+-----+
|name|productId|score|
+----+---------+-----+
| aaa|      100| 0.12|
| aaa|      200| 0.29|
| bbb|      200| 0.53|
| bbb|      300| 0.42|
+----+---------+-----+

scala> ds.printSchema
root
 |-- name: string (nullable = true)
 |-- productId: integer (nullable = false)
 |-- score: double (nullable = false)

// In DataFrames we work with Row instances
scala> df.map(_.getClass.getName).show(false)
+--------------------------------------------------------------+
|value                                                         |
+--------------------------------------------------------------+
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
|org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema|
+--------------------------------------------------------------+

// In Datasets we work with case class instances
scala> ds.map(_.getClass.getName).show(false)
+---------------------------+
|value                      |
+---------------------------+
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
|$line40.$read$$iw$$iw$Token|
+---------------------------+
----

==== [[toDS-internals]] Internals of toDS

Internally, the Scala compiler makes `toDS` implicitly available to any `Seq[T]` (using `SQLImplicits.localSeqToDatasetHolder` implicit method).

NOTE: This and other implicit methods are in scope whenever you do `import spark.implicits._`.

The input `Seq[T]` is converted into `Dataset[T]` by means of link:spark-sql-SQLContext.adoc#createDataset[SQLContext.createDataset] that in turn passes all calls on to link:spark-sql-SparkSession.adoc#createDataset[SparkSession.createDataset]. Once created, the `Dataset[T]` is wrapped in `DatasetHolder[T]` with `toDS` that just returns the input `ds`.

=== [[Queryable]] Queryable

CAUTION: FIXME

=== [[withNewRDDExecutionId]] `withNewRDDExecutionId` Internal Method

[source, scala]
----
withNewRDDExecutionId[U](body: => U): U
----

`withNewRDDExecutionId` executes the input `body` action under link:spark-sql-SQLExecution.adoc#withNewExecutionId[new execution id].

CAUTION: FIXME What's the difference between `withNewRDDExecutionId` and <<withNewExecutionId, withNewExecutionId>>?

NOTE: `withNewRDDExecutionId` is used when `Dataset` executes link:spark-sql-dataset-operators.adoc#foreach[foreach] and link:spark-sql-dataset-operators.adoc#foreachPartition[foreachPartition] actions.

=== [[ofRows]] Creating DataFrame -- `ofRows` Internal Method

[source, scala]
----
ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame
----

NOTE: `ofRows` is a `private[sql]` operator that can only be accessed from code in `org.apache.spark.sql` package. It is not a part of ``Dataset``'s public API.

`ofRows` returns link:spark-sql-DataFrame.adoc[DataFrame] (which is the type alias for `Dataset[Row]`). `ofRows` uses link:spark-sql-RowEncoder.adoc[RowEncoder] to convert the schema (based on the input `logicalPlan` logical plan).

Internally, `ofRows` link:spark-sql-SessionState.adoc#executePlan[prepares the input `logicalPlan` for execution] and creates a `Dataset[Row]` with the current link:spark-sql-SparkSession.adoc[SparkSession], the link:spark-sql-QueryExecution.adoc[QueryExecution] and link:spark-sql-RowEncoder.adoc[RowEncoder].

=== [[withAction]] `withAction` Internal Method

[source, scala]
----
withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U)
----

`withAction`...FIXME

NOTE: `withAction` is used when...FIXME

=== [[withNewExecutionId]] Tracking Multi-Job Structured Query Execution (PySpark) -- `withNewExecutionId` Internal Method

[source, scala]
----
withNewExecutionId[U](body: => U): U
----

`withNewExecutionId` executes the input `body` action under link:spark-sql-SQLExecution.adoc#withNewExecutionId[new execution id].

NOTE: `withNewExecutionId` sets a unique execution id so that all Spark jobs belong to the `Dataset` action execution.

[NOTE]
====
`withNewExecutionId` is used exclusively when `Dataset` is executing Python-based actions (i.e. `collectToPython`, `collectAsArrowToPython` and `toPythonIterator`) that are not of much interest in this gitbook.

Feel free to contact me at jacek@japila.pl if you think I should re-consider my decision.
====

=== [[i-want-more]] Further reading or watching

* (video) https://youtu.be/i7l3JQRx7Qw[Structuring Spark: DataFrames, Datasets, and Streaming]
