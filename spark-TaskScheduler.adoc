== [[TaskScheduler]] TaskScheduler Contract -- Spark Schedulers

`TaskScheduler` is the <<contract, abstraction>> of <<implementations, Spark schedulers>> that can <<submitTasks, submit tasks for execution>> in a Spark application (per <<schedulingMode, scheduling policy>>).

.TaskScheduler and SparkContext
image::images/sparkstandalone-sparkcontext-taskscheduler-schedulerbackend.png[align="center"]

NOTE: `TaskScheduler` works closely with link:spark-dagscheduler.adoc[DAGScheduler] that <<submitTasks, submits sets of tasks for execution>> (for every stage in a Spark job).

`TaskScheduler` can track the executors available in a Spark application using <<executorHeartbeatReceived, executorHeartbeatReceived>> and <<executorLost, executorLost>> interceptors (that inform about active and lost executors, respectively).

[[contract]]
.TaskScheduler Contract (Abstract Methods Only)
[cols="1m,3",options="header",width="100%"]
|===
| Method
| Description

| applicationAttemptId
a| [[applicationAttemptId]]

[source, scala]
----
applicationAttemptId(): Option[String]
----

*Unique identifier of an (execution) attempt* of the Spark application

Used exclusively when `SparkContext` is <<spark-SparkContext-creating-instance-internals.adoc#_applicationAttemptId, created>>

| cancelTasks
a| [[cancelTasks]]

[source, scala]
----
cancelTasks(
  stageId: Int,
  interruptThread: Boolean): Unit
----

Cancels all the tasks of a given <<spark-DAGScheduler-Stage.adoc#, stage>>

Used exclusively when `DAGScheduler` is requested to <<spark-dagscheduler.adoc#failJobAndIndependentStages, failJobAndIndependentStages>>

| defaultParallelism
a| [[defaultParallelism]]

[source, scala]
----
defaultParallelism(): Int
----

*Default level of parallelism*

Used exclusively when `SparkContext` is requested for the <<spark-SparkContext.adoc#defaultParallelism, default level of parallelism>>

| executorHeartbeatReceived
a| [[executorHeartbeatReceived]]

[source, scala]
----
executorHeartbeatReceived(
  execId: String,
  accumUpdates: Array[(Long, Seq[AccumulatorV2[_, _]])],
  blockManagerId: BlockManagerId): Boolean
----

Handles heartbeats (with task metrics) from executors

Expected to return `true` when the executor `execId` is managed by the `TaskScheduler`. `false` indicates that the link:spark-Executor.adoc#reportHeartBeat[block manager (on the executor) should re-register].

Used exclusively when `HeartbeatReceiver` RPC endpoint is requested to link:spark-HeartbeatReceiver.adoc#Heartbeat[handle a Heartbeat and task metrics from an executor]

| executorLost
a| [[executorLost]]

[source, scala]
----
executorLost(
  executorId: String,
  reason: ExecutorLossReason): Unit
----

Handles an executor lost event

Used when:

* `HeartbeatReceiver` RPC endpoint is requested to link:spark-HeartbeatReceiver.adoc#expireDeadHosts[expireDeadHosts]

* `DriverEndpoint` RPC endpoint is requested to link:spark-CoarseGrainedSchedulerBackend-DriverEndpoint.adoc#removeExecutor[removes] (_forgets_) and link:spark-CoarseGrainedSchedulerBackend-DriverEndpoint.adoc#disableExecutor[disables] a malfunctioning executor (i.e. either lost or blacklisted for some reason)

* Spark on Mesos' `MesosFineGrainedSchedulerBackend` is requested to `recordSlaveLost`

| killAllTaskAttempts
a| [[killAllTaskAttempts]]

[source, scala]
----
killAllTaskAttempts(
  stageId: Int,
  interruptThread: Boolean,
  reason: String): Unit
----

Used when:

* `DAGScheduler` is requested to <<spark-dagscheduler.adoc#handleTaskCompletion, handleTaskCompletion>>

* `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#cancelTasks, cancel all the tasks of a stage>>

| killTaskAttempt
a| [[killTaskAttempt]]

[source, scala]
----
killTaskAttempt(
  taskId: Long,
  interruptThread: Boolean,
  reason: String): Boolean
----

Used exclusively when `DAGScheduler` is requested to <<spark-dagscheduler.adoc#killTaskAttempt, killTaskAttempt>>

| rootPool
a| [[rootPool]]

[source, scala]
----
rootPool: Pool
----

Top-level (root) link:spark-taskscheduler-pool.adoc[schedulable pool]

Used when:

* `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#initialize, initialize>>

* `SparkContext` is requested to <<spark-SparkContext.adoc#getAllPools, getAllPools>> and <<spark-SparkContext.adoc#getPoolForName, getPoolForName>>

* `TaskSchedulerImpl` is requested to <<spark-TaskSchedulerImpl.adoc#resourceOffers, resourceOffers>>, <<spark-TaskSchedulerImpl.adoc#checkSpeculatableTasks, checkSpeculatableTasks>>, and <<spark-TaskSchedulerImpl.adoc#removeExecutor, removeExecutor>>

| schedulingMode
a| [[schedulingMode]]

[source, scala]
----
schedulingMode: SchedulingMode
----

<<spark-taskscheduler-schedulingmode.adoc#, Scheduling mode>>

Used when:

* `TaskSchedulerImpl` is <<spark-TaskSchedulerImpl.adoc#rootPool, created>> and <<spark-TaskSchedulerImpl.adoc#initialize, initialized>>

* `SparkContext` is requested to <<spark-SparkContext.adoc#getSchedulingMode, getSchedulingMode>>

| setDAGScheduler
a| [[setDAGScheduler]]

[source, scala]
----
setDAGScheduler(dagScheduler: DAGScheduler): Unit
----

Associates a link:spark-dagscheduler.adoc[DAGScheduler]

Used exclusively when `DAGScheduler` is link:spark-dagscheduler.adoc#creating-instance[created]

| start
a| [[start]]

[source, scala]
----
start(): Unit
----

Starts the `TaskScheduler`

Used exclusively when `SparkContext` is link:spark-SparkContext-creating-instance-internals.adoc#taskScheduler-start[created]

| stop
a| [[stop]]

[source, scala]
----
stop(): Unit
----

Stops the `TaskScheduler`

Used exclusively when `DAGScheduler` is requested to link:spark-dagscheduler.adoc#stop[stop]

| submitTasks
a| [[submitTasks]]

[source, scala]
----
submitTasks(taskSet: TaskSet): Unit
----

Submits tasks (as link:spark-taskscheduler-tasksets.adoc[TaskSet]) for execution

Used exclusively when `DAGScheduler` is requested to link:spark-dagscheduler.adoc#submitMissingTasks[submit missing tasks (of a stage)]

| workerRemoved
a| [[workerRemoved]]

[source, scala]
----
workerRemoved(
  workerId: String,
  host: String,
  message: String): Unit
----

Used exclusively when `DriverEndpoint` is requested to <<spark-CoarseGrainedSchedulerBackend-DriverEndpoint.adoc#removeWorker, handle a RemoveWorker event>>

|===

[[implementations]]
.TaskSchedulers (All Available Implementations)
[cols="1,3",options="header",width="100%"]
|===
| TaskScheduler
| Description

| <<spark-TaskSchedulerImpl.adoc#, TaskSchedulerImpl>>
| [[TaskSchedulerImpl]] Default Spark scheduler

| <<yarn/spark-yarn-yarnscheduler.adoc#, YarnScheduler>>
| [[YarnScheduler]] TaskScheduler for <<spark-submit.adoc#deploy-mode, client>> deploy mode in <<yarn/README.adoc#, Spark on YARN>>

| <<yarn/spark-yarn-yarnclusterscheduler.adoc#, YarnClusterScheduler>>
| [[YarnClusterScheduler]] TaskScheduler for <<spark-submit.adoc#deploy-mode, cluster>> deploy mode in <<yarn/README.adoc#, Spark on YARN>>

|===

=== [[postStartHook]] Post-Start Initialization -- `postStartHook` Method

[source, scala]
----
postStartHook(): Unit
----

`postStartHook` does nothing by default, but allows <<implementations, custom implementations>> for some additional post-start initialization.

[NOTE]
====
`postStartHook` is used when:

* `SparkContext` is link:spark-SparkContext-creating-instance-internals.adoc#postStartHook[created] (right before considered fully initialized)

* Spark on YARN's `YarnClusterScheduler` is requested to <<yarn/spark-yarn-yarnclusterscheduler.adoc#postStartHook, postStartHook>>
====

=== [[applicationId]][[appId]] Unique Identifier of Spark Application -- `applicationId` Method

[source, scala]
----
applicationId(): String
----

`applicationId` is the *unique identifier* of the Spark application and defaults to *spark-application-[currentTimeMillis]*.

NOTE: `applicationId` is used exclusively when `SparkContext` is link:spark-SparkContext-creating-instance-internals.adoc#_applicationId[created].

=== [[lifecycle]] TaskScheduler's Lifecycle

A `TaskScheduler` is created while link:spark-SparkContext.adoc#creating-instance[SparkContext is being created] (by calling link:spark-SparkContext.adoc#createTaskScheduler[SparkContext.createTaskScheduler] for a given link:spark-deployment-environments.adoc[master URL] and link:spark-submit.adoc#deploy-mode[deploy mode]).

.TaskScheduler uses SchedulerBackend to support different clusters
image::diagrams/taskscheduler-uses-schedulerbackend.png[align="center"]

At this point in SparkContext's lifecycle, the internal `_taskScheduler` points at the `TaskScheduler` (and it is "announced" by sending a blocking link:spark-HeartbeatReceiver.adoc#TaskSchedulerIsSet[`TaskSchedulerIsSet` message to HeartbeatReceiver RPC endpoint]).

The <<start, TaskScheduler is started>> right after the blocking `TaskSchedulerIsSet` message receives a response.

The <<applicationId, application ID>> and the <<applicationAttemptId, application's attempt ID>> are set at this point (and `SparkContext` uses the application id to set link:spark-SparkConf.adoc#spark.app.id[spark.app.id] Spark property, and configure link:spark-webui-SparkUI.adoc[SparkUI], and link:spark-BlockManager.adoc[BlockManager]).

CAUTION: FIXME The application id is described as "associated with the job." in TaskScheduler, but I think it is "associated with the application" and you can have many jobs per application.

Right before SparkContext is fully initialized, <<postStartHook, TaskScheduler.postStartHook>> is called.

The internal `_taskScheduler` is cleared (i.e. set to `null`) while link:spark-SparkContext.adoc#stop[SparkContext is being stopped].

<<stop, TaskScheduler is stopped>> while link:spark-dagscheduler.adoc#stop[DAGScheduler is being stopped].

WARNING: FIXME If it is SparkContext to start a TaskScheduler, shouldn't SparkContext stop it too? Why is this the way it is now?
