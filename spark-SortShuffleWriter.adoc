== [[SortShuffleWriter]] SortShuffleWriter -- Fallback ShuffleWriter

`SortShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that is used when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` returns a `ShuffleWriter` for `ShuffleHandle`] (and the more specialized link:spark-BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter] and link:spark-UnsafeShuffleWriter.adoc[UnsafeShuffleWriter] could not be used).

NOTE: `SortShuffleWriter` is parameterized by types for `K` keys, `V` values, and `C` combiner values.

[[internal-registries]]
.SortShuffleWriter's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[mapStatus]] `mapStatus`
| `MapStatus` the <<write, `SortShuffleWriter` has recently persisted>> (as a shuffle partitioned file in disk store).

NOTE: Since <<write, write>> does not return a value, `mapStatus` attribute is used to be returned when <<stop, `SortShuffleWriter` is closed>>.

| [[stopping]] `stopping`
| Internal flag to mark that <<stop, `SortShuffleWriter` is closed>>.
|===

[TIP]
====
Enable `ERROR` logging level for `org.apache.spark.shuffle.sort.SortShuffleWriter` logger to see what happens in `SortShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.SortShuffleWriter=ERROR
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating SortShuffleWriter Instance

`SortShuffleWriter` takes the following when created:

1. link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver]
2. link:spark-BaseShuffleHandle.adoc[BaseShuffleHandle]
3. `mapId` -- the mapper task id
4. link:spark-taskscheduler-taskcontext.adoc[TaskContext]

NOTE: `SortShuffleWriter` is created when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` returns a `ShuffleWriter` for the fallback `BaseShuffleHandle`].

=== [[write]] Writing Records Into Shuffle Partitioned File In Disk Store -- `write` Method

[source, scala]
----
write(records: Iterator[Product2[K, V]]): Unit
----

NOTE: `write` is part of link:spark-ShuffleWriter.adoc#contract[ShuffleWriter contract] to write a sequence of records (for a RDD partition).

Internally, `write` creates a link:spark-ExternalSorter.adoc[ExternalSorter] with the types `K, V, C` or `K, V, V` depending on link:spark-rdd-ShuffleDependency.adoc#mapSideCombine[`mapSideCombine` flag of the `ShuffleDependency`] being enabled or not, respectively.

NOTE: link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] is defined when <<creating-instance, `SortShuffleWriter` is created>> (as the link:spark-BaseShuffleHandle.adoc#dependency[`dependency` of `BaseShuffleHandle`]).

NOTE: `write` makes sure that link:spark-rdd-ShuffleDependency.adoc#aggregator[`Aggregator` is defined for `ShuffleDependency`] when `mapSideCombine` flag is enabled.

`write` link:spark-ExternalSorter.adoc#insertAll[inserts all the records to `ExternalSorter`]

`write` link:spark-IndexShuffleBlockResolver.adoc#getDataFile[requests `IndexShuffleBlockResolver` for the shuffle data output file] (for the link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and `mapId`) and creates a temporary file for the shuffle data file in the same directory.

`write` creates a link:spark-blockdatamanager.adoc#ShuffleBlockId[ShuffleBlockId] (for the link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and `mapId` and the special `IndexShuffleBlockResolver.NOOP_REDUCE_ID` reduce id).

`write` link:spark-ExternalSorter.adoc#writePartitionedFile[requests `ExternalSorter` to write all the records (previously inserted in) into the temporary partitioned file in the disk store].

`write` link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[requests `IndexShuffleBlockResolver` to write an index file] (for the temporary partitioned file).

`write` creates a `MapStatus` (with the link:spark-blockmanager.adoc#shuffleServerId[location of the shuffle server] that serves the executor's shuffle files and the sizes of the shuffle partitioned file's partitions).

NOTE: The newly-created `MapStatus` is available as <<mapStatus, mapStatus>> internal attribute.

NOTE: `write` does not handle exceptions so when they occur, they will break the processing.

In the end, `write` deletes the temporary partitioned file. You may see the following ERROR message in the logs if `write` did not manage to do so:

```
ERROR Error while deleting temp file [path]
```

=== [[stop]] Closing SortShuffleWriter (and Calculating MapStatus) -- `stop` Method

[source, scala]
----
stop(success: Boolean): Option[MapStatus]
----

NOTE: `stop` is part of link:spark-ShuffleWriter.adoc#contract[ShuffleWriter contract] to close itself (and return the last written link:spark-MapStatus.adoc[MapStatus]).

`stop` turns <<stopping, stopping>> flag on and returns the internal <<mapStatus, mapStatus>> if the input `success` is enabled.

Otherwise, when <<stopping, stopping>> flag is already enabled or the input `success` is disabled, `stop` returns no `MapStatus` (i.e. `None`).

In the end, `stop` link:spark-ExternalSorter.adoc#stop[stops the `ExternalSorter`] and increments the shuffle write time task metrics.
