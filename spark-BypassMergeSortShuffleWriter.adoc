== [[BypassMergeSortShuffleWriter]] BypassMergeSortShuffleWriter

`BypassMergeSortShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that link:spark-taskscheduler-ShuffleMapTask.adoc[ShuffleMapTask] uses to <<write, write records into one single shuffle block data file>> when the link:spark-taskscheduler-ShuffleMapTask.adoc#runTask[task runs for a `ShuffleDependency`].

.BypassMergeSortShuffleWriter writing records (for ShuffleMapTask) using DiskBlockObjectWriters
image::images/spark-BypassMergeSortShuffleWriter-write.png[align="center"]

<<creating-instance, `BypassMergeSortShuffleWriter` is created>> exclusively when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` selects a `ShuffleWriter`] (for a link:spark-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]).

TIP: Review the link:spark-SortShuffleManager.adoc#shouldBypassMergeSort[conditions `SortShuffleManager` uses to select `BypassMergeSortShuffleHandle` for a `ShuffleHandle`].

[[internal-registries]]
.BypassMergeSortShuffleWriter's Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[numPartitions]] `numPartitions`
| FIXME

| [[partitionWriters]] `partitionWriters`
| FIXME

| [[partitionWriterSegments]] `partitionWriterSegments`
| FIXME

| [[shuffleBlockResolver]] `shuffleBlockResolver`
| link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver].

Initialized when <<creating-instance, `BypassMergeSortShuffleWriter` is created>>.

Used when <<write, `BypassMergeSortShuffleWriter` writes records>>.

| [[mapStatus]] `mapStatus`
| link:spark-MapStatus.adoc[MapStatus] that <<stop, `BypassMergeSortShuffleWriter` returns when stopped>>

Initialized every time `BypassMergeSortShuffleWriter` <<write, writes records>>.

Used when <<stop, `BypassMergeSortShuffleWriter` stops>> (with `success` enabled) as a marker if <<write, any records were written>> and <<stop, returned if they did>>.

| [[partitionLengths]] `partitionLengths`
| Temporary array of partition lengths after records are <<write, written to a shuffle system>>.

Initialized every time `BypassMergeSortShuffleWriter` <<write, writes records>> before passing it in to link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[IndexShuffleBlockResolver]). After `IndexShuffleBlockResolver` finishes, it is used to initialize <<mapStatus, mapStatus>> internal property.

| [[transferToEnabled]] `transferToEnabled`
| Internal flag that controls the use of Java New I/O when <<writePartitionedFile, `BypassMergeSortShuffleWriter` concatenates per-partition shuffle files into a single shuffle block data file>>.

Specified when <<creating-instance, `BypassMergeSortShuffleWriter` is created>> and controlled by link:spark-UnsafeShuffleWriter.adoc#spark_file_transferTo[spark.file.transferTo] Spark property. Enabled by default.

|===

[TIP]
====
Enable `ERROR` logging level for `org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter` logger to see what happens in `BypassMergeSortShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter=ERROR
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[creating-instance]] Creating BypassMergeSortShuffleWriter Instance

`BypassMergeSortShuffleWriter` takes the following when created:

1. link:spark-blockmanager.adoc[BlockManager]
2. link:spark-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver]
3. link:spark-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle]
4. `mapId`
5. link:spark-taskscheduler-taskcontext.adoc[TaskContext]
6. link:spark-SparkConf.adoc[SparkConf]

[[fileBufferSize]]
`BypassMergeSortShuffleWriter` uses link:spark-ExternalSorter.adoc#spark_shuffle_file_buffer[spark.shuffle.file.buffer] (for `fileBufferSize` as `32k` by default) and link:spark-UnsafeShuffleWriter.adoc#spark_file_transferTo[spark.file.transferTo] (for <<transferToEnabled, transferToEnabled>> internal flag which is enabled by default) Spark properties.

`BypassMergeSortShuffleWriter` initializes the <<internal-registries, internal registries and counters>>.

=== [[write]] Writing Records (Into One Single Shuffle Block Data File) -- `write` Method

[source, java]
----
void write(Iterator<Product2<K, V>> records) throws IOException
----

NOTE: `write` is part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract] to write a sequence of records to a shuffle system.

Internally, when the input `records` iterator has no more records, `write` creates an empty <<partitionLengths, partitionLengths>> internal array of `numPartitions` size.

`write` then link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[requests the internal `IndexShuffleBlockResolver` to write shuffle index and data files] (with `dataTmp` as `null`) and sets the internal `mapStatus` (with the address of link:spark-blockmanager.adoc[BlockManager] in use and <<partitionLengths, partitionLengths>>).

However, when there are records to write, `write` creates a new link:spark-Serializer.adoc[Serializer].

NOTE: link:spark-Serializer.adoc[Serializer] was specified when <<creating-instance, `BypassMergeSortShuffleWriter` was created>> and is exactly the `Serializer` of the link:spark-rdd-ShuffleDependency.adoc#serializer[ShuffleDependency].

`write` initializes <<partitionWriters, partitionWriters>> internal array of link:spark-blockmanager-DiskBlockObjectWriter.adoc[DiskBlockObjectWriters] for <<numPartitions, every partition>>.

For <<numPartitions, every partition>>, `write` link:spark-DiskBlockManager.adoc#createTempShuffleBlock[requests `DiskBlockManager` for a temporary shuffle block and its file].

NOTE: `write` uses link:spark-blockmanager.adoc#diskBlockManager[`BlockManager` to access `DiskBlockManager`]. `BlockManager` was specified when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

`write` link:spark-blockmanager.adoc#getDiskWriter[requests `BlockManager` for a `DiskBlockObjectWriter`] (for the temporary `blockId` and `file`, link:spark-SerializerInstance.adoc[SerializerInstance], `fileBufferSize` and `writeMetrics`).

After link:spark-blockmanager-DiskBlockObjectWriter.adoc[DiskBlockObjectWriters] were created, `write` link:spark-taskmetrics-ShuffleWriteMetrics.adoc#incWriteTime[increments shuffle write time].

`write` initializes <<partitionWriterSegments, partitionWriterSegments>> with `FileSegment` for <<numPartitions, every partition>>.

`write` takes records serially, i.e. record by record, and, after link:spark-rdd-Partitioner.adoc#getPartition[computing the partition for a key], link:spark-blockmanager-DiskBlockObjectWriter.adoc#write[requests the corresponding `DiskBlockObjectWriter` to write them].

NOTE: `write` uses <<partitionWriters, partitionWriters>> internal array of `DiskBlockObjectWriter` indexed by partition number.

NOTE: `write` uses the link:spark-rdd-ShuffleDependency.adoc#partitioner[`Partitioner` from the `ShuffleDependency`] for which <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

NOTE: `write` initializes <<partitionWriters, partitionWriters>> with <<numPartitions, numPartitions>> number of `DiskBlockObjectWriters`.

After all the `records` have been written, `write` requests every `DiskBlockObjectWriter` to `commitAndGet` and saves the commit results in <<partitionWriterSegments, partitionWriterSegments>>. `write` link:spark-blockmanager-DiskBlockObjectWriter.adoc#close[closes every `DiskBlockObjectWriter`].

`write` link:spark-IndexShuffleBlockResolver.adoc#getDataFile[requests `IndexShuffleBlockResolver` for the shuffle block data file] for `shuffleId` and `mapId`.

NOTE: `IndexShuffleBlockResolver` was defined when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

`write` creates a temporary shuffle block data file and <<writePartitionedFile, writes the per-partition shuffle files to it>>.

NOTE: This is the moment when `BypassMergeSortShuffleWriter` concatenates per-partition shuffle file segments into one single map shuffle data file.

In the end, `write` link:spark-IndexShuffleBlockResolver.adoc#writeIndexFileAndCommit[requests `IndexShuffleBlockResolver` to write shuffle index and data files] for the `shuffleId` and `mapId` (with `partitionLengths` and the temporary file) and creates a new <<mapStatus, mapStatus>> (with the link:spark-blockmanager.adoc#shuffleServerId[location of the `BlockManager`] and <<partitionLengths, partitionLengths>>).

=== [[writePartitionedFile]] Concatenating Per-Partition Files Into Single File (and Tracking Write Time) -- `writePartitionedFile` Internal Method

[source, scala]
----
long[] writePartitionedFile(File outputFile) throws IOException
----

`writePartitionedFile` creates a file output stream for the input `outputFile` in append mode.

NOTE: `writePartitionedFile` uses Java's https://docs.oracle.com/javase/8/docs/api/java/io/FileOutputStream.html[java.io.FileOutputStream] to create a file output stream.

`writePartitionedFile` starts tracking write time (as `writeStartTime`).

For every <<numPartitions, numPartitions>> partition, `writePartitionedFile` takes the file from the `FileSegment` (from <<partitionWriterSegments, partitionWriterSegments>>) and creates a file input stream to read raw bytes.

NOTE: `writePartitionedFile` uses Java's https://docs.oracle.com/javase/8/docs/api/java/io/FileInputStream.html[java.io.FileInputStream] to create a file input stream.

`writePartitionedFile` then <<copyStream, copies the raw bytes from each partition segment input stream to `outputFile`>> (possibly using Java New I/O per <<transferToEnabled, transferToEnabled>> flag set when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>) and records the length of the shuffle data file (in `lengths` internal array).

NOTE: `transferToEnabled` is controlled by link:spark-UnsafeShuffleWriter.adoc#spark_file_transferTo[spark.file.transferTo] Spark property and is enabled (i.e. `true`) by default.

In the end, `writePartitionedFile` link:spark-taskmetrics-ShuffleWriteMetrics.adoc#incWriteTime[increments shuffle write time], clears <<partitionWriters, partitionWriters>> array and returns the lengths of the shuffle data files per partition.

NOTE: `writePartitionedFile` uses `ShuffleWriteMetrics` to track shuffle write time that was created when <<creating-instance, `BypassMergeSortShuffleWriter` was created>>.

NOTE: `writePartitionedFile` is used exclusively when `BypassMergeSortShuffleWriter` <<write, writes records>>.

=== [[copyStream]] Copying Raw Bytes Between Input Streams (Possibly Using Java New I/O) -- `Utils.copyStream` Method

[source, scala]
----
copyStream(
  in: InputStream,
  out: OutputStream,
  closeStreams: Boolean = false,
  transferToEnabled: Boolean = false): Long
----

`copyStream` branches off depending on the type of `in` and `out` streams, i.e. whether they are both `FileInputStream` with `transferToEnabled` input flag is enabled.

If they are both `FileInputStream` with `transferToEnabled` enabled, `copyStream` gets their `FileChannels` and transfers bytes from the input file to the output file and counts the number of bytes, possibly zero, that were actually transferred.

NOTE: `copyStream` uses Java's https://docs.oracle.com/javase/8/docs/api/java/nio/channels/FileChannel.html[java.nio.channels.FileChannel] to manage file channels.

If either `in` and `out` input streams are not `FileInputStream` or `transferToEnabled` flag is disabled (default), `copyStream` reads data from `in` to write to `out` and counts the number of bytes written.

`copyStream` can optionally close `in` and `out` streams (depending on the input `closeStreams` -- disabled by default).

NOTE: `Utils.copyStream` is used when <<writePartitionedFile, `BypassMergeSortShuffleWriter` writes records into one single shuffle block data file>> (among other places).

NOTE: `Utils.copyStream` is here temporarily (until I find a better place).

TIP: Visit the official web site of https://jcp.org/jsr/detail/51.jsp[JSR 51: New I/O APIs for the Java Platform] and read up on link:http://docs.oracle.com/javase/8/docs/api/java/nio/package-summary.html[java.nio package].
