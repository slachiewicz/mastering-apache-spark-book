== [[UnsafeShuffleWriter]] UnsafeShuffleWriter -- ShuffleWriter for SerializedShuffleHandle

`UnsafeShuffleWriter` is a link:spark-ShuffleWriter.adoc[ShuffleWriter] that is used to <<write, write records>> (i.e. key-value pairs).

`UnsafeShuffleWriter` is chosen when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` is requested for a `ShuffleWriter`] for a link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle].

`UnsafeShuffleWriter` can use a specialized NIO-based merge procedure that avoids extra serialization/deserialization.

.UnsafeShuffleWriter's Internal Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Name
| Initial Value
| Description

| `sorter`
| (uninitialized)
| link:spark-ShuffleExternalSorter.adoc[ShuffleExternalSorter]

Initialized when `UnsafeShuffleWriter` <<open, opens>> (which is when <<creating-instance, `UnsafeShuffleWriter` is created>>) and destroyed when it <<closeAndWriteOutput, closes internal resources and writes spill files merged>>.

Used when `UnsafeShuffleWriter` <<insertRecordIntoSorter, inserts a record into `ShuffleExternalSorter`>>, <<write, writes records>>, <<forceSorterToSpill, forceSorterToSpill>>, <<updatePeakMemoryUsed, updatePeakMemoryUsed>>, <<closeAndWriteOutput, closes internal resources and writes spill files merged>>, <<stop, stops>>.
|===

[TIP]
====
Enable `ERROR` or `DEBUG` logging levels for `org.apache.spark.shuffle.sort.UnsafeShuffleWriter` logger to see what happens in `UnsafeShuffleWriter`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.shuffle.sort.UnsafeShuffleWriter=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[mergeSpillsWithTransferTo]] `mergeSpillsWithTransferTo` Method

CAUTION: FIXME

=== [[forceSorterToSpill]] `forceSorterToSpill` Method

CAUTION: FIXME

=== [[mergeSpills]] `mergeSpills` Method

CAUTION: FIXME

=== [[updatePeakMemoryUsed]] `updatePeakMemoryUsed` Method

CAUTION: FIXME

=== [[write]] Writing Records -- `write` Method

[source, java]
----
void write(Iterator<Product2<K, V>> records) throws IOException
----

NOTE: `write` is part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract].

Internally, `write` traverses the input sequence of records (for a RDD partition) and <<insertRecordIntoSorter, insertRecordIntoSorter>> one by one. When all the records have been processed, `write` <<closeAndWriteOutput, closes internal resources and writes spill files merged>>.

In the end, `write` link:spark-ShuffleExternalSorter.adoc#cleanupResources[requests `ShuffleExternalSorter` to clean after itself].

CAUTION: FIXME

=== [[stop]] Stopping UnsafeShuffleWriter -- `stop` Method

[source, java]
----
Option<MapStatus> stop(boolean success)
----

CAUTION: FIXME

NOTE: `stop` is part of link:spark-ShuffleWriter.adoc#contract[`ShuffleWriter` contract].

=== [[creating-instance]] Creating UnsafeShuffleWriter Instance

`UnsafeShuffleWriter` takes the following when created:

1. link:spark-blockmanager.adoc[BlockManager]
2. `IndexShuffleBlockResolver`
3. link:spark-taskscheduler-taskmemorymanager.adoc[TaskMemoryManager]
4. link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle]
5. `mapId`
6. link:spark-taskscheduler-taskcontext.adoc[TaskContext]
7. link:spark-SparkConf.adoc[SparkConf]

`UnsafeShuffleWriter` makes sure that the number of shuffle output partitions (of the `ShuffleDependency` of the input `SerializedShuffleHandle`) is at most `(1 << 24) - 1`, i.e. `16777215`.

NOTE: The number of shuffle output partitions is first enforced when link:spark-SortShuffleManager.adoc#canUseSerializedShuffle[`SortShuffleManager` checks if `SerializedShuffleHandle` can be used for `ShuffleHandle`] (that eventually leads to `UnsafeShuffleWriter`).

`UnsafeShuffleWriter` uses <<spark_file_transferTo, spark.file.transferTo>> and <<spark_shuffle_sort_initialBufferSize, spark.shuffle.sort.initialBufferSize>> Spark properties to initialize `transferToEnabled` and `initialSortBufferSize` attributes, respectively.

If the number of shuffle output partitions is greater than the maximum, `UnsafeShuffleWriter` throws a `IllegalArgumentException`.

```
UnsafeShuffleWriter can only be used for shuffles with at most 16777215 reduce partitions
```

NOTE: `UnsafeShuffleWriter` is created exclusively when link:spark-SortShuffleManager.adoc#getWriter[`SortShuffleManager` selects a `ShuffleWriter`] (for a link:spark-SerializedShuffleHandle.adoc[SerializedShuffleHandle]).

=== [[open]] Opening UnsafeShuffleWriter (i.e. Creating ShuffleExternalSorter and SerializationStream) -- `open` Internal Method

[source, java]
----
void open() throws IOException
----

`open` makes sure that the internal reference to link:spark-ShuffleExternalSorter.adoc[ShuffleExternalSorter] (as `sorter`) is not defined and link:spark-ShuffleExternalSorter.adoc#creating-instance[creates one itself].

`open` creates a new byte array output stream (as `serBuffer`) with the buffer capacity of `1M`.

`open` creates a new link:spark-SerializationStream.adoc[SerializationStream] for the new byte array output stream using link:spark-SerializerInstance.adoc[SerializerInstance].

NOTE: `SerializerInstance` was defined when <<creating-instance, `UnsafeShuffleWriter` was created>> (and is exactly the one used to link:spark-rdd-ShuffleDependency.adoc#creating-instance[create the `ShuffleDependency`]).

NOTE: `open` is used exclusively when <<creating-instance, `UnsafeShuffleWriter` is created>>.

=== [[insertRecordIntoSorter]] Inserting Record Into ShuffleExternalSorter -- `insertRecordIntoSorter` Method

[source, java]
----
void insertRecordIntoSorter(Product2<K, V> record)
throws IOException
----

`insertRecordIntoSorter` link:spark-rdd-Partitioner.adoc#getPartition[calculates the partition for the key of the input `record`].

NOTE: `Partitioner` is defined when <<creating-instance, `UnsafeShuffleWriter` is created>>.

`insertRecordIntoSorter` then writes the key and the value of the input `record` to link:spark-SerializationStream.adoc[SerializationStream] and calculates the size of the serialized buffer.

NOTE: `SerializationStream` is created when <<open, `UnsafeShuffleWriter` opens>>.

In the end, `insertRecordIntoSorter` link:spark-ShuffleExternalSorter.adoc#insertRecord[inserts the serialized buffer to `ShuffleExternalSorter`] (as `Platform.BYTE_ARRAY_OFFSET` ).

NOTE: `ShuffleExternalSorter` is created when <<open, `UnsafeShuffleWriter` opens>>.

NOTE: `insertRecordIntoSorter` is used exclusively when <<write, `UnsafeShuffleWriter` writes records>>.

=== [[closeAndWriteOutput]] Closing Internal Resources and Writing Spill Files Merged -- `closeAndWriteOutput` Method

[source, java]
----
void closeAndWriteOutput() throws IOException
----

`closeAndWriteOutput` first <<updatePeakMemoryUsed, updates peak memory used>>.

`closeAndWriteOutput` removes the internal `ByteArrayOutputStream` and link:spark-SerializationStream.adoc[SerializationStream].

`closeAndWriteOutput` requests link:spark-ShuffleExternalSorter.adoc#closeAndGetSpills[`ShuffleExternalSorter` to close itself and return `SpillInfo` metadata].

`closeAndWriteOutput` removes the internal `ShuffleExternalSorter`.

`closeAndWriteOutput` requests `IndexShuffleBlockResolver` for the data file for the `shuffleId` and `mapId`.

`closeAndWriteOutput` creates a temporary file to <<mergeSpills, merge spill files>>, deletes them afterwards, and requests `IndexShuffleBlockResolver` to write index file and commit.

`closeAndWriteOutput` creates a link:spark-MapStatus.adoc[MapStatus] with the link:spark-blockmanager.adoc#shuffleServerId[location of the executor's `BlockManager`] and partition lengths in the merged file.

If there is an issue with deleting spill files, you should see the following ERROR message in the logs:

```
ERROR Error while deleting spill file [path]
```

If there is an issue with deleting the temporary file, you should see the following ERROR message in the logs:

```
ERROR Error while deleting temp file [path]
```

NOTE: `closeAndWriteOutput` is used exclusively when <<write, `UnsafeShuffleWriter` writes records>>.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property
| Default Value
| Description

| [[spark_file_transferTo]] `spark.file.transferTo`
| `true`
| Controls whether...FIXME

| [[spark_shuffle_sort_initialBufferSize]] `spark.shuffle.sort.initialBufferSize`
| `4096` (bytes)
| Default initial sort buffer size

|===
