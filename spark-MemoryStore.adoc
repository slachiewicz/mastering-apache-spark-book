== [[MemoryStore]] MemoryStore

*Memory store* (`MemoryStore`) manages blocks.

`MemoryStore` requires link:spark-SparkConf.adoc[SparkConf], link:spark-BlockInfoManager.adoc[BlockInfoManager], link:spark-SerializerManager.adoc[SerializerManager], link:spark-MemoryManager.adoc[MemoryManager] and a link:spark-BlockEvictionHandler.adoc[BlockEvictionHandler].

.`MemoryStore` Internal Registries
[cols="1,2",options="header",width="100%"]
|===
| Name | Description
| [[entries]] `entries` | Collection of ...FIXME

`entries` is Java's `LinkedHashMap` with the initial capacity of `32`, the load factor of `0.75` and _access-order_ ordering mode (i.e. iteration is in the order in which its entries were last accessed, from least-recently accessed to most-recently).

NOTE: `entries` is Java's https://docs.oracle.com/javase/8/docs/api/java/util/LinkedHashMap.html[java.util.LinkedHashMap].
|===

CAUTION: FIXME Where are these dependencies used?

CAUTION: FIXME Where is the `MemoryStore` created? What params provided?

NOTE: `MemoryStore` is a `private[spark]` class.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.storage.memory.MemoryStore` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.memory.MemoryStore=DEBUG
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[releaseUnrollMemoryForThisTask]] `releaseUnrollMemoryForThisTask` Method

CAUTION: FIXME

=== [[getValues]] `getValues` Method

[source, scala]
----
getValues(blockId: BlockId): Option[Iterator[_]]
----

`getValues` does...FIXME

=== [[getBytes]] `getBytes` Method

[source, scala]
----
getBytes(blockId: BlockId): Option[ChunkedByteBuffer]
----

`getBytes` does...FIXME

=== [[contains]] Is Block Available? -- `contains` Method

[source, scala]
----
contains(blockId: BlockId): Boolean
----

`contains` returns `true` when the internal <<entries, entries>> registry contains `blockId`.

=== [[putIteratorAsBytes]] `putIteratorAsBytes` Method

[source, scala]
----
putIteratorAsBytes[T](
  blockId: BlockId,
  values: Iterator[T],
  classTag: ClassTag[T],
  memoryMode: MemoryMode): Either[PartiallySerializedBlock[T], Long]
----

`putIteratorAsBytes` tries to put the `blockId` block in memory store as bytes.

CAUTION: FIXME

=== [[putIteratorAsValues]] `putIteratorAsValues` Method

[source, scala]
----
putIteratorAsValues[T](
  blockId: BlockId,
  values: Iterator[T],
  classTag: ClassTag[T]): Either[PartiallyUnrolledIterator[T], Long]
----

`putIteratorAsValues` tries to put the `blockId` block in memory store as `values`.

NOTE: `putIteratorAsValues` is a `private[storage]` method.

NOTE: is called when `BlockManager` stores  link:spark-BlockManager.adoc#doPutBytes[bytes of a block] or link:spark-BlockManager.adoc#doPutIterator[iterator of values of a block] or when link:spark-BlockManager.adoc#maybeCacheDiskValuesInMemory[attempting to cache spilled values read from disk].

=== [[remove]] Removing Block

CAUTION: FIXME

=== [[putBytes]] Acquiring Storage Memory for Blocks -- `putBytes` Method

[source, scala]
----
putBytes[T](
  blockId: BlockId,
  size: Long,
  memoryMode: MemoryMode,
  _bytes: () => ChunkedByteBuffer): Boolean
----

`putBytes` requests link:spark-MemoryManager.adoc#acquireStorageMemory[storage memory  for `blockId` from `MemoryManager`] and registers the block in <<entries, entries>> internal registry.

Internally, `putBytes` first makes sure that `blockId` block has not been registered already in <<entries, entries>> internal registry.

`putBytes` then requests link:spark-MemoryManager.adoc#acquireStorageMemory[`size` memory for the `blockId` block in a given `memoryMode` from the current `MemoryManager`].

[NOTE]
====
`memoryMode` can be `ON_HEAP` or `OFF_HEAP` and is a property of a link:spark-rdd-StorageLevel.adoc[StorageLevel].

```
import org.apache.spark.storage.StorageLevel._
scala> MEMORY_AND_DISK.useOffHeap
res0: Boolean = false

scala> OFF_HEAP.useOffHeap
res1: Boolean = true
```
====

If successful, `putBytes` "materializes" `_bytes` byte buffer and makes sure that the size is exactly `size`. It then registers a `SerializedMemoryEntry` (for the bytes and `memoryMode`) for `blockId` in the internal <<entries, entries>> registry.

You should see the following INFO message in the logs:

```
INFO Block [blockId] stored as bytes in memory (estimated size [size], free [bytes])
```

`putBytes` returns `true` only after `blockId` was successfully registered in the internal <<entries, entries>> registry.

=== [[settings]] Settings

.Spark Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Spark Property | Default Value | Description
| [[spark_storage_unrollMemoryThreshold]] `spark.storage.unrollMemoryThreshold` | `1k` |
|===

=== [[evictBlocksToFreeSpace]] Evicting Blocks From Memory -- `evictBlocksToFreeSpace` Method

[source, scala]
----
evictBlocksToFreeSpace(
  blockId: Option[BlockId],
  space: Long,
  memoryMode: MemoryMode): Long
----

`evictBlocksToFreeSpace`...FIXME

NOTE: `evictBlocksToFreeSpace` is used when `StorageMemoryPool` is requested to link:spark-StorageMemoryPool.adoc#acquireMemory[acquireMemory] and link:spark-StorageMemoryPool.adoc#freeSpaceToShrinkPool[freeSpaceToShrinkPool].
