== [[JobProgressListener]] `JobProgressListener` Spark Listener

`JobProgressListener` is a link:spark-SparkListener.adoc[SparkListener] for link:spark-webui.adoc[web UI].

`JobProgressListener` intercepts the following link:spark-SparkListener.adoc#SparkListenerEvent[Spark events].

.`JobProgressListener` Events
[cols="1,2",options="header",width="100%"]
|===
| Handler | Purpose
| <<onJobStart, onJobStart>> | Creates a <<JobUIData, JobUIData>>. It updates <<jobGroupToJobIds, jobGroupToJobIds>>, <<pendingStages, pendingStages>>, <<jobIdToData, jobIdToData>>, <<activeJobs, activeJobs>>, <<stageIdToActiveJobIds, stageIdToActiveJobIds>>, <<stageIdToInfo, stageIdToInfo>> and <<stageIdToData, stageIdToData>>.

| <<onJobEnd, onJobEnd>> | Removes an entry in <<activeJobs, activeJobs>>. It also removes entries in <<pendingStages, pendingStages>> and <<stageIdToActiveJobIds, stageIdToActiveJobIds>>. It updates <<completedJobs, completedJobs>>, <<numCompletedJobs, numCompletedJobs>>, <<failedJobs, failedJobs>>, <<numFailedJobs, numFailedJobs>> and <<skippedStages, skippedStages>>.

| <<onStageCompleted, onStageCompleted>> | Updates the `StageUIData` and `JobUIData`.
| <<onTaskStart, onTaskStart>> | Updates the task's `StageUIData` and `JobUIData`, and registers a new `TaskUIData`.
| <<onTaskEnd, onTaskEnd>> | Updates the task's `StageUIData` (and `TaskUIData`), `ExecutorSummary`, and `JobUIData`.

| <<onExecutorMetricsUpdate, onExecutorMetricsUpdate>> |

| `onEnvironmentUpdate` | Sets `schedulingMode` property using the current link:spark-TaskSchedulerImpl.adoc#spark_scheduler_mode[spark.scheduler.mode] (from `Spark Properties` environment details).

Used in link:spark-webui-jobs.adoc#AllJobsPage[Jobs tab] (for the Scheduling Mode), and to display pools in `JobsTab` and `StagesTab`.

*FIXME*: Add the links/screenshots for pools.
| `onBlockManagerAdded` | Records an executor and its block manager in the internal <<executorIdToBlockManagerId, executorIdToBlockManagerId>> registry.
| `onBlockManagerRemoved` | Removes the executor from the internal <<executorIdToBlockManagerId, executorIdToBlockManagerId>> registry.
| `onApplicationStart` | Records a Spark application's start time (in the internal `startTime`).

Used in link:spark-webui-jobs.adoc[Jobs tab] (for a total uptime and the event timeline) and link:spark-webui-jobs.adoc[Job page] (for the event timeline).
| `onApplicationEnd` | Records a Spark application's end time (in the internal `endTime`).

Used in link:spark-webui-jobs.adoc[Jobs tab] (for a total uptime).
| `onTaskGettingResult` | Does nothing.

*FIXME*: Why is this event intercepted at all?!
|===

=== [[updateAggregateMetrics]] `updateAggregateMetrics` Method

CAUTION: FIXME

=== [[registries]] Registries and Counters

`JobProgressListener` uses registries to collect information about job executions.

.`JobProgressListener` Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name | Description
| [[numCompletedStages]] `numCompletedStages` |
| [[numFailedStages]] `numFailedStages` |

| [[stageIdToData]] `stageIdToData` | Holds <<StageUIData, StageUIData>> per stage, i.e. the stage and stage attempt ids.
| [[stageIdToInfo]] `stageIdToInfo` |
| [[stageIdToActiveJobIds]] `stageIdToActiveJobIds` |
| [[poolToActiveStages]] `poolToActiveStages` |

| [[activeJobs]] `activeJobs` |
| [[completedJobs]] `completedJobs` |
| [[failedJobs]] `failedJobs` |
| [[jobIdToData]] `jobIdToData` |
| [[jobGroupToJobIds]] `jobGroupToJobIds` |

| [[pendingStages]] `pendingStages` |
| [[activeStages]] `activeStages` |
| [[completedStages]] `completedStages` |
| [[skippedStages]] `skippedStages` |
| [[failedStages]] `failedStages` |

| [[executorIdToBlockManagerId]] `executorIdToBlockManagerId` | The lookup table for `BlockManagerId` per executor id.

Used to track block managers so the Stage page can display `Address` in  link:spark-webui-StagePage.adoc#ExecutorTable[Aggregated Metrics by Executor].

FIXME: How does Executors page collect the very same information?
|===

=== [[onJobStart]] `onJobStart` Callback

[source, scala]
----
onJobStart(jobStart: SparkListenerJobStart): Unit
----

`onJobStart` creates a <<JobUIData, JobUIData>>. It updates <<jobGroupToJobIds, jobGroupToJobIds>>, <<pendingStages, pendingStages>>, <<jobIdToData, jobIdToData>>, <<activeJobs, activeJobs>>, <<stageIdToActiveJobIds, stageIdToActiveJobIds>>, <<stageIdToInfo, stageIdToInfo>> and <<stageIdToData, stageIdToData>>.

`onJobStart` reads the optional Spark Job group id as `spark.jobGroup.id` (from `properties` in the input `jobStart`).

`onJobStart` then creates a `JobUIData` using the input `jobStart` with `status` attribute set to `JobExecutionStatus.RUNNING` and records it in <<jobIdToData, jobIdToData>> and <<activeJobs, activeJobs>> registries.

`onJobStart` looks the job ids for the group id (in <<jobGroupToJobIds, jobGroupToJobIds>> registry) and adds the job id.

The internal <<pendingStages, pendingStages>> is updated with link:spark-dagscheduler-StageInfo.adoc[StageInfo] for the stage id (for every `StageInfo` in `SparkListenerJobStart.stageInfos` collection).

`onJobStart` records the stages of the job in <<stageIdToActiveJobIds, stageIdToActiveJobIds>>.

`onJobStart` records link:spark-dagscheduler-StageInfo.adoc[StageInfos] in <<stageIdToInfo, stageIdToInfo>> and <<stageIdToData, stageIdToData>>.

=== [[onJobEnd]] `onJobEnd` Method

[source, scala]
----
onJobEnd(jobEnd: SparkListenerJobEnd): Unit
----

`onJobEnd` removes an entry in <<activeJobs, activeJobs>>. It also removes entries in <<pendingStages, pendingStages>> and <<stageIdToActiveJobIds, stageIdToActiveJobIds>>. It updates <<completedJobs, completedJobs>>, <<numCompletedJobs, numCompletedJobs>>, <<failedJobs, failedJobs>>, <<numFailedJobs, numFailedJobs>> and <<skippedStages, skippedStages>>.

`onJobEnd` removes the job from <<activeJobs, activeJobs>> registry. It removes stages from <<pendingStages, pendingStages>> registry.

When completed successfully, the job is added to <<completedJobs, completedJobs>> registry with `status` attribute set to `JobExecutionStatus.SUCCEEDED`. <<numCompletedJobs, numCompletedJobs>> gets incremented.

When failed, the job is added to <<failedJobs, failedJobs>> registry with `status` attribute set to `JobExecutionStatus.FAILED`. <<numFailedJobs, numFailedJobs>> gets incremented.

For every stage in the job, the stage is removed from the active jobs (in <<stageIdToActiveJobIds, stageIdToActiveJobIds>>) that can remove the entire entry if no active jobs exist.

Every pending stage in <<stageIdToInfo, stageIdToInfo>> gets added to <<skippedStages, skippedStages>>.

=== [[onExecutorMetricsUpdate]] `onExecutorMetricsUpdate` Method

[source, scala]
----
onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit
----

=== [[onTaskStart]] `onTaskStart` Method

[source, scala]
----
onTaskStart(taskStart: SparkListenerTaskStart): Unit
----

`onTaskStart` updates `StageUIData` and `JobUIData`, and registers a new `TaskUIData`.

`onTaskStart` takes link:spark-TaskInfo.adoc[TaskInfo] from the input `taskStart`.

`onTaskStart` looks the `StageUIData` for the stage and stage attempt ids up (in <<stageIdToData, stageIdToData>> registry).

`onTaskStart` increments `numActiveTasks` and puts a `TaskUIData` for the task in `stageData.taskData`.

Ultimately, `onTaskStart` looks the stage in the internal <<stageIdToActiveJobIds, stageIdToActiveJobIds>> and for each active job reads its `JobUIData` (from <<jobIdToData, jobIdToData>>). It then increments `numActiveTasks`.

=== [[onTaskEnd]] `onTaskEnd` Method

[source, scala]
----
onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit
----

`onTaskEnd` updates the `StageUIData` (and `TaskUIData`), `ExecutorSummary`, and `JobUIData`.

`onTaskEnd` takes link:spark-TaskInfo.adoc[TaskInfo] from the input `taskEnd`.

NOTE: `onTaskEnd` does its processing when the `TaskInfo` is available and `stageAttemptId` is not `-1`.

`onTaskEnd` looks the `StageUIData` for the stage and stage attempt ids up (in <<stageIdToData, stageIdToData>> registry).

`onTaskEnd` saves `accumulables` in the `StageUIData`.

`onTaskEnd` reads the `ExecutorSummary` for the executor (the task has finished on).

Depending on the task end's reason `onTaskEnd` increments `succeededTasks`, `killedTasks` or `failedTasks` counters.

`onTaskEnd` adds the task's duration to `taskTime`.

`onTaskEnd` decrements the number of active tasks (in the `StageUIData`).

_Again_, depending on the task end's reason `onTaskEnd` computes `errorMessage` and updates `StageUIData`.

CAUTION: FIXME Why is the same information in two different registries -- `stageData` and `execSummary`?!

If `taskMetrics` is available, <<updateAggregateMetrics, updateAggregateMetrics>> is executed.

The task's `TaskUIData` is looked up in `stageData.taskData` and `updateTaskInfo` and `updateTaskMetrics` are executed. `errorMessage` is updated.

`onTaskEnd` makes sure that the number of tasks in `StageUIData` (`stageData.taskData`) is not above <<spark_ui_retainedTasks, spark.ui.retainedTasks>> and drops the excess.

Ultimately, `onTaskEnd` looks the stage in the internal <<stageIdToActiveJobIds, stageIdToActiveJobIds>> and for each active job reads its `JobUIData` (from <<jobIdToData, jobIdToData>>). It then decrements `numActiveTasks` and increments `numCompletedTasks`, `numKilledTasks` or `numFailedTasks` depending on the task's end reason.

=== [[onStageSubmitted]] `onStageSubmitted` Method

[source, scala]
----
onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit
----

=== [[onStageCompleted]] `onStageCompleted` Method

[source, scala]
----
onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit
----

`onStageCompleted` updates the `StageUIData` and `JobUIData`.

`onStageCompleted` reads `stageInfo` from the input `stageCompleted` and records it in <<stageIdToInfo, stageIdToInfo>> registry.

`onStageCompleted` looks the `StageUIData` for the stage and the stage attempt ids up in <<stageIdToData, stageIdToData>> registry.

`onStageCompleted` records `accumulables` in `StageUIData`.

`onStageCompleted` removes the stage from <<poolToActiveStages, poolToActiveStages>> and <<activeStages, activeStages>> registries.

If the stage completed successfully (i.e. has no `failureReason`), `onStageCompleted` adds the stage to <<completedStages, completedStages>> registry and increments <<numCompletedStages, numCompletedStages>> counter. It trims <<completedStages, completedStages>>.

Otherwise, when the stage failed, `onStageCompleted` adds the stage to <<failedStages, failedStages>> registry and increments <<numFailedStages, numFailedStages>> counter. It trims <<failedStages, failedStages>>.

Ultimately, `onStageCompleted` looks the stage in the internal <<stageIdToActiveJobIds, stageIdToActiveJobIds>> and for each active job reads its `JobUIData` (from <<jobIdToData, jobIdToData>>). It then decrements `numActiveStages`. When completed successfully, it adds the stage to `completedStageIndices`. With failure, `numFailedStages` gets incremented.

=== [[JobUIData]] JobUIData

CAUTION: FIXME

=== [[blockManagerIds]] blockManagerIds method

[source, scala]
----
blockManagerIds: Seq[BlockManagerId]
----

CAUTION: FIXME

=== [[StageUIData]] StageUIData

CAUTION: FIXME

=== [[settings]] Settings

.Spark Properties
[options="header",width="100%"]
|===
| Setting | Default Value | Description
| [[spark_ui_retainedJobs]] `spark.ui.retainedJobs` | `1000` | The number of jobs to hold information about
| [[spark_ui_retainedStages]] `spark.ui.retainedStages` | `1000` | The number of stages to hold information about
| [[spark_ui_retainedTasks]] `spark.ui.retainedTasks` | `100000` | The number of tasks to hold information about
|===
