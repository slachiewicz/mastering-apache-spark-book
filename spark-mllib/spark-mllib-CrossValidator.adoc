== [[CrossValidator]] CrossValidator

`CrossValidator` is an link:spark-mllib-estimators.adoc[Estimator] that <<fit, fits a CrossValidatorModel to a dataset>>.

`CrossValidator` takes <<parameters, parameters>>.

[[parameters]]
.CrossValidator' Parameters
[cols="1,1,2",options="header",width="100%"]
|===
| Parameter
| Default Value
| Description

| [[estimator]] `estimator`
| (undefined)
| link:spark-mllib-Estimator.adoc[Estimator] for best model selection

| [[estimatorParamMaps]] `estimatorParamMaps`
| (undefined)
| Param maps for the <<estimator, estimator>>

| [[evaluator]] `evaluator`
| (undefined)
| link:spark-mllib-Evaluator.adoc[Evaluator] to select hyper-parameters that maximize the validated metric

| [[numFolds]] `numFolds`
| `3`
| The number of folds for cross validation

Must be at least `2`.

| [[parallelism]] `parallelism`
| `1`
| The number of threads to use while <<fit, fitting a model>>

Must be at least `1`.

| [[seed]] `seed`
|
| Random seed
|===

TIP: What makes `CrossValidator` a very useful tool for _model selection_ is its ability to work with any link:spark-mllib-estimators.adoc[Estimator] instance, link:spark-mllib-Pipeline.adoc[Pipelines] including, that can transform datasets from their raw format to another before use.

```
import org.apache.spark.ml.tuning.CrossValidator
val cv = new CrossValidator
```

[TIP]
====
Enable `DEBUG` logging level for `org.apache.spark.ml.tuning.CrossValidator` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.ml.tuning.CrossValidator=DEBUG
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[fit]] Fitting CrossValidatorModel -- `fit` Method

[source, scala]
----
fit(dataset: Dataset[_]): CrossValidatorModel
----

NOTE: `fit` is a part of link:spark-mllib-Estimator.adoc#fit[Estimator Contract] to fit a model (i.e. produce a model).

`fit` link:spark-mllib-PipelineStage.adoc#transformSchema[validates the schema] (with logging turned on).

With `DEBUG` logging level turned on, you should see the following DEBUG message in the logs:

```
DEBUG CrossValidator: Input schema: [json]
```

`fit` makes sure that <<estimator, estimator>>, <<evaluator, evaluator>> and <<estimatorParamMaps, estimatorParamMaps>> parameters are defined.

`fit` makes sure that <<parallelism, parallelism>> parameter is defined and link:spark-mllib-HasParallelism.adoc#getExecutionContext[creates] a `ExecutionContext`.

`fit` link:spark-mllib-Instrumentation.adoc#create[creates] a `Instrumentation` and requests it to link:spark-mllib-Instrumentation.adoc#logParams[print] out the parameters <<numFolds, numFolds>>, <<seed, seed>>, <<parallelism, parallelism>> to the logs.

```
INFO ...FIXME
```

CAUTION: FIXME Finish me...from `splits`

`fit` requests `Instrumentation` to link:spark-mllib-ValidatorParams.adoc#logTuningParams[print out the tuning parameters to the logs].

```
INFO ...FIXME
```


`fit` reports a `NoSuchElementException` when the required parameters are not defined.

```
java.util.NoSuchElementException: Failed to find a default value for [name]
```

=== [[transformSchema]] Validating and Transforming Schema -- `transformSchema` Method

[source, scala]
----
transformSchema(schema: StructType): StructType
----

NOTE: `transformSchema` is a part of link:spark-mllib-PipelineStage.adoc#transformSchema[PipelineStage Contract].

`transformSchema` simply passes the call to link:spark-mllib-ValidatorParams.adoc#transformSchemaImpl[transformSchemaImpl] (that is shared between `CrossValidator` and link:spark-mllib-TrainValidationSplit.adoc[TrainValidationSplit]).
