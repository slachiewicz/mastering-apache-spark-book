== [[BlockManagerMaster]] BlockManagerMaster -- BlockManager for Driver

`BlockManagerMaster` link:spark-SparkEnv.adoc#BlockManagerMaster[runs on the driver].

`BlockManagerMaster` uses link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMasterEndpoint] registered under *BlockManagerMaster* RPC endpoint name on the driver (with the endpoint references on executors) to allow executors for sending block status updates to it and hence keep track of block statuses.

NOTE: `BlockManagerMaster` is created in link:spark-SparkEnv.adoc#BlockManagerMaster[`SparkEnv` (for the driver and executors)], and immediately used to create their link:spark-blockmanager.adoc[BlockManagers].

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.storage.BlockManagerMaster` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.storage.BlockManagerMaster=INFO
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[removeExecutorAsync]] `removeExecutorAsync` Method

CAUTION: FIXME

=== [[contains]] `contains` Method

CAUTION: FIXME

=== [[creating-instance]] Creating BlockManagerMaster Instance

`BlockManagerMaster` takes the following when created:

* [[driverEndpoint]] link:spark-RpcEndpointRef.adoc[RpcEndpointRef] to...FIXME
* [[conf]] link:spark-SparkConf.adoc[SparkConf]
* [[isDriver]] Flag whether `BlockManagerMaster` is created for the driver or executors.

`BlockManagerMaster` initializes the <<internal-registries, internal registries and counters>>.

=== [[removeExecutor]] Removing Executor -- `removeExecutor` Method

[source, scala]
----
removeExecutor(execId: String): Unit
----

`removeExecutor` posts link:spark-blockmanager-BlockManagerMasterEndpoint.adoc#RemoveExecutor[`RemoveExecutor` to `BlockManagerMaster` RPC endpoint] and waits for a response.

If `false` in response comes in, a `SparkException` is thrown with the following message:

```
BlockManagerMasterEndpoint returned false, expected true.
```

If all goes fine, you should see the following INFO message in the logs:

```
INFO BlockManagerMaster: Removed executor [execId]
```

NOTE: `removeExecutor` is executed when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` processes `ExecutorLost` event].

=== [[removeBlock]] Removing Block -- `removeBlock` Method

[source, scala]
----
removeBlock(blockId: BlockId): Unit
----

`removeBlock` simply posts a `RemoveBlock` blocking message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] (and ultimately disregards the reponse).

=== [[removeRdd]] Removing RDD Blocks -- `removeRdd` Method

[source, scala]
----
removeRdd(rddId: Int, blocking: Boolean)
----

`removeRdd` removes all the blocks of `rddId` RDD, possibly in `blocking` fashion.

Internally, `removeRdd` posts a `RemoveRdd(rddId)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] on a separate thread.

If there is an issue, you should see the following WARN message in the logs and the entire exception:

```
WARN Failed to remove RDD [rddId] - [exception]
```

If it is a `blocking` operation, it waits for a result for link:spark-rpc.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], link:spark-rpc.adoc#spark.network.timeout[spark.network.timeout] or `120` secs.

=== [[removeShuffle]] Removing Shuffle Blocks -- `removeShuffle` Method

[source, scala]
----
removeShuffle(shuffleId: Int, blocking: Boolean)
----

`removeShuffle` removes all the blocks of `shuffleId` shuffle, possibly in a `blocking` fashion.

It posts a `RemoveShuffle(shuffleId)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] on a separate thread.

If there is an issue, you should see the following WARN message in the logs and the entire exception:

```
WARN Failed to remove shuffle [shuffleId] - [exception]
```

If it is a `blocking` operation, it waits for the result for link:spark-rpc.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], link:spark-rpc.adoc#spark.network.timeout[spark.network.timeout] or `120` secs.

NOTE: `removeShuffle` is used exclusively when link:spark-service-contextcleaner.adoc#doCleanupShuffle[`ContextCleaner` removes a shuffle].

=== [[removeBroadcast]] Removing Broadcast Blocks -- `removeBroadcast` Method

[source, scala]
----
removeBroadcast(broadcastId: Long, removeFromMaster: Boolean, blocking: Boolean)
----

`removeBroadcast` removes all the blocks of `broadcastId` broadcast, possibly in a `blocking` fashion.

It posts a `RemoveBroadcast(broadcastId, removeFromMaster)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] on a separate thread.

If there is an issue, you should see the following WARN message in the logs and the entire exception:

```
WARN Failed to remove broadcast [broadcastId] with removeFromMaster = [removeFromMaster] - [exception]
```

If it is a `blocking` operation, it waits for the result for link:spark-rpc.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], link:spark-rpc.adoc#spark.network.timeout[spark.network.timeout] or `120` secs.

=== [[stop]] Stopping BlockManagerMaster -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` sends a `StopBlockManagerMaster` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response.

NOTE: It is only executed for the driver.

If all goes fine, you should see the following INFO message in the logs:

```
INFO BlockManagerMaster: BlockManagerMaster stopped
```

Otherwise, a `SparkException` is thrown.

```
BlockManagerMasterEndpoint returned false, expected true.
```

=== [[registerBlockManager]] Registering BlockManager with Driver -- `registerBlockManager` Method

[source, scala]
----
registerBlockManager(
  blockManagerId: BlockManagerId,
  maxMemSize: Long,
  slaveEndpoint: RpcEndpointRef): BlockManagerId
----

`registerBlockManager` prints the following INFO message to the logs:

```
INFO BlockManagerMaster: Registering BlockManager [blockManagerId]
```

.Registering BlockManager with the Driver
image::images/spark-BlockManagerMaster-RegisterBlockManager.png[align="center"]

`registerBlockManager` then notifies the driver that the `blockManagerId` link:spark-blockmanager.adoc[BlockManager] tries to register. `registerBlockManager` posts a link:spark-blockmanager-BlockManagerMasterEndpoint.adoc#RegisterBlockManager[blocking `RegisterBlockManager` message to BlockManagerMaster RPC endpoint].

NOTE: The input `maxMemSize` is the link:spark-blockmanager.adoc#maxMemory[total available on-heap and off-heap memory for storage on a `BlockManager`].

`registerBlockManager` waits until a confirmation comes (as link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId]) that becomes the return value.

You should see the following INFO message in the logs:

```
INFO BlockManagerMaster: Registered BlockManager [updatedId]
```

NOTE: `registerBlockManager` is used when `BlockManager` link:spark-blockmanager.adoc#initialize[is initialized] or link:spark-blockmanager.adoc#reregister[re-registers itself with the driver] (and reports the blocks).

=== [[updateBlockInfo]] Relaying Block Status Update From BlockManager to Driver (by Sending Blocking UpdateBlockInfo to BlockManagerMaster RPC endpoint) -- `updateBlockInfo` Method

[source, scala]
----
updateBlockInfo(
  blockManagerId: BlockManagerId,
  blockId: BlockId,
  storageLevel: StorageLevel,
  memSize: Long,
  diskSize: Long): Boolean
----

`updateBlockInfo` sends a link:spark-blockmanager-BlockManagerMasterEndpoint.adoc#UpdateBlockInfo[blocking `UpdateBlockInfo` message to BlockManagerMaster RPC endpoint] and waits for a response.

You should see the following DEBUG message in the logs:

```
DEBUG BlockManagerMaster: Updated info of block [blockId]
```

`updateBlockInfo` returns the response from the `BlockManagerMaster` RPC endpoint.

NOTE: `updateBlockInfo` is used when `BlockManager` link:spark-blockmanager.adoc#tryToReportBlockStatus[reports a block status update to the driver].

=== [[getLocations-block]] Get Block Locations of One Block -- `getLocations` Method

[source, scala]
----
getLocations(blockId: BlockId): Seq[BlockManagerId]
----

`getLocations` link:spark-blockmanager-BlockManagerMasterEndpoint.adoc#GetLocations[posts a blocking `GetLocations` message to BlockManagerMaster RPC endpoint] and returns the response.

NOTE: `getLocations` is used when <<contains, `BlockManagerMaster` checks if a block was registered>> and link:spark-blockmanager.adoc#getLocations[`BlockManager` getLocations].

=== [[getLocations-block-array]] Get Block Locations for Multiple Blocks -- `getLocations` Method

[source, scala]
----
getLocations(blockIds: Array[BlockId]): IndexedSeq[Seq[BlockManagerId]]
----

`getLocations` link:spark-blockmanager-BlockManagerMasterEndpoint.adoc#GetLocationsMultipleBlockIds[posts a blocking `GetLocationsMultipleBlockIds` message to BlockManagerMaster RPC endpoint] and returns the response.

NOTE: `getLocations` is used when link:spark-dagscheduler.adoc#getCacheLocs[`DAGScheduler` finds BlockManagers (and so executors) for cached RDD partitions] and when `BlockManager` link:spark-blockmanager.adoc#getLocationBlockIds[getLocationBlockIds] and link:spark-blockmanager.adoc#blockIdsToHosts[blockIdsToHosts].

=== [[getPeers]] Finding Peers of BlockManager -- `getPeers` Internal Method

[source, scala]
----
getPeers(blockManagerId: BlockManagerId): Seq[BlockManagerId]
----

`getPeers` link:spark-blockmanager-BlockManagerMasterEndpoint.adoc#GetPeers[posts a blocking `GetPeers` message to BlockManagerMaster RPC endpoint] and returns the response.

NOTE: *Peers* of a link:spark-blockmanager.adoc[BlockManager] are the other BlockManagers in a cluster (except the driver's BlockManager). Peers are used to know the available executors in a Spark application.

NOTE: `getPeers` is used when link:spark-blockmanager.adoc#getPeers[`BlockManager` finds the peers of a `BlockManager`], Structured Streaming's `KafkaSource` and Spark Streaming's `KafkaRDD`.

=== [[getExecutorEndpointRef]] `getExecutorEndpointRef` Method

[source, scala]
----
getExecutorEndpointRef(executorId: String): Option[RpcEndpointRef]
----

`getExecutorEndpointRef` posts `GetExecutorEndpointRef(executorId)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response which becomes the return value.

=== [[getMemoryStatus]] `getMemoryStatus` Method

[source, scala]
----
getMemoryStatus: Map[BlockManagerId, (Long, Long)]
----

`getMemoryStatus` posts a `GetMemoryStatus` message link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response which becomes the return value.

=== [[getStorageStatus]] Storage Status (Posting GetStorageStatus to BlockManagerMaster RPC endpoint) -- `getStorageStatus` Method

[source, scala]
----
getStorageStatus: Array[StorageStatus]
----

`getStorageStatus` posts a `GetStorageStatus` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response which becomes the return value.

=== [[getBlockStatus]] `getBlockStatus` Method

[source, scala]
----
getBlockStatus(
  blockId: BlockId,
  askSlaves: Boolean = true): Map[BlockManagerId, BlockStatus]
----

`getBlockStatus` posts a `GetBlockStatus(blockId, askSlaves)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response (of type `Map[BlockManagerId, Future[Option[BlockStatus]]]`).

It then builds a sequence of future results that are `BlockStatus` statuses and waits for a result for link:spark-rpc.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], link:spark-rpc.adoc#spark.network.timeout[spark.network.timeout] or `120` secs.

No result leads to a `SparkException` with the following message:

```
BlockManager returned null for BlockStatus query: [blockId]
```

=== [[getMatchingBlockIds]] `getMatchingBlockIds` Method

[source, scala]
----
getMatchingBlockIds(
  filter: BlockId => Boolean,
  askSlaves: Boolean): Seq[BlockId]
----

`getMatchingBlockIds` posts a `GetMatchingBlockIds(filter, askSlaves)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response which becomes the result for link:spark-rpc.adoc#spark.rpc.askTimeout[spark.rpc.askTimeout], link:spark-rpc.adoc#spark.network.timeout[spark.network.timeout] or `120` secs.

=== [[hasCachedBlocks]] `hasCachedBlocks` Method

[source, scala]
----
hasCachedBlocks(executorId: String): Boolean
----

`hasCachedBlocks` posts a `HasCachedBlocks(executorId)` message to link:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMaster RPC endpoint] and waits for a response which becomes the result.
