== [[ShuffleDependency]] ShuffleDependency -- Shuffle Dependency

`ShuffleDependency` is a link:spark-rdd-dependencies.adoc[RDD Dependency] on the output of a link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] for a <<rdd, key-value pair RDD>>.

`ShuffleDependency` uses the <<rdd, RDD>> to know the number of (map-side/pre-shuffle) partitions and the <<partitioner, `Partitioner`>> for the number of (reduce-size/post-shuffle) partitions.

`ShuffleDependency` is a dependency of link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] as well as link:spark-rdd-cogroupedrdd.adoc[CoGroupedRDD] and link:spark-rdd-SubtractedRDD.adoc[SubtractedRDD] but only when link:spark-rdd-Partitioner.adoc[partitioners] (of the RDD's and after transformations) are different.

A `ShuffleDependency` is <<creating-instance, created>> for a key-value pair RDD, i.e. `RDD[Product2[K, V]]` with `K` and `V` being the types of keys and values, respectively.

TIP: Use link:spark-rdd.adoc#dependencies[`dependencies` method] on an RDD to know the dependencies.

```
scala> val rdd = sc.parallelize(0 to 8).groupBy(_ % 3)
rdd: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[2] at groupBy at <console>:24

scala> rdd.dependencies
res0: Seq[org.apache.spark.Dependency[_]] = List(org.apache.spark.ShuffleDependency@454f6cc5)
```

[[shuffleId]]
Every `ShuffleDependency` has a unique application-wide *shuffleId* number that is assigned when <<creating-instance, `ShuffleDependency` is created>> (and is used throughout Spark's code to reference a `ShuffleDependency`).

NOTE: Shuffle ids are link:spark-SparkContext.adoc#nextShuffleId[tracked by `SparkContext`].

=== [[keyOrdering]] `keyOrdering` Property

CAUTION: FIXME

=== [[serializer]] `serializer` Property

CAUTION: FIXME

=== [[creating-instance]] Creating ShuffleDependency Instance

`ShuffleDependency` takes the following when created:

1. A single key-value pair RDD, i.e. `RDD[Product2[K, V]]`,
2. link:spark-rdd-Partitioner.adoc[Partitioner] (available as <<partitioner, `partitioner` property>>),
3. link:spark-SparkEnv.adoc#serializer[Serializer],
4. Optional key ordering (of Scala's link:http://www.scala-lang.org/api/current/scala/math/Ordering.html[scala.math.Ordering] type),
5. Optional <<aggregator, Aggregator>>,
6. <<mapSideCombine, mapSideCombine>> flag which is disabled (i.e. `false`) by default.

NOTE: `ShuffleDependency` uses link:spark-SparkEnv.adoc#serializer[`SparkEnv` to access the current `Serializer`].

When created, `ShuffleDependency` gets link:spark-SparkContext.adoc#nextShuffleId[shuffle id] (as `shuffleId`).

NOTE: `ShuffleDependency` uses the link:spark-rdd.adoc#context[input RDD to access `SparkContext`] and so the `shuffleId`.

`ShuffleDependency` link:spark-ShuffleManager.adoc#registerShuffle[registers itself with `ShuffleManager`] and gets a `ShuffleHandle` (available as <<shuffleHandle, shuffleHandle>> property).

NOTE: `ShuffleDependency` accesses link:spark-SparkEnv.adoc#shuffleManager[`ShuffleManager` using `SparkEnv`].

In the end, `ShuffleDependency` link:spark-service-contextcleaner.adoc#registerShuffleForCleanup[registers itself for cleanup with `ContextCleaner`].

NOTE: `ShuffleDependency` accesses the link:spark-SparkContext.adoc#cleaner[optional `ContextCleaner` through `SparkContext`].

NOTE: `ShuffleDependency` is created when link:spark-rdd-ShuffledRDD.adoc#getDependencies[ShuffledRDD], link:spark-rdd-cogroupedrdd.adoc#getDependencies[CoGroupedRDD], and link:spark-rdd-SubtractedRDD.adoc#getDependencies[SubtractedRDD] return their RDD dependencies.

=== [[rdd]] `rdd` Property

[source, scala]
----
rdd: RDD[Product2[K, V]]
----

`rdd` returns a key-value pair RDD this <<creating-instance, `ShuffleDependency` was created for>>.

[NOTE]
====
`rdd` is used when:

1. link:spark-service-MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[`MapOutputTrackerMaster` finds preferred `BlockManagers` with most map outputs] for a `ShuffleDependency`,

2. link:spark-dagscheduler.adoc#getOrCreateShuffleMapStage[`DAGScheduler` finds or creates new `ShuffleMapStage` stages] for a `ShuffleDependency`,

3. link:spark-dagscheduler.adoc#createShuffleMapStage[`DAGScheduler` creates a `ShuffleMapStage`] for a `ShuffleDependency` and a `ActiveJob`,

4. link:spark-dagscheduler.adoc#getMissingAncestorShuffleDependencies[`DAGScheduler` finds missing ShuffleDependencies for a RDD],

5. link:spark-dagscheduler.adoc#submitMapStage[`DAGScheduler` submits a `ShuffleDependency` for execution].
====

=== [[partitioner]] `partitioner` Property

`partitioner` property is a link:spark-rdd-Partitioner.adoc[Partitioner] that is used to partition the shuffle output.

`partitioner` is specified when <<creating-instance, `ShuffleDependency` is created>>.

[NOTE]
====
`partitioner` is used when:

1. link:spark-service-mapoutputtracker.adoc#getStatistics[`MapOutputTracker` computes the statistics for a `ShuffleDependency`] (and is the size of the array with the total sizes of shuffle blocks),

2. link:spark-service-MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[`MapOutputTrackerMaster` finds preferred `BlockManagers` with most map outputs] for a `ShuffleDependency`,

3. link:spark-sql-ShuffledRowRDD.adoc#numPreShufflePartitions[ShuffledRowRDD.adoc#numPreShufflePartitions],

4. link:spark-SortShuffleManager.adoc#canUseSerializedShuffle[`SortShuffleManager` checks if `SerializedShuffleHandle` can be used] (for a `ShuffleHandle`).

5. FIXME
====

=== [[shuffleHandle]] `shuffleHandle` Property

[source, scala]
----
shuffleHandle: ShuffleHandle
----

`shuffleHandle` is the `ShuffleHandle` of a `ShuffleDependency` as assigned eagerly when <<creating-instance, `ShuffleDependency` was created>>.

NOTE: `shuffleHandle` is used to compute link:spark-rdd-cogroupedrdd.adoc#compute[CoGroupedRDDs], link:spark-rdd-ShuffledRDD.adoc#compute[ShuffledRDD], link:spark-rdd-SubtractedRDD.adoc#compute[SubtractedRDD], and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] (to get a link:spark-ShuffleReader.adoc[ShuffleReader] for a `ShuffleDependency`) and when a link:spark-taskscheduler-ShuffleMapTask.adoc#runTask[`ShuffleMapTask` runs] (to get a `ShuffleWriter` for a `ShuffleDependency`).

=== [[mapSideCombine]] Map-Size Combine Flag -- `mapSideCombine` Attribute

`mapSideCombine` is a flag to control whether to use *partial aggregation* (aka *map-side combine*).

`mapSideCombine` is by default disabled (i.e. `false`) when <<creating-instance, creating a `ShuffleDependency`>>.

When enabled, link:spark-SortShuffleWriter.adoc[SortShuffleWriter] and link:spark-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader] assume that an link:spark-Aggregator.adoc[Aggregator] is also defined.

NOTE: `mapSideCombine` is exclusively set (and hence can be enabled) when link:spark-rdd-ShuffledRDD.adoc#getDependencies[`ShuffledRDD` returns the dependencies] (which is a single `ShuffleDependency`).

=== [[aggregator]] `aggregator` Property

[source, scala]
----
aggregator: Option[Aggregator[K, V, C]] = None
----

`aggregator` is a link:spark-Aggregator.adoc[map/reduce-side Aggregator] (for a RDD's shuffle).

`aggregator` is by default undefined (i.e. `None`) when <<creating-instance, `ShuffleDependency` is created>>.

NOTE: `aggregator` is used when link:spark-SortShuffleWriter.adoc#write[`SortShuffleWriter` writes records] and link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined key-values for a reduce task].

=== Usage

The places where `ShuffleDependency` is used:

* link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] and link:spark-sql-ShuffledRowRDD.adoc[ShuffledRowRDD] that are RDDs from a shuffle

The RDD operations that may or may not use the above RDDs and hence shuffling:

* link:spark-rdd-partitions.adoc#coalesce[coalesce]
** link:spark-rdd-partitions.adoc#repartition[repartition]

* `cogroup`
** `intersection`
* `subtractByKey`
** `subtract`
* `sortByKey`
** `sortBy`
* `repartitionAndSortWithinPartitions`
* link:spark-rdd-PairRDDFunctions.adoc#combineByKeyWithClassTag[combineByKeyWithClassTag]
** `combineByKey`
** `aggregateByKey`
** `foldByKey`
** `reduceByKey`
** `countApproxDistinctByKey`
** `groupByKey`
* `partitionBy`

NOTE: There may be other dependent methods that use the above.
