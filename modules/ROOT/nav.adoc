* xref:spark-overview.adoc[Overview]

* xref:spark-barrier-execution-mode.adoc[Barrier Execution Mode]

* Spark Core / Shuffle System

** xref:spark-shuffle-ShuffleManager.adoc[ShuffleManager Contract -- Pluggable Shuffle Systems]
*** xref:spark-shuffle-SortShuffleManager.adoc[SortShuffleManager -- The Default Shuffle System]

** xref:spark-shuffle-ShuffleHandle.adoc[ShuffleHandle Contract]
*** xref:spark-shuffle-BaseShuffleHandle.adoc[BaseShuffleHandle -- Fallback Shuffle Handle]
*** xref:spark-shuffle-BypassMergeSortShuffleHandle.adoc[BypassMergeSortShuffleHandle -- Marker Interface for Bypass Merge Sort Shuffle Handles]
*** xref:spark-shuffle-SerializedShuffleHandle.adoc[SerializedShuffleHandle -- Marker Interface for Serialized Shuffle Handles]

** xref:spark-shuffle-ShuffleWriter.adoc[ShuffleWriter Contract]
*** xref:spark-shuffle-BypassMergeSortShuffleWriter.adoc[BypassMergeSortShuffleWriter]
*** xref:spark-shuffle-SortShuffleWriter.adoc[SortShuffleWriter]
*** xref:spark-shuffle-UnsafeShuffleWriter.adoc[UnsafeShuffleWriter -- ShuffleWriter for SerializedShuffleHandle]

** xref:spark-shuffle-ShuffleReader.adoc[ShuffleReader]
*** xref:spark-shuffle-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader]

** xref:spark-shuffle-ShuffleExternalSorter.adoc[ShuffleExternalSorter -- Cache-Efficient Sorter]

** xref:spark-shuffle-ShuffleBlockResolver.adoc[ShuffleBlockResolver]
*** xref:spark-shuffle-IndexShuffleBlockResolver.adoc[IndexShuffleBlockResolver]

** xref:spark-shuffle-ShuffleInMemorySorter.adoc[ShuffleInMemorySorter]

* Spark Core / High-Level Spark Stage Scheduler

** xref:spark-scheduler-DAGScheduler.adoc[DAGScheduler -- Stage-Oriented Scheduler]

** xref:spark-scheduler-Stage.adoc[Stage -- Physical Unit Of Execution]
*** xref:spark-scheduler-ShuffleMapStage.adoc[ShuffleMapStage -- Intermediate Stage in Execution DAG]
*** xref:spark-scheduler-ResultStage.adoc[ResultStage -- Final Stage in Job]

** xref:spark-scheduler-StageInfo.adoc[StageInfo]

** xref:spark-scheduler-DAGSchedulerEventProcessLoop.adoc[DAGScheduler Event Bus]

** xref:spark-scheduler-JobListener.adoc[JobListener]
*** xref:spark-scheduler-JobWaiter.adoc[JobWaiter]

* Spark Core / Low-Level Spark Task Scheduler

** xref:spark-scheduler-ActiveJob.adoc[Jobs]

** xref:spark-scheduler-SchedulableBuilder.adoc[SchedulableBuilder Contract -- Builders of Schedulable Pools]
*** xref:spark-scheduler-FIFOSchedulableBuilder.adoc[FIFOSchedulableBuilder]
*** xref:spark-scheduler-FairSchedulableBuilder.adoc[FairSchedulableBuilder]

** xref:spark-scheduler-TaskScheduler.adoc[TaskScheduler Contract -- Spark Schedulers]
*** xref:spark-scheduler-TaskSchedulerImpl.adoc[TaskSchedulerImpl -- Default TaskScheduler]

** xref:spark-scheduler-Task.adoc[Task -- Smallest Individual Unit of Execution]
*** xref:spark-scheduler-ShuffleMapTask.adoc[ShuffleMapTask -- Task to Compute MapStatus for ShuffleMapStage]
*** xref:spark-scheduler-ResultTask.adoc[ResultTask -- Task to Compute Result for ResultStage]

** xref:spark-scheduler-TaskSet.adoc[TaskSet -- Set of Missing Tasks (of Stage)]

** xref:spark-scheduler-TaskSetManager.adoc[TaskSetManager]

** xref:spark-scheduler-Schedulable.adoc[Schedulable Contract -- Schedulable Entities]
*** xref:spark-scheduler-Pool.adoc[Schedulable Pool]

** xref:spark-scheduler-SchedulingMode.adoc[Scheduling Mode -- `spark.scheduler.mode` Spark Property]

** xref:spark-scheduler-TaskInfo.adoc[TaskInfo]

** xref:spark-TaskRunner-FetchFailedException.adoc[FetchFailedException]

** xref:spark-scheduler-MapStatus.adoc[MapStatus -- Shuffle Map Output Status]

** xref:spark-scheduler-TaskDescription.adoc[TaskDescription -- Metadata of Single Task]

** xref:spark-taskschedulerimpl-speculative-execution.adoc[Speculative Execution of Tasks]
** xref:spark-scheduler-TaskResultGetter.adoc[TaskResultGetter]

** xref:spark-TaskContext.adoc[TaskContext]
*** xref:spark-BarrierTaskContext.adoc[BarrierTaskContext -- TaskContext for Barrier Tasks]
*** xref:spark-TaskContextImpl.adoc[TaskContextImpl -- Default TaskContext]

** xref:spark-scheduler-TaskResult.adoc[TaskResults -- DirectTaskResult and IndirectTaskResult]

** xref:spark-scheduler-TaskSetBlacklist.adoc[TaskSetBlacklist -- Blacklisting Executors and Nodes For TaskSet]

* Spark Core / Memory Management

** xref:spark-memory-unified-memory-management.adoc[Unified Memory Management]

** xref:spark-memory-TaskMemoryManager.adoc[TaskMemoryManager -- Memory Manager of Single Task]
*** xref:spark-memory-MemoryConsumer.adoc[MemoryConsumer]

* Spark Core / Spark Local

** xref:local/spark-local.adoc[Spark local]
** xref:local/spark-LocalSchedulerBackend.adoc[LocalSchedulerBackend]
** xref:local/spark-LocalEndpoint.adoc[LocalEndpoint -- RPC Endpoint for LocalSchedulerBackend]

** xref:spark-LauncherBackend.adoc[LauncherBackend]

* Spark Core / Architecture

** xref:spark-architecture.adoc[Spark Architecture]
** xref:spark-driver.adoc[Driver]
** xref:spark-Executor.adoc[Executor]
*** xref:spark-Executor-TaskRunner.adoc[TaskRunner]
*** xref:spark-executor-ExecutorSource.adoc[ExecutorSource]
** xref:spark-master.adoc[Master]
** xref:spark-workers.adoc[Workers]

** xref:spark-SparkConf.adoc[SparkConf -- Programmable Configuration for Spark Applications]
*** xref:spark-properties.adoc[Spark Properties and spark-defaults.conf Properties File]
*** xref:spark-deploy-mode.adoc[Deploy Mode]

** xref:spark-SparkContext.adoc[SparkContext]
*** xref:spark-HeartbeatReceiver.adoc[HeartbeatReceiver RPC Endpoint]
*** xref:spark-SparkContext-creating-instance-internals.adoc[Inside Creating SparkContext]
*** xref:spark-sparkcontext-ConsoleProgressBar.adoc[ConsoleProgressBar]
*** xref:spark-sparkcontext-SparkStatusTracker.adoc[SparkStatusTracker]
*** xref:spark-sparkcontext-local-properties.adoc[Local Properties -- Creating Logical Job Groups]

** xref:spark-configuration-properties.adoc[Spark Configuration Properties]

* Spark Core / Transferring Data Blocks In Spark Cluster

** xref:spark-ShuffleClient.adoc[ShuffleClient -- Contract to Fetch Shuffle Blocks]
*** xref:spark-BlockTransferService.adoc[BlockTransferService -- Pluggable Block Transfers (To Fetch and Upload Blocks)]
*** xref:spark-ShuffleClient-ExternalShuffleClient.adoc[ExternalShuffleClient]

** xref:spark-NettyBlockTransferService.adoc[NettyBlockTransferService -- Netty-Based BlockTransferService]
*** xref:spark-NettyBlockRpcServer.adoc[NettyBlockRpcServer -- NettyBlockTransferService's RpcHandler]

** xref:spark-BlockFetchingListener.adoc[BlockFetchingListener]
** xref:spark-RetryingBlockFetcher.adoc[RetryingBlockFetcher]
*** xref:spark-RetryingBlockFetcher-BlockFetchStarter.adoc[BlockFetchStarter]

* Spark Core / Web UI

** xref:spark-webui.adoc[Web UI -- Spark Application's Web Console]
*** xref:spark-webui-jobs.adoc[Jobs]
*** xref:spark-webui-stages.adoc[Stages]
*** xref:spark-webui-storage.adoc[Storage]
*** xref:spark-webui-environment.adoc[Environment]
*** xref:spark-webui-executors.adoc[Executors]

** xref:spark-webui-JobsTab.adoc[JobsTab]
*** xref:spark-webui-AllJobsPage.adoc[AllJobsPage]
*** xref:spark-webui-JobPage.adoc[JobPage]

** xref:spark-webui-StagesTab.adoc[StagesTab -- Stages for All Jobs]
*** xref:spark-webui-AllStagesPage.adoc[AllStagesPage -- Stages for All Jobs]
*** xref:spark-webui-StagePage.adoc[StagePage -- Stage Details]
*** xref:spark-webui-PoolPage.adoc[PoolPage -- Pool Details]

** xref:spark-webui-StorageTab.adoc[StorageTab]
*** xref:spark-webui-StoragePage.adoc[StoragePage]
*** xref:spark-webui-RDDPage.adoc[RDDPage]

** xref:spark-webui-EnvironmentTab.adoc[EnvironmentTab]
*** xref:spark-webui-EnvironmentPage.adoc[EnvironmentPage]

** xref:spark-webui-ExecutorsTab.adoc[ExecutorsTab]
*** xref:spark-webui-ExecutorsPage.adoc[ExecutorsPage]
*** xref:spark-webui-ExecutorThreadDumpPage.adoc[ExecutorThreadDumpPage]

** xref:spark-webui-SparkUI.adoc[SparkUI -- Web UI of Spark Application]
*** xref:spark-webui-SparkUITab.adoc[SparkUITab]

** xref:spark-webui-BlockStatusListener.adoc[BlockStatusListener Spark Listener]
** xref:spark-webui-EnvironmentListener.adoc[EnvironmentListener Spark Listener]
** xref:spark-webui-executors-ExecutorsListener.adoc[ExecutorsListener Spark Listener]
** xref:spark-webui-JobProgressListener.adoc[JobProgressListener Spark Listener]
** xref:spark-webui-StorageStatusListener.adoc[StorageStatusListener Spark Listener]
** xref:spark-webui-StorageListener.adoc[StorageListener -- Spark Listener for Tracking Persistence Status of RDD Blocks]
** xref:spark-webui-RDDOperationGraphListener.adoc[RDDOperationGraphListener Spark Listener]

** xref:spark-webui-WebUI.adoc[WebUI -- Framework For Web UIs]
*** xref:spark-webui-WebUIPage.adoc[WebUIPage -- Contract of Pages in Web UI]
*** xref:spark-webui-WebUITab.adoc[WebUITab -- Contract of Tabs in Web UI]

** xref:spark-webui-RDDStorageInfo.adoc[RDDStorageInfo]
** xref:spark-core-RDDInfo.adoc[RDDInfo]

** xref:spark-core-LiveEntity.adoc[LiveEntity]
*** xref:spark-core-LiveRDD.adoc[LiveRDD]

** xref:spark-webui-UIUtils.adoc[UIUtils]
** xref:spark-webui-JettyUtils.adoc[JettyUtils]

** xref:spark-webui-properties.adoc[web UI Configuration Properties]

* Spark Core / Metrics

** xref:spark-metrics.adoc[Spark Metrics]
** xref:spark-metrics-MetricsSystem.adoc[MetricsSystem]
** xref:spark-metrics-MetricsConfig.adoc[MetricsConfig -- Metrics System Configuration]

** xref:spark-metrics-Source.adoc[Source -- Contract of Metrics Sources]
*** xref:spark-scheduler-DAGSchedulerSource.adoc[DAGSchedulerSource -- Metrics Source for DAGScheduler]

** xref:spark-metrics-Sink.adoc[Sink -- Contract of Metrics Sinks]
*** xref:spark-metrics-MetricsServlet.adoc[MetricsServlet JSON Metrics Sink]
** xref:spark-metrics-properties.adoc[Metrics Configuration Properties]

** xref:spark-executor-TaskMetrics.adoc[TaskMetrics]
*** xref:spark-executor-ShuffleWriteMetrics.adoc[ShuffleWriteMetrics]

* Spark Core / Status REST API

** xref:spark-api.adoc[Status REST API -- Monitoring Spark Applications Using REST API]

** xref:spark-api-ApiRootResource.adoc[ApiRootResource -- /api/v1 URI Handler]
*** xref:spark-api-ApplicationListResource.adoc[ApplicationListResource -- applications URI Handler]
*** xref:spark-api-OneApplicationResource.adoc[OneApplicationResource -- applications/appId URI Handler]
**** xref:spark-api-StagesResource.adoc[StagesResource]
*** xref:spark-api-OneApplicationAttemptResource.adoc[OneApplicationAttemptResource]

** xref:spark-api-AbstractApplicationResource.adoc[AbstractApplicationResource]
** xref:spark-api-BaseAppResource.adoc[BaseAppResource]
** xref:spark-api-ApiRequestContext.adoc[ApiRequestContext]

** xref:spark-api-UIRoot.adoc[UIRoot -- Contract for Root Contrainers of Application UI Information]
*** xref:spark-api-UIRootFromServletContext.adoc[UIRootFromServletContext]

* Spark Core / Tools

** xref:spark-shell.adoc[Spark Shell -- spark-shell shell script]

** xref:spark-submit.adoc[Spark Submit -- spark-submit shell script]
*** xref:spark-submit-SparkSubmitArguments.adoc[SparkSubmitArguments]
*** xref:spark-submit-SparkSubmitOptionParser.adoc[SparkSubmitOptionParser -- spark-submit's Command-Line Parser]
*** xref:spark-submit-SparkSubmitCommandBuilder.adoc[`SparkSubmitCommandBuilder` Command Builder]

** xref:spark-class.adoc[spark-class shell script]
*** xref:spark-AbstractCommandBuilder.adoc[AbstractCommandBuilder]

** xref:spark-SparkLauncher.adoc[SparkLauncher -- Launching Spark Applications Programmatically]

* Spark Core / RDD

** xref:spark-anatomy-spark-application.adoc[Anatomy of Spark Application]

** xref:spark-rdd.adoc[RDD -- Resilient Distributed Dataset]
*** xref:spark-rdd-RDD.adoc[RDD -- Description of Distributed Computation]
*** xref:spark-rdd-lineage.adoc[RDD Lineage -- Logical Execution Plan]
*** xref:spark-TaskLocation.adoc[TaskLocation]
*** xref:spark-rdd-ParallelCollectionRDD.adoc[ParallelCollectionRDD]
*** xref:spark-rdd-MapPartitionsRDD.adoc[MapPartitionsRDD]
*** xref:spark-rdd-OrderedRDDFunctions.adoc[OrderedRDDFunctions]
*** xref:spark-rdd-CoGroupedRDD.adoc[CoGroupedRDD]
*** xref:spark-rdd-SubtractedRDD.adoc[SubtractedRDD]
*** xref:spark-rdd-HadoopRDD.adoc[HadoopRDD]
*** xref:spark-rdd-NewHadoopRDD.adoc[NewHadoopRDD]
*** xref:spark-rdd-ShuffledRDD.adoc[ShuffledRDD]

** xref:spark-rdd-operations.adoc[Operators]
*** xref:spark-rdd-transformations.adoc[Transformations -- Lazy Operations on RDD (to Create One or More RDDs)]
**** xref:spark-rdd-PairRDDFunctions.adoc[PairRDDFunctions]
*** xref:spark-rdd-actions.adoc[Actions]

** xref:spark-rdd-caching.adoc[Caching and Persistence]
*** xref:spark-rdd-StorageLevel.adoc[StorageLevel]

** xref:spark-rdd-partitions.adoc[Partitions and Partitioning]
*** xref:spark-rdd-Partition.adoc[Partition]
*** xref:spark-rdd-Partitioner.adoc[Partitioner]
**** xref:spark-rdd-HashPartitioner.adoc[HashPartitioner]

** xref:spark-rdd-shuffle.adoc[Shuffling]

** xref:spark-rdd-checkpointing.adoc[Checkpointing]
*** xref:spark-rdd-CheckpointRDD.adoc[CheckpointRDD]

** xref:spark-rdd-Dependency.adoc[RDD Dependencies]
*** xref:spark-rdd-NarrowDependency.adoc[NarrowDependency -- Narrow Dependencies]
*** xref:spark-rdd-ShuffleDependency.adoc[ShuffleDependency -- Shuffle Dependencies]

** xref:spark-Aggregator.adoc[Map/Reduce-side Aggregator]

** xref:spark-core-AppStatusStore.adoc[AppStatusStore]
** xref:spark-core-AppStatusPlugin.adoc[AppStatusPlugin]

** xref:spark-core-KVStore.adoc[KVStore]
*** xref:spark-core-KVStoreView.adoc[KVStoreView]
*** xref:spark-core-ElementTrackingStore.adoc[ElementTrackingStore]
*** xref:spark-core-InMemoryStore.adoc[InMemoryStore]
*** xref:spark-core-LevelDB.adoc[LevelDB]

** xref:spark-InterruptibleIterator.adoc[InterruptibleIterator -- Iterator With Support For Task Cancellation]

** xref:spark-RDDBarrier.adoc[RDDBarrier]

* Spark Core / Optimizations

** xref:spark-broadcast.adoc[Broadcast variables]
** xref:spark-accumulators.adoc[Accumulators]
*** xref:spark-AccumulatorContext.adoc[AccumulatorContext]

* Spark Core / Services

** xref:spark-SerializerManager.adoc[SerializerManager]

** xref:spark-MemoryManager.adoc[MemoryManager -- Memory Management]
*** xref:spark-UnifiedMemoryManager.adoc[UnifiedMemoryManager -- Spark's Memory Manager]
*** xref:spark-StaticMemoryManager.adoc[StaticMemoryManager -- Legacy Memory Manager]
*** xref:spark-MemoryManager-properties.adoc[MemoryManager Configuration Properties]

** xref:spark-SparkEnv.adoc[SparkEnv -- Spark Runtime Environment]

** xref:spark-SchedulerBackend.adoc[SchedulerBackend -- Pluggable Task Scheduling Systems]
*** xref:spark-CoarseGrainedSchedulerBackend.adoc[CoarseGrainedSchedulerBackend]
**** xref:spark-CoarseGrainedSchedulerBackend-DriverEndpoint.adoc[DriverEndpoint -- CoarseGrainedSchedulerBackend RPC Endpoint]

** xref:spark-ExecutorBackend.adoc[ExecutorBackend -- Pluggable Executor Backends]
*** xref:spark-CoarseGrainedExecutorBackend.adoc[CoarseGrainedExecutorBackend]
*** xref:spark-executor-backends-MesosExecutorBackend.adoc[MesosExecutorBackend]

** xref:spark-ExternalShuffleService.adoc[ExternalShuffleService]
** xref:spark-OneForOneStreamManager.adoc[OneForOneStreamManager]
** xref:spark-ShuffleBlockFetcherIterator.adoc[ShuffleBlockFetcherIterator]
** xref:spark-ExternalSorter.adoc[ExternalSorter]

** xref:spark-BlockManager.adoc[BlockManager -- Key-Value Store of Blocks of Data]
*** xref:spark-MemoryStore.adoc[MemoryStore]
*** xref:spark-BlockEvictionHandler.adoc[BlockEvictionHandler]
*** xref:spark-StorageMemoryPool.adoc[StorageMemoryPool]
*** xref:spark-MemoryPool.adoc[MemoryPool]
*** xref:spark-DiskStore.adoc[DiskStore]
*** xref:spark-BlockDataManager.adoc[BlockDataManager]
*** xref:spark-RpcHandler.adoc[RpcHandler]
*** xref:spark-RpcResponseCallback.adoc[RpcResponseCallback]
*** xref:spark-TransportRequestHandler.adoc[TransportRequestHandler]
*** xref:spark-TransportContext.adoc[TransportContext]
*** xref:spark-TransportServer.adoc[TransportServer]
*** xref:spark-TransportClientFactory.adoc[TransportClientFactory]
*** xref:spark-MessageHandler.adoc[MessageHandler]
*** xref:spark-BlockManagerMaster.adoc[BlockManagerMaster -- BlockManager for Driver]
**** xref:spark-blockmanager-BlockManagerMasterEndpoint.adoc[BlockManagerMasterEndpoint -- BlockManagerMaster RPC Endpoint]
*** xref:spark-DiskBlockManager.adoc[DiskBlockManager]
*** xref:spark-BlockInfoManager.adoc[BlockInfoManager]
**** xref:spark-BlockInfo.adoc[BlockInfo]
*** xref:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint]
*** xref:spark-blockmanager-DiskBlockObjectWriter.adoc[DiskBlockObjectWriter]
*** xref:spark-BlockManager-BlockManagerSource.adoc[BlockManagerSource -- Metrics Source for BlockManager]
*** xref:spark-BlockManager-ShuffleMetricsSource.adoc[ShuffleMetricsSource -- Metrics Source of BlockManager for Shuffle-Related Metrics]
*** xref:spark-blockmanager-StorageStatus.adoc[StorageStatus]
*** xref:spark-ManagedBuffer.adoc[ManagedBuffer]

** xref:spark-service-mapoutputtracker.adoc[MapOutputTracker -- Shuffle Map Output Registry]
*** xref:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster -- MapOutputTracker For Driver]
**** xref:spark-service-MapOutputTrackerMasterEndpoint.adoc[MapOutputTrackerMasterEndpoint]
*** xref:spark-service-MapOutputTrackerWorker.adoc[MapOutputTrackerWorker -- MapOutputTracker for Executors]

** xref:spark-serialization.adoc[Serialization]
*** xref:spark-Serializer.adoc[Serializer -- Task SerDe]
*** xref:spark-SerializerInstance.adoc[SerializerInstance]
*** xref:spark-SerializationStream.adoc[SerializationStream]
*** xref:spark-DeserializationStream.adoc[DeserializationStream]

** xref:spark-ExternalClusterManager.adoc[ExternalClusterManager -- Pluggable Cluster Managers]

** xref:spark-service-broadcastmanager.adoc[BroadcastManager]
*** xref:spark-BroadcastFactory.adoc[BroadcastFactory -- Pluggable Broadcast Variable Factories]
**** xref:spark-TorrentBroadcastFactory.adoc[TorrentBroadcastFactory]
**** xref:spark-TorrentBroadcast.adoc[TorrentBroadcast]
*** xref:spark-CompressionCodec.adoc[CompressionCodec]

** xref:spark-service-contextcleaner.adoc[ContextCleaner -- Spark Application Garbage Collector]
*** xref:spark-CleanerListener.adoc[CleanerListener]

** xref:spark-dynamic-allocation.adoc[Dynamic Allocation (of Executors)]
*** xref:spark-ExecutorAllocationManager.adoc[ExecutorAllocationManager -- Allocation Manager for Spark Core]
*** xref:spark-service-ExecutorAllocationClient.adoc[ExecutorAllocationClient]
*** xref:spark-service-ExecutorAllocationManagerSource.adoc[ExecutorAllocationManagerSource]

** xref:spark-http-file-server.adoc[HTTP File Server]
** xref:spark-data-locality.adoc[Data Locality]
** xref:spark-cachemanager.adoc[Cache Manager]
** xref:spark-service-outputcommitcoordinator.adoc[OutputCommitCoordinator]

** xref:spark-rpc.adoc[RpcEnv -- RPC Environment]
*** xref:spark-rpc-RpcEnv.adoc[RpcEnv]
*** xref:spark-rpc-RpcEndpoint.adoc[RpcEndpoint]
*** xref:spark-RpcEndpointRef.adoc[RpcEndpointRef]
*** xref:spark-RpcEnvFactory.adoc[RpcEnvFactory]
*** xref:spark-rpc-netty.adoc[Netty-based RpcEnv]

** xref:spark-TransportConf.adoc[TransportConf -- Transport Configuration]
** xref:spark-Utils.adoc[Utils Helper Object]

* Spark Core / Security

** xref:spark-webui-security.adoc[Securing Web UI]

* Spark Deployment Environments

** xref:spark-deployment-environments.adoc[Deployment Environments -- Run Modes]
** xref:spark-cluster.adoc[Spark on cluster]

* Execution Model

** xref:spark-execution-model.adoc[Execution Model]

* Monitoring, Tuning, Debugging and Testing

** xref:spark-history-server.adoc[Spark History Server]
*** xref:spark-history-server-HistoryServer.adoc[HistoryServer -- WebUI For Active And Completed Spark Applications]
*** xref:spark-history-server-SQLHistoryListener.adoc[SQLHistoryListener]
*** xref:spark-history-server-FsHistoryProvider.adoc[FsHistoryProvider -- File-System-Based History Provider]
*** xref:spark-history-server-ApplicationHistoryProvider.adoc[ApplicationHistoryProvider]
*** xref:spark-history-server-HistoryServerArguments.adoc[HistoryServerArguments]
*** xref:spark-history-server-ApplicationCacheOperations.adoc[ApplicationCacheOperations]
*** xref:spark-history-server-ApplicationCache.adoc[ApplicationCache]

** xref:spark-logging.adoc[Logging]
** xref:spark-tuning.adoc[Performance Tuning]

** xref:spark-scheduler-SparkListener.adoc[SparkListener -- Intercepting Events from Spark Scheduler]
*** xref:spark-SparkListener-AppStatusListener.adoc[AppStatusListener]
*** xref:spark-SparkListener-EventLoggingListener.adoc[EventLoggingListener -- Spark Listener for Persisting Events]
*** xref:spark-SparkListener-ExecutorAllocationListener.adoc[ExecutorAllocationListener]
*** xref:spark-SparkListener-SpillListener.adoc[SpillListener -- Detecting Spills in Jobs (for Testing)]
*** xref:spark-SparkListener-StatsReportListener.adoc[StatsReportListener -- Logging Summary Statistics]

** xref:spark-scheduler-LiveListenerBus.adoc[LiveListenerBus]

** xref:spark-SparkListenerBus.adoc[SparkListenerBus -- Internal Contract for Spark Event Buses]
*** xref:spark-SparkListenerBus-AsyncEventQueue.adoc[AsyncEventQueue]
*** xref:spark-SparkListenerBus-ReplayListenerBus.adoc[ReplayListenerBus]

** xref:spark-JsonProtocol.adoc[JsonProtocol]

** xref:spark-debugging.adoc[Debugging Spark]

* Varia

** xref:varia/spark-building-from-sources.adoc[Building Apache Spark from Sources]
** xref:varia/spark-hadoop.adoc[Spark and Hadoop]
*** xref:spark-SparkHadoopUtil.adoc[SparkHadoopUtil]
** xref:varia/spark-inmemory-filesystems.adoc[Spark and software in-memory file systems]
** xref:varia/spark-others.adoc[Spark and The Others]
** xref:varia/spark-deeplearning.adoc[Distributed Deep Learning on Spark]
** xref:varia/spark-packages.adoc[Spark Packages]

* Interactive Notebooks

** xref:interactive-notebooks/README.adoc[Interactive Notebooks]
*** xref:interactive-notebooks/apache-zeppelin.adoc[Apache Zeppelin]
*** xref:interactive-notebooks/spark-notebook.adoc[Spark Notebook]

* Spark Tips and Tricks

** xref:spark-tips-and-tricks.adoc[Spark Tips and Tricks]
** xref:spark-tips-and-tricks-access-private-members-spark-shell.adoc[Access private members in Scala in Spark shell]
** xref:spark-tips-and-tricks-sparkexception-task-not-serializable.adoc[SparkException: Task not serializable]
** xref:spark-tips-and-tricks-running-spark-windows.adoc[Running Spark Applications on Windows]

* Further Learning

** xref:spark-courses.adoc[Courses]
** xref:spark-books.adoc[Books]

* xref:spark-sql.adoc[Spark SQL]

* xref:spark-structured-streaming.adoc[Spark Structured Streaming]
