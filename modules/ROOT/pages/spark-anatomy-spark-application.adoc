== Anatomy of Spark Application

Every Spark application starts from creating link:spark-SparkContext.adoc[SparkContext].

NOTE: Without link:spark-SparkContext.adoc[SparkContext] no computation (as a Spark job) can be started.

NOTE: A Spark application is an instance of SparkContext. Or, put it differently, a Spark context constitutes a Spark application.

A Spark application is uniquely identified by a pair of the link:spark-SparkContext.adoc#applicationId[application] and link:spark-SparkContext.adoc#applicationAttemptId[application attempt] ids.

For it to work, you have to link:spark-SparkConf.adoc[create a Spark configuration using SparkConf] or use a link:spark-SparkContext.adoc#creating-instance[custom SparkContext constructor].

[source, scala]
----
package pl.japila.spark

import org.apache.spark.{SparkContext, SparkConf}

object SparkMeApp {
  def main(args: Array[String]) {

    val masterURL = "local[*]"  // <1>

    val conf = new SparkConf()  // <2>
      .setAppName("SparkMe Application")
      .setMaster(masterURL)

    val sc = new SparkContext(conf) // <3>

    val fileName = util.Try(args(0)).getOrElse("build.sbt")

    val lines = sc.textFile(fileName).cache() // <4>

    val c = lines.count() // <5>
    println(s"There are $c lines in $fileName")
  }
}
----
<1> link:spark-deployment-environments.adoc#master-urls[Master URL] to connect the application to
<2> Create Spark configuration
<3> Create Spark context
<4> Create `lines` RDD
<5> Execute `count` action

TIP: link:spark-shell.adoc[Spark shell] creates a Spark context and SQL context for you at startup.

When a Spark application starts (using link:spark-submit.adoc[spark-submit script] or as a standalone application), it connects to link:spark-master.adoc[Spark master] as described by link:spark-deployment-environments.adoc#master-urls[master URL]. It is part of link:spark-SparkContext.adoc#creating-instance[Spark context's initialization].

.Submitting Spark application to master using master URL
image::diagrams/spark-submit-master-workers.png[align="center"]

NOTE: Your Spark application can run locally or on the cluster which is based on the cluster manager and the deploy mode (`--deploy-mode`). Refer to link:spark-deployment-environments.adoc[Deployment Modes].

You can then link:spark-rdd.adoc#creating-rdds[create RDDs], link:spark-rdd-transformations.adoc[transform them to other RDDs] and ultimately link:spark-rdd-actions.adoc[execute actions]. You can also link:spark-rdd-caching.adoc[cache interim RDDs] to speed up data processing.

After all the data processing is completed, the Spark application finishes by link:spark-SparkContext.adoc#stopping[stopping the Spark context].
