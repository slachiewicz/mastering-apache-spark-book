== [[YarnSchedulerEndpoint]] YarnSchedulerEndpoint RPC Endpoint

`YarnSchedulerEndpoint` is a link:spark-rpc.adoc#ThreadSafeRpcEndpoint[thread-safe RPC endpoint] for communication between link:spark-yarn-yarnschedulerbackend.adoc[YarnSchedulerBackend] on the driver and link:spark-yarn-applicationmaster.adoc[ApplicationMaster] on YARN (inside a YARN container).

CAUTION: FIXME Picture it.

It uses the <<amEndpoint, reference to the remote ApplicationMaster RPC Endpoint>> to send messages to.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint=INFO
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[messages]] RPC Messages

==== [[RequestExecutors]] RequestExecutors

[source, scala]
----
RequestExecutors(
  requestedTotal: Int,
  localityAwareTasks: Int,
  hostToLocalTaskCount: Map[String, Int])
extends CoarseGrainedClusterMessage
----

`RequestExecutors` is to inform link:spark-yarn-applicationmaster.adoc[ApplicationMaster] about the current requirements for the total number of executors (as `requestedTotal`), including already pending and running executors.

.RequestExecutors Message Flow (client deploy mode)
image::../images/spark-YarnSchedulerEndpoint-RequestExecutors.png[align="center"]

When a `RequestExecutors` arrives, `YarnSchedulerEndpoint` simply passes it on to link:spark-yarn-applicationmaster.adoc[ApplicationMaster] (via the <<amEndpoint, internal RPC endpoint reference>>). The result of the forward call is sent back in response.

Any issues communicating with the remote `ApplicationMaster` RPC endpoint are reported as ERROR messages in the logs:

```
ERROR Sending RequestExecutors to AM was unsuccessful
```

==== [[RemoveExecutor]] RemoveExecutor

==== [[KillExecutors]] KillExecutors

==== [[AddWebUIFilter]] AddWebUIFilter

[source, scala]
----
AddWebUIFilter(
  filterName: String,
  filterParams: Map[String, String],
  proxyBase: String)
----

`AddWebUIFilter` triggers setting `spark.ui.proxyBase` system property and adding the `filterName` filter to web UI.

`AddWebUIFilter` is sent by link:spark-yarn-applicationmaster.adoc#addAmIpFilter[`ApplicationMaster` when it adds `AmIpFilter` to web UI].

It firstly sets `spark.ui.proxyBase` system property to the input `proxyBase` (if not empty).

If it defines a filter, i.e. the input `filterName` and `filterParams` are both not empty, you should see the following INFO message in the logs:

```
INFO Add WebUI Filter. [filterName], [filterParams], [proxyBase]
```

It then sets `spark.ui.filters` to be the input `filterName` in the internal `conf` link:../spark-SparkConf.adoc[SparkConf] attribute.

All the `filterParams` are also set as `spark.[filterName].param.[key]` and `[value]`.

The filter is added to web UI using `JettyUtils.addFilters(ui.getHandlers, conf)`.

CAUTION: FIXME Review `JettyUtils.addFilters(ui.getHandlers, conf)`.

==== [[RegisterClusterManager]] RegisterClusterManager Message

[source, scala]
----
RegisterClusterManager(am: RpcEndpointRef)
----

When `RegisterClusterManager` message arrives, the following INFO message is printed out to the logs:

```
INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as [am]
```

The <<amEndpoint, internal reference to the remote ApplicationMaster RPC Endpoint>> is set (to `am`).

If the internal link:spark-yarn-yarnschedulerbackend.adoc#shouldResetOnAmRegister[shouldResetOnAmRegister] flag is enabled, link:spark-yarn-yarnschedulerbackend.adoc#reset[YarnSchedulerBackend is reset]. It is disabled initially, so `shouldResetOnAmRegister` is enabled.

NOTE: `shouldResetOnAmRegister` controls link:spark-yarn-cluster-YarnSchedulerEndpoint.adoc#RegisterClusterManager[whether to reset `YarnSchedulerBackend` when another `RegisterClusterManager` RPC message arrives] that could be because the link:spark-yarn-applicationmaster.adoc[ApplicationManager] failed and a new one was registered.

==== [[RetrieveLastAllocatedExecutorId]] RetrieveLastAllocatedExecutorId

When `RetrieveLastAllocatedExecutorId` is received, `YarnSchedulerEndpoint` responds with the current value of link:../spark-CoarseGrainedSchedulerBackend.adoc#currentExecutorIdCounter[currentExecutorIdCounter].

NOTE: It is used by link:spark-yarn-YarnAllocator.adoc[`YarnAllocator` to initialize the internal `executorIdCounter` (so it gives proper identifiers for new executors when `ApplicationMaster` restarts)]

=== [[onDisconnected]] onDisconnected Callback

`onDisconnected` clears the <<amEndpoint, internal reference to the remote ApplicationMaster RPC Endpoint>> (i.e. it sets it to `None`) if the remote address matches the reference's.

NOTE: It is a callback method to be called when...FIXME

You should see the following WARN message in the logs if that happens:

```
WARN ApplicationMaster has disassociated: [remoteAddress]
```

=== [[onStop]] onStop Callback

`onStop` shuts <<askAmThreadPool, askAmThreadPool>> down immediately.

NOTE: `onStop` is a callback method to be called when...FIXME

=== [[amEndpoint]] Internal Reference to ApplicationMaster RPC Endpoint (amEndpoint variable)

`amEndpoint` is a reference to a remote link:spark-yarn-AMEndpoint.adoc[ApplicationMaster RPC Endpoint].

It is set to the current link:spark-yarn-AMEndpoint.adoc#onStart[ApplicationMaster RPC Endpoint] when <<RegisterClusterManager, RegisterClusterManager>> arrives and cleared when <<onDisconnected, the connection to the endpoint disconnects>>.

=== [[askAmThreadPool]] askAmThreadPool Thread Pool

`askAmThreadPool` is a thread pool called *yarn-scheduler-ask-am-thread-pool* that creates new threads as needed and reuses previously constructed threads when they are available.
