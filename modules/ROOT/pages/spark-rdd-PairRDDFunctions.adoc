== [[PairRDDFunctions]] PairRDDFunctions

TIP: Read up the scaladoc of http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions[PairRDDFunctions].

`PairRDDFunctions` are available in RDDs of key-value pairs via Scala's implicit conversion.

TIP: *Partitioning* is an advanced feature that is directly linked to (or inferred by) use of `PairRDDFunctions`. Read up about it in link:spark-rdd-partitions.adoc[Partitions and Partitioning].

=== [[countApproxDistinctByKey]] `countApproxDistinctByKey` Transformation

CAUTION: FIXME

=== [[foldByKey]] `foldByKey` Transformation

CAUTION: FIXME

=== [[aggregateByKey]] `aggregateByKey` Transformation

CAUTION: FIXME

=== [[combineByKey]] `combineByKey` Transformation

CAUTION: FIXME

=== [[partitionBy]] `partitionBy` Operator

[source, scala]
----
partitionBy(partitioner: Partitioner): RDD[(K, V)]
----

CAUTION: FIXME

=== [[reduceByKey]][[groupByKey]] `groupByKey` and `reduceByKey` Transformations

`reduceByKey` is sort of a particular case of <<aggregateByKey, aggregateByKey>>.

You may want to look at the number of partitions from another angle.

It may often not be important to have a given number of partitions upfront (at RDD creation time upon link:spark-data-sources.adoc[loading data from data sources]), so only "regrouping" the data by key after it is an RDD might be...the key (_pun not intended_).

You can use `groupByKey` or another `PairRDDFunctions` method to have a key in one processing flow.

You could use `partitionBy` that is available for RDDs to be RDDs of tuples, i.e. `PairRDD`:

```
rdd.keyBy(_.kind)
  .partitionBy(new HashPartitioner(PARTITIONS))
  .foreachPartition(...)
```

Think of situations where `kind` has low cardinality or highly skewed distribution and using the technique for partitioning might be not an optimal solution.

You could do as follows:

```
rdd.keyBy(_.kind).reduceByKey(....)
```

or `mapValues` or plenty of other solutions. _FIXME, man_.

=== [[mapValues]][[flatMapValues]] mapValues, flatMapValues

CAUTION: FIXME

=== [[combineByKeyWithClassTag]] `combineByKeyWithClassTag` Transformations

[source, scala]
----
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C)(implicit ct: ClassTag[C]): RDD[(K, C)] // <1>
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  numPartitions: Int)(implicit ct: ClassTag[C]): RDD[(K, C)] // <2>
combineByKeyWithClassTag[C](
  createCombiner: V => C,
  mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C,
  partitioner: Partitioner,
  mapSideCombine: Boolean = true,
  serializer: Serializer = null)(implicit ct: ClassTag[C]): RDD[(K, C)]
----
<1> FIXME
<2> FIXME too

`combineByKeyWithClassTag` transformations use `mapSideCombine` enabled (i.e. `true`) by default. They create a link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD] with the value of `mapSideCombine` when the input partitioner is different from the current one in an RDD.

NOTE: `combineByKeyWithClassTag` is a base transformation for <<combineByKey, combineByKey>>-based transformations, <<aggregateByKey, aggregateByKey>>, <<foldByKey, foldByKey>>, <<reduceByKey, reduceByKey>>, <<countApproxDistinctByKey, countApproxDistinctByKey>>, and <<groupByKey, groupByKey>>.
