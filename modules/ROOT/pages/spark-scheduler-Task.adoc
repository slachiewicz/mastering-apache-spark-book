== [[Task]] Task -- Smallest Individual Unit of Execution

`Task` is the <<contract, abstraction>> of smallest individual <<implementations, units of execution>> that <<run, compute an RDD partition>>.

[[contract]]
.Task Contract
[cols="1m,3",options="header",width="100%"]
|===
| Method
| Description

| runTask
a| [[runTask]]

[source, scala]
----
runTask(context: TaskContext): T
----

Runs the task

Used exclusively when `Task` is requested to <<run, run>>

|===

`Task` is <<creating-instance, created>> when `DAGScheduler` is requested to <<spark-scheduler-DAGScheduler.adoc#submitMissingTasks, submit missing tasks of a stage>>.

NOTE: `Task` is a Scala abstract class and cannot be <<creating-instance, created>> directly. It is created indirectly for the <<implementations, concrete Tasks>>.

.Tasks Are Runtime Representation of RDD Partitions
image::spark-rdd-partitions-job-stage-tasks.png[align="center"]

[[creating-instance]]
`Task` is described by the following:

* [[stageId]] ID of the <<spark-scheduler-Stage.adoc#, stage>> (*Stage ID*)
* [[stageAttemptId]] Stage (execution) attempt ID
* [[partitionId]] ID of the <<spark-rdd-Partition.adoc#, partition>> (*Partition ID*)
* [[localProperties]] Local properties
* [[serializedTaskMetrics]] Serialized <<spark-executor-TaskMetrics.adoc#, TaskMetrics>> (`Array[Byte]`)
* [[jobId]] Optional ID of the <<spark-scheduler-ActiveJob.adoc#, job>> (default: `None`)
* [[appId]] Optional ID of the Spark application (default: `None`)
* [[appAttemptId]] Optional ID of the Spark application's (execution) attempt ID (default: `None`)
* [[isBarrier]] `isBarrier` flag that is to say whether the task belongs to a barrier stage (default: `false`)

`Task` can be <<runTask, run>> (possibly on <<preferredLocations, preferred executor>>).

Tasks are link:spark-Executor.adoc#launchTask[launched on executors] and <<run, ran when `TaskRunner` starts>>.

In other words, a task is a computation on the records in a RDD partition in a stage of a RDD in a Spark job.

NOTE: In Scala `Task` is actually `Task[T]` in which `T` is the type of the result of a task (i.e. the type of the value computed).

[[implementations]]
.Tasks
[cols="1,3",options="header",width="100%"]
|===
| Task
| Description

| <<spark-scheduler-ResultTask.adoc#, ResultTask>>
| [[ResultTask]] Computes a <<spark-scheduler-ResultStage.adoc#, ResultStage>> and gives the result back to the driver

| <<spark-scheduler-ShuffleMapTask.adoc#, ShuffleMapTask>>
| [[ShuffleMapTask]] Computes a <<spark-scheduler-ShuffleMapStage.adoc#, ShuffleMapStage>>

|===

In most cases, the last stage of a Spark job consists of one or more link:spark-scheduler-ResultTask.adoc[ResultTasks], while earlier stages are link:spark-scheduler-ShuffleMapTask.adoc[ShuffleMapTasks].

NOTE: It is possible to have one or more <<spark-scheduler-ShuffleMapTask.adoc#, ShuffleMapTasks>> as part of the last stage.

A task can only belong to one stage and operate on a single partition. All tasks in a stage must be completed before the stages that follow can start.

Tasks are spawned one by one for each stage and partition.

=== [[preferredLocations]] `preferredLocations` Method

[source, scala]
----
preferredLocations: Seq[TaskLocation] = Nil
----

<<spark-TaskLocation.adoc#, TaskLocations>> that represent preferred locations (executors) to execute the task on.

Empty by default and so no task location preferences are defined that says the task could be launched on any executor.

NOTE: Defined by the <<implementations, concrete tasks>>, i.e. link:spark-scheduler-ShuffleMapTask.adoc#preferredLocations[ShuffleMapTask] and link:spark-scheduler-ResultTask.adoc#preferredLocations[ResultTask].

NOTE: `preferredLocations` is used exclusively when `TaskSetManager` is requested to link:spark-scheduler-TaskSetManager.adoc#addPendingTask[register a task as pending execution] and link:spark-scheduler-TaskSetManager.adoc#dequeueSpeculativeTask[dequeueSpeculativeTask].

=== [[run]] Running Task Thread -- `run` Final Method

[source, scala]
----
run(
  taskAttemptId: Long,
  attemptNumber: Int,
  metricsSystem: MetricsSystem): T
----

`run` link:spark-BlockManager.adoc#registerTask[registers the task (identified as `taskAttemptId`) with the local `BlockManager`].

NOTE: `run` uses link:spark-SparkEnv.adoc#blockManager[`SparkEnv` to access the current `BlockManager`].

`run` link:spark-TaskContextImpl.adoc#creating-instance[creates a `TaskContextImpl`] that in turn becomes the task's link:spark-TaskContext.adoc#setTaskContext[TaskContext].

NOTE: `run` is a `final` method and so must not be overriden.

`run` checks <<_killed, _killed>> flag and, if enabled, <<kill, kills the task>> (with `interruptThread` flag disabled).

`run` creates a Hadoop `CallerContext` and sets it.

`run` <<runTask, runs the task>>.

NOTE: This is the moment when the custom ``Task``'s <<runTask, runTask>> is executed.

In the end, `run` link:spark-TaskContextImpl.adoc#markTaskCompleted[notifies `TaskContextImpl` that the task has completed] (regardless of the final outcome -- a success or a failure).

In case of any exceptions, `run` link:spark-TaskContextImpl.adoc#markTaskFailed[notifies `TaskContextImpl` that the task has failed]. `run` link:spark-MemoryStore.adoc#releaseUnrollMemoryForThisTask[requests `MemoryStore` to release unroll memory for this task] (for both `ON_HEAP` and `OFF_HEAP` memory modes).

NOTE: `run` uses link:spark-SparkEnv.adoc#blockManager[`SparkEnv` to access the current `BlockManager`] that it uses to access link:spark-BlockManager.adoc#memoryStore[MemoryStore].

`run` link:spark-MemoryManager.adoc[requests `MemoryManager` to notify any tasks waiting for execution memory to be freed to wake up and try to acquire memory again].

`run` link:spark-TaskContext.adoc#unset[unsets the task's `TaskContext`].

NOTE: `run` uses link:spark-SparkEnv.adoc#memoryManager[`SparkEnv` to access the current `MemoryManager`].

NOTE: `run` is used exclusively when `TaskRunner` is requested to <<spark-Executor-TaskRunner.adoc#run, run>> (when `Executor` is requested to <<spark-Executor.adoc#launchTask, launch a task (on "Executor task launch worker" thread pool sometime in the future)>>).

. The `Task` instance has just been deserialized from `taskBytes` that were sent over the wire to an executor. `localProperties` and link:spark-memory-TaskMemoryManager.adoc[TaskMemoryManager] are already assigned.

=== [[states]][[TaskState]] Task States

A task can be in one of the following states (as described by `TaskState` enumeration):

* `LAUNCHING`
* `RUNNING` when the task is being started.
* `FINISHED` when the task finished with the serialized result.
* `FAILED` when the task fails, e.g. when link:spark-TaskRunner-FetchFailedException.adoc[FetchFailedException], `CommitDeniedException` or any `Throwable` occurs
* `KILLED` when an executor kills a task.
* `LOST`

States are the values of `org.apache.spark.TaskState`.

NOTE: Task status updates are sent from executors to the driver through link:spark-ExecutorBackend.adoc[ExecutorBackend].

Task is finished when it is in one of `FINISHED`, `FAILED`, `KILLED`, `LOST`.

`LOST` and `FAILED` states are considered failures.

TIP: Task states correspond to https://github.com/apache/mesos/blob/master/include/mesos/mesos.proto[org.apache.mesos.Protos.TaskState].

=== [[collectAccumulatorUpdates]] Collect Latest Values of (Internal and External) Accumulators -- `collectAccumulatorUpdates` Method

[source, scala]
----
collectAccumulatorUpdates(taskFailed: Boolean = false): Seq[AccumulableInfo]
----

`collectAccumulatorUpdates` collects the latest values of internal and external accumulators from a task (and returns the values as a collection of link:spark-accumulators.adoc#AccumulableInfo[AccumulableInfo]).

Internally, `collectAccumulatorUpdates` link:spark-TaskContextImpl.adoc#taskMetrics[takes `TaskMetrics`].

NOTE: `collectAccumulatorUpdates` uses <<context, TaskContextImpl>> to access the task's `TaskMetrics`.

`collectAccumulatorUpdates` collects the latest values of:

* link:spark-executor-TaskMetrics.adoc#internalAccums[internal accumulators] whose current value is not the zero value and the `RESULT_SIZE` accumulator (regardless whether the value is its zero or not).

* link:spark-executor-TaskMetrics.adoc#externalAccums[external accumulators] when `taskFailed` is disabled (`false`) or which link:spark-accumulators.adoc#countFailedValues[should be included on failures].

`collectAccumulatorUpdates` returns an empty collection when <<context, TaskContextImpl>> is not initialized.

NOTE: `collectAccumulatorUpdates` is used when link:spark-Executor-TaskRunner.adoc#run[`TaskRunner` runs a task] (and sends a task's final results back to the driver).

=== [[kill]] Killing Task -- `kill` Method

[source, scala]
----
kill(interruptThread: Boolean)
----

`kill` marks the task to be killed, i.e. it sets the internal `_killed` flag to `true`.

`kill` calls link:spark-TaskContextImpl.adoc#markInterrupted[TaskContextImpl.markInterrupted] when `context` is set.

If `interruptThread` is enabled and the internal `taskThread` is available, `kill` interrupts it.

CAUTION: FIXME When could `context` and `interruptThread` not be set?

=== [[internal-registries]] Internal Properties

.Task's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,3",options="header",width="100%"]
|===
| Name
| Description

| _executorDeserializeCpuTime
| [[_executorDeserializeCpuTime]]

| _executorDeserializeTime
| [[_executorDeserializeTime]]

| _reasonIfKilled
| [[_reasonIfKilled]]

| _killed
| [[_killed]]

| context
| [[context]] <<spark-TaskContext.adoc#, TaskContext>>

Set to be a <<spark-BarrierTaskContext.adoc#, BarrierTaskContext>> or <<spark-TaskContextImpl.adoc#, TaskContextImpl>> when the <<isBarrier, isBarrier>> flag is enabled or not, respectively, when `Task` is requested to <<run, run>>

| epoch
| [[epoch]] Task epoch

Starts as `-1`

Set when `TaskSetManager` is <<spark-scheduler-TaskSetManager.adoc#, created>> (to be the <<spark-service-MapOutputTrackerMaster.adoc#getEpoch, epoch>> of the `MapOutputTrackerMaster`)

| metrics
| [[metrics]] link:spark-executor-TaskMetrics.adoc[TaskMetrics]

Created lazily when <<creating-instance, `Task` is created>> from <<serializedTaskMetrics, serializedTaskMetrics>>.

| taskMemoryManager
| [[taskMemoryManager]] link:spark-memory-TaskMemoryManager.adoc[TaskMemoryManager] that manages the memory allocated by the task.

| taskThread
| [[taskThread]]

|===
