== Spark Configuration Properties

[[properties]]
.Spark Configuration Properties
[cols="1m,1",options="header",width="100%"]
|===
| Name
| Description

| spark.default.parallelism
a| [[spark.default.parallelism]] Number of partitions to use for <<spark-rdd-HashPartitioner.adoc#, HashPartitioner>>.

`spark.default.parallelism` corresponds to link:spark-SchedulerBackend.adoc#defaultParallelism[default parallelism] of a scheduler backend and is as follows:

* The number of threads for link:local/spark-LocalSchedulerBackend.adoc[LocalSchedulerBackend].
* the number of CPU cores in link:spark-mesos.adoc#defaultParallelism[Spark on Mesos] and defaults to `8`.
* Maximum of `totalCoreCount` and `2` in link:spark-CoarseGrainedSchedulerBackend.adoc#defaultParallelism[CoarseGrainedSchedulerBackend].

| spark.driver.maxResultSize
a| [[maxResultSize]][[spark.driver.maxResultSize]][[MAX_RESULT_SIZE]] The maximum size of all results of the tasks in a `TaskSet`

Default: `1g`

Used when:

* `Executor` is <<spark-Executor.adoc#maxResultSize, created>> (and later for a <<spark-Executor-TaskRunner.adoc#, TaskRunner>>)

* `TaskSetManager` is <<spark-TaskSetManager.adoc#maxResultSize, created>> (and later requested to <<spark-TaskSetManager.adoc#canFetchMoreResults, check available memory for task results>>)

| spark.executor.extraClassPath
a| [[spark.executor.extraClassPath]][[EXECUTOR_CLASS_PATH]] *User-defined class path for executors*, i.e. URLs representing user-defined class path entries that are added to an executor's class path. URLs are separated by system-dependent path separator, i.e. `:` on Unix-like systems and `;` on Microsoft Windows.

Default: `(empty)`

Used when:

* Spark Standalone's `StandaloneSchedulerBackend` is requested to <<spark-standalone-StandaloneSchedulerBackend.adoc#start, start>> (and creates a command for <<spark-CoarseGrainedExecutorBackend.adoc#, org.apache.spark.executor.CoarseGrainedExecutorBackend>>)

* Spark local's `LocalSchedulerBackend` is requested for the <<local/spark-LocalSchedulerBackend.adoc#getUserClasspath, user-defined class path for executors>>

* Spark on Mesos' `MesosCoarseGrainedSchedulerBackend` is requested to <<spark-mesos/spark-mesos-MesosCoarseGrainedSchedulerBackend.adoc#createCommand, create a command for CoarseGrainedExecutorBackend>>

* Spark on Mesos' `MesosFineGrainedSchedulerBackend` is requested to create a command for `MesosExecutorBackend`

* Spark on Kubernetes' `BasicExecutorFeatureStep` is requested to `configurePod`

* Spark on YARN's `ExecutorRunnable` is requested to <<yarn/spark-yarn-ExecutorRunnable.adoc#prepareEnvironment, prepareEnvironment>> (for `CoarseGrainedExecutorBackend`)

| spark.locality.wait
a| [[spark.locality.wait]] For locality-aware delay scheduling for `PROCESS_LOCAL`, `NODE_LOCAL`, and `RACK_LOCAL` link:spark-TaskSchedulerImpl.adoc#TaskLocality[TaskLocalities] when locality-specific setting is not set.

Default: `3s`

| spark.locality.wait.node
a| [[spark.locality.wait.node]] Scheduling delay for `NODE_LOCAL` link:spark-TaskSchedulerImpl.adoc#TaskLocality[TaskLocality]

Default: The value of <<spark.locality.wait, spark.locality.wait>>

| spark.locality.wait.process
a| [[spark.locality.wait.process]] Scheduling delay for `PROCESS_LOCAL` link:spark-TaskSchedulerImpl.adoc#TaskLocality[TaskLocality]

Default: The value of <<spark.locality.wait, spark.locality.wait>>

| spark.locality.wait.rack
a| [[spark.locality.wait.rack]] Scheduling delay for `RACK_LOCAL` link:spark-TaskSchedulerImpl.adoc#TaskLocality[TaskLocality]

Default: The value of <<spark.locality.wait, spark.locality.wait>>

| spark.logging.exceptionPrintInterval
a| [[spark.logging.exceptionPrintInterval]] How frequently to reprint duplicate exceptions in full (in millis).

Default: `10000`

| spark.scheduler.executorTaskBlacklistTime
a| [[spark.scheduler.executorTaskBlacklistTime]] Time interval to pass after which a task can be re-launched on the executor where it has once failed. It is to prevent repeated task failures due to executor failures.

Default: `0L`

| spark.storage.exceptionOnPinLeak
a| [[spark.storage.exceptionOnPinLeak]]

| spark.task.cpus
a| [[spark.task.cpus]][[CPUS_PER_TASK]] The number of CPU cores used to schedule (_allocate for_) a task

Default: `1`

Used when:

* `ExecutorAllocationManager` is <<spark-ExecutorAllocationManager.adoc#tasksPerExecutorForFullParallelism, created>>

* `TaskSchedulerImpl` is <<spark-TaskSchedulerImpl.adoc#CPUS_PER_TASK, created>>

* `AppStatusListener` is requested to <<spark-core-AppStatusListener.adoc#onEnvironmentUpdate, handle an SparkListenerEnvironmentUpdate event>>

* `LocalityPreferredContainerPlacementStrategy` is requested to `numExecutorsPending`

| spark.unsafe.exceptionOnMemoryLeak
a| [[spark.unsafe.exceptionOnMemoryLeak]]

|===
