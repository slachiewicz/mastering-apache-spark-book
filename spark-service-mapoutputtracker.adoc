== [[MapOutputTracker]] MapOutputTracker -- Shuffle Map Output Registry

`MapOutputTracker` is a Spark service that runs on the driver and executors that <<mapStatuses, tracks the shuffle map outputs>> (with link:spark-MapStatus.adoc[information about the `BlockManager` and estimated size of the reduce blocks per shuffle]).

NOTE: `MapOutputTracker` is registered as the *MapOutputTracker* RPC Endpoint in the RPC Environment when link:spark-SparkEnv.adoc#MapOutputTrackerMasterEndpoint[`SparkEnv` is created].

There are two concrete `MapOutputTrackers`, i.e. one for the driver and another for executors:

* link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] for the driver
* link:spark-service-MapOutputTrackerWorker.adoc[MapOutputTrackerWorker] for executors

Given the different runtime environments of the driver and executors, accessing the current `MapOutputTracker` is possible using link:spark-SparkEnv.adoc#get[SparkEnv].

[source, scala]
----
SparkEnv.get.mapOutputTracker
----

[[internal-registries]]
.`MapOutputTracker` Internal Registries and Counters
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| [[mapStatuses]] `mapStatuses`
| Internal cache with link:spark-MapStatus.adoc[MapStatus] array (indexed by partition id) per link:spark-rdd-ShuffleDependency.adoc#shuffleId[shuffle id].

Used when `MapOutputTracker` <<getStatuses, finds map outputs for a `ShuffleDependency`>>, <<updateEpoch, updates epoch>> and <<unregisterShuffle, unregisters a shuffle>>.

| [[epoch]] `epoch`
| Tracks the epoch in a Spark application.

Starts from `0` when <<creating-instance, `MapOutputTracker` is created>>.

Can be <<updateEpoch, updated>> (on `MapOutputTrackerWorkers`) or link:spark-service-MapOutputTrackerMaster.adoc#incrementEpoch[incremented] (on the driver's `MapOutputTrackerMaster`).

| [[epochLock]] `epochLock`
| FIXME

|===

`MapOutputTracker` is also used for `mapOutputTracker.containsShuffle` and link:spark-service-MapOutputTrackerMaster.adoc#registerShuffle[MapOutputTrackerMaster.registerShuffle] when a new link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] is created.

link:spark-service-MapOutputTrackerMaster.adoc#getStatistics[MapOutputTrackerMaster.getStatistics(dependency)] returns `MapOutputStatistics` that becomes the result of link:spark-dagscheduler-JobWaiter.adoc[JobWaiter.taskSucceeded] for link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] if it's the final stage in a job.

link:spark-service-MapOutputTrackerMaster.adoc#registerMapOutputs[MapOutputTrackerMaster.registerMapOutputs] for a shuffle id and a list of `MapStatus` when a link:spark-dagscheduler-ShuffleMapStage.adoc[ShuffleMapStage] is finished.

NOTE: `MapOutputTracker` is used in link:spark-BlockStoreShuffleReader.adoc[BlockStoreShuffleReader] and when creating link:spark-blockmanager.adoc[BlockManager] and link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc[BlockManagerSlaveEndpoint].

=== [[trackerEndpoint]] `trackerEndpoint` Property

`trackerEndpoint` is a link:spark-RpcEndpointRef.adoc[RpcEndpointRef] that `MapOutputTracker` uses to <<askTracker, send RPC messages>>.

`trackerEndpoint` is initialized when link:spark-SparkEnv.adoc#MapOutputTrackerMasterEndpoint[`SparkEnv` is created] for the driver and executors and cleared when link:spark-service-MapOutputTrackerMaster.adoc#stop[`MapOutputTrackerMaster` is stopped].

=== [[creating-instance]] Creating MapOutputTracker Instance

CAUTION: FIXME

=== [[deserializeMapStatuses]] `deserializeMapStatuses` Method

CAUTION: FIXME

=== [[sendTracker]] `sendTracker` Method

CAUTION: FIXME

=== [[serializeMapStatuses]] `serializeMapStatuses` Method

CAUTION: FIXME

=== [[getStatistics]] Computing Statistics for ShuffleDependency -- `getStatistics` Method

[source, scala]
----
getStatistics(dep: ShuffleDependency[_, _, _]): MapOutputStatistics
----

`getStatistics` returns a `MapOutputStatistics` which is simply a pair of the link:spark-rdd-ShuffleDependency.adoc#shuffleId[shuffle id] (of the input `ShuffleDependency`) and the total sums of estimated sizes of the reduce shuffle blocks from all the link:spark-blockmanager.adoc[BlockManager]s.

Internally, `getStatistics` <<getStatuses, finds map outputs>> for the input link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] and calculates the total sizes for the link:spark-MapStatus.adoc#getSizeForBlock[estimated sizes of the reduce block (in bytes)] for every link:spark-MapStatus.adoc[MapStatus] and partition.

NOTE: The internal `totalSizes` array has the number of elements as specified by the link:spark-rdd-Partitioner.adoc#numPartitions[number of partitions of the `Partitioner`] of the input `ShuffleDependency`. `totalSizes` contains elements as a sum of the estimated size of the block for partition in a link:spark-blockmanager.adoc[BlockManager] (for a `MapStatus`).

NOTE: `getStatistics` is used when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleMapStageSubmitted[`DAGScheduler` accepts a `ShuffleDependency` for execution] (and the link:spark-dagscheduler-ShuffleMapStage.adoc#isAvailable[corresponding `ShuffleMapStage` has already been computed]) and link:#handleTaskCompletion-Success-ShuffleMapTask[gets notified that a `ShuffleMapTask` has completed] (and map-stage jobs waiting for the stage are then marked as finished).

=== [[getMapSizesByExecutorId]] Computing BlockManagerIds with Their Blocks and Sizes -- `getMapSizesByExecutorId` Methods

[source, scala]
----
getMapSizesByExecutorId(shuffleId: Int, startPartition: Int, endPartition: Int)
: Seq[(BlockManagerId, Seq[(BlockId, Long)])]

getMapSizesByExecutorId(shuffleId: Int, reduceId: Int)
: Seq[(BlockManagerId, Seq[(BlockId, Long)])] // <1>
----
<1> Calls the other `getMapSizesByExecutorId` with `endPartition` as `reduceId + 1` and is used exclusively in tests.

CAUTION: FIXME How do the start and end partitions influence the return value?

`getMapSizesByExecutorId` returns a collection of link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId]s with their blocks and sizes.

When executed, you should see the following DEBUG message in the logs:

```
DEBUG Fetching outputs for shuffle [id], partitions [startPartition]-[endPartition]
```

`getMapSizesByExecutorId` <<getStatuses, finds map outputs>> for the input `shuffleId`.

NOTE: `getMapSizesByExecutorId` gets the map outputs for all the partitions (despite the method's signature).

In the end, `getMapSizesByExecutorId` <<convertMapStatuses, converts shuffle map outputs>> (as `MapStatuses`) into the collection of link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId]s with their blocks and sizes.

NOTE: `getMapSizesByExecutorId` is exclusively used when link:spark-BlockStoreShuffleReader.adoc#read[`BlockStoreShuffleReader` reads combined records for a reduce task].

=== [[getEpoch]] Returning Current Epoch -- `getEpoch` Method

[source, scala]
----
getEpoch: Long
----

`getEpoch` returns the current <<epoch, epoch>>.

NOTE: `getEpoch` is used when link:spark-dagscheduler-DAGSchedulerEventProcessLoop.adoc#handleExecutorLost[`DAGScheduler` is notified that an executor was lost] and when link:spark-TaskSetManager.adoc#creating-instance[`TaskSetManager` is created] (and sets the epoch for the tasks in a link:spark-taskscheduler-tasksets.adoc[TaskSet]).

=== [[updateEpoch]] Updating Epoch -- `updateEpoch` Method

[source, scala]
----
updateEpoch(newEpoch: Long): Unit
----

`updateEpoch` updates <<epoch, epoch>> when the input `newEpoch` is greater (and hence more recent) and clears the <<mapStatuses, `mapStatuses` internal cache>>.

You should see the following INFO message in the logs:

```
INFO MapOutputTrackerWorker: Updating epoch to [newEpoch] and clearing cache
```

NOTE: `updateEpoch` is exclusively used when link:spark-executor-TaskRunner.adoc#run[`TaskRunner` runs] (for a task).

=== [[unregisterShuffle]] Unregistering Shuffle -- `unregisterShuffle` Method

[source, scala]
----
unregisterShuffle(shuffleId: Int): Unit
----

`unregisterShuffle` unregisters `shuffleId`, i.e. removes `shuffleId` entry from the <<mapStatuses, mapStatuses>> internal cache.

NOTE: `unregisterShuffle` is used when link:spark-service-contextcleaner.adoc#doCleanupShuffle[`ContextCleaner` removes a shuffle (blocks) from `MapOutputTrackerMaster` and `BlockManagerMaster`] (aka _shuffle cleanup_) and when `BlockManagerSlaveEndpoint` link:spark-blockmanager-BlockManagerSlaveEndpoint.adoc#RemoveShuffle[handles `RemoveShuffle` message].

=== [[stop]] `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` does nothing at all.

NOTE: `stop` is used exclusively when link:spark-SparkEnv.adoc#stop[`SparkEnv` stops] (and stops all the services, `MapOutputTracker` including).

NOTE: `stop` is overriden by link:spark-service-MapOutputTrackerMaster.adoc#stop[MapOutputTrackerMaster].

=== [[getStatuses]] Finding Map Outputs For `ShuffleDependency` in Cache or Fetching Remotely -- `getStatuses` Internal Method

[source, scala]
----
getStatuses(shuffleId: Int): Array[MapStatus]
----

`getStatuses` finds link:spark-MapStatus.adoc[MapStatuses] for the input `shuffleId` in the <<mapStatuses, mapStatuses>> internal cache and, when not available, fetches them from a remote link:spark-service-MapOutputTrackerMaster.adoc[MapOutputTrackerMaster] (using RPC).

Internally, `getStatuses` first queries the <<mapStatuses, `mapStatuses` internal cache>> and returns the map outputs if found.

If not found (in the `mapStatuses` internal cache), you should see the following INFO message in the logs:

```
INFO Don't have map outputs for shuffle [id], fetching them
```

If some other process fetches the map outputs for the `shuffleId` (as recorded in `fetching` internal registry), `getStatuses` waits until it is done.

When no other process fetches the map outputs, `getStatuses` registers the input `shuffleId` in `fetching` internal registry (of shuffle map outputs being fetched).

You should see the following INFO message in the logs:

```
INFO Doing the fetch; tracker endpoint = [trackerEndpoint]
```

`getStatuses` sends a `GetMapOutputStatuses` RPC remote message for the input `shuffleId` to the `trackerEndpoint` expecting a `Array[Byte]`.

NOTE: `getStatuses` requests shuffle map outputs remotely within a timeout and with retries. Refer to link:spark-RpcEndpointRef.adoc[RpcEndpointRef].

`getStatuses` <<deserializeMapStatuses, deserializes the map output statuses>> and records the result in the <<mapStatuses, `mapStatuses` internal cache>>.

You should see the following INFO message in the logs:

```
INFO Got the output locations
```

`getStatuses` removes the input `shuffleId` from `fetching` internal registry.

You should see the following DEBUG message in the logs:

```
DEBUG Fetching map output statuses for shuffle [id] took [time] ms
```

If `getStatuses` could not find the map output locations for the input `shuffleId` (locally and remotely), you should see the following ERROR message in the logs and throws a `MetadataFetchFailedException`.

```
ERROR Missing all output locations for shuffle [id]
```

NOTE: `getStatuses` is used when `MapOutputTracker` <<getMapSizesByExecutorId, getMapSizesByExecutorId>> and <<getStatistics, computes statistics for `ShuffleDependency`>>.

=== [[convertMapStatuses]] Converting MapStatuses To BlockManagerIds with ShuffleBlockIds and Their Sizes -- `convertMapStatuses` Internal Method

[source, scala]
----
convertMapStatuses(
  shuffleId: Int,
  startPartition: Int,
  endPartition: Int,
  statuses: Array[MapStatus]): Seq[(BlockManagerId, Seq[(BlockId, Long)])]
----

`convertMapStatuses` iterates over the input `statuses` array (of link:spark-MapStatus.adoc[MapStatus] entries indexed by map id) and creates a collection of link:spark-blockmanager.adoc#BlockManagerId[BlockManagerId] (for each `MapStatus` entry) with a link:spark-blockdatamanager.adoc#ShuffleBlockId[ShuffleBlockId] (with the input `shuffleId`, a `mapId`, and `partition` ranging from the input `startPartition` and `endPartition`) and link:spark-MapStatus.adoc#getSizeForBlock[estimated size for the reduce block] for every status and partitions.

For any empty `MapStatus`, you should see the following ERROR message in the logs:

```
ERROR Missing an output location for shuffle [id]
```

And `convertMapStatuses` throws a `MetadataFetchFailedException` (with `shuffleId`, `startPartition`, and the above error message).

NOTE: `convertMapStatuses` is exclusively used when <<getMapSizesByExecutorId, `MapOutputTracker` computes ``BlockManagerId``s with their ``ShuffleBlockId``s and sizes>>.

=== [[askTracker]] Sending Blocking Messages To trackerEndpoint RpcEndpointRef -- `askTracker` Method

[source, scala]
----
askTracker[T](message: Any): T
----

`askTracker` link:spark-RpcEndpointRef.adoc#askWithRetry[sends the `message`] to <<trackerEndpoint, `trackerEndpoint` RpcEndpointRef>> and waits for a result.

When an exception happens, you should see the following ERROR message in the logs and `askTracker` throws a `SparkException`.

```
ERROR Error communicating with MapOutputTracker
```

NOTE: `askTracker` is used when `MapOutputTracker` <<getStatuses, fetches map outputs for `ShuffleDependency` remotely>> and <<sendTracker, sends a one-way message>>.
